{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.1 : calcul et publication du temps d'attente moyen par station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-KAFKA-STREAM\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = \"sandbox-hdp.hortonworks.com:6667\"\n",
    "kafka_topic = \"transilien-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire despécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"station\", IntegerType(), True),\n",
    "        StructField(\"train\", StringType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"mode\", StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-01 \n",
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = kafka_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    "```\n",
    "#### Séquence de calcul du temps d'attente moyen par station : étape-02\n",
    "Spécification de la watermark du stream spark. Nous n'acceptons pas les messages ayant plus d'une minute de retard. Il s'agit d'un choix arbitraire qui n'a que peu d'intérêt dans notre projet. Il est toutefois nécessaire de spécifier cette valeur car l'implémentation sous-jacente doit borner l'accumulation des données du stream. [La documentation de Spark explique clairement le concept de _Stateful Stream Processing in Structured Streaming_](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.withWatermark(\"timestamp\", \"1 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-03\n",
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-04\n",
    "Définition de la fênêtre (temporelle) dans laquelle les calculs sont aggrégés. Il s'agit ici de calculer le temps d'attente moyen par station **sur la dernière heure**. Notre fenêtre a donc une largeur temporelle de 60 minutes (_window length_). On choisit de suivre cette moyenne par pas de 2 minutes (_sliding interval_). Dans la mesure où le calcul est demandé par station, la fonction _groupBy_ s'applique au champ _station_ du dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.groupBy(\"station\", window(\"timestamp\", \"60 minutes\", \"2 minutes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-05\n",
    "Pour chaque station, on définit le temps moyen d'attente sur une période de P minutes comme le rapport de P vs le nombre de trains au départ de cette station sur la période P. Ici P = 60 minutes. \n",
    "\n",
    "On crée une _aggrégation_ qui contiendra, pour chaque station et pour chaque fenêtre d'une heure :\n",
    "- une colonne _nt_ qui indiquant le nombre de trains sur la période\n",
    "- une colonne _awt_ donnant le temps d'attente moyen recherché. \n",
    "\n",
    "La colonne _nt_ est injectée à titre indicatif (visualisation dans la console, validation du calcul)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.agg(count(\"train\").alias(\"nt\"), format_number((60. / count(\"train\")), 2).cast(\"double\").alias(\"awt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-06\n",
    "Selection de la fenêtre temporelle associée à la dernière heure. \n",
    "\n",
    "Il s'agit de selectionner, parmis les N fenêtres temporelles produites par Spark, celle qui correspond à la dernière heure écoulée. On utilise ici la fonction **current_timestamp** de Spark afin de rendre la sélection dynamique (i.e. glissante). Le calcul est effectué est dans l'unité de **unix_timestamp** (la seconde) - beaucoup plus facile à manipulée dans ce contexte. \n",
    "\n",
    "Dans l'idée de pourvoir visualiser (si besoin) les valeurs mises en jeu dans le calcul, on choisit de créér les colonnes associées:\n",
    "- oha = one hour ago = now - 62 minutes = now - (window length + sliding interval) \n",
    "- now = now - 1 minutes = now - (sliding interval / 2.) => **valeur ajustée pour n'obtenir qu'une seule fenêtre**\n",
    "- wstart = window.start = borne inférieure de la fenêtre temporelle \n",
    "- wend = window.wend = borne supérieure de la fenêtre temporelle\n",
    "\n",
    "**La clause _where_ permet de selectionner la fenêtre associée à la dernière heure**.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df \\\n",
    "    .withColumn(\"oha\", unix_timestamp(current_timestamp()) - 3720) \\\n",
    "    .withColumn(\"now\", unix_timestamp(current_timestamp()) - 60) \\\n",
    "    .withColumn(\"wstart\", unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", unix_timestamp(\"window.end\")) \\\n",
    "    .where((col(\"oha\") <= col(\"wstart\")) & (col(\"wend\") <= col(\"now\"))) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : concaténation des étapes 01 à 06\n",
    "On s'offre simplement la possibilité d'exécuter la séquence en un seul appel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hour_stream = kafka_stream \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\") \\\n",
    "    .select(\"station\", \"train\", \"timestamp\") \\\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "    .dropDuplicates([\"train\", \"timestamp\"]) \\\n",
    "    .groupBy(\"station\", window(\"timestamp\", \"60 minutes\", \"2 minutes\")) \\\n",
    "    .agg(count(\"train\").alias(\"nt\"), format_number((60. / count(\"train\")), 2).cast(\"double\").alias(\"awt\")) \\\n",
    "    .withColumn(\"oha\", unix_timestamp(current_timestamp()) - 3720) \\\n",
    "    .withColumn(\"now\", unix_timestamp(current_timestamp()) - 60) \\\n",
    "    .withColumn(\"wstart\", unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", unix_timestamp(\"window.end\")) \\\n",
    "    .where((col(\"oha\") <= col(\"wstart\")) & (col(\"wend\") <= col(\"now\"))) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**last_hour_stream** constitue 'l'état' de référence de notre stream Spark. C'est à partir de cet état que l'on produit les résultats, métriques, indicateurs, ... demandés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.2 à 1.6\n",
    "- Calcul du temps moyen d’attente sur la ligne station par station sur la dernière heure\n",
    "- Calcul du temps moyen d’attente globale sur la ligne sur la dernière heure\n",
    "- Trier les stations par temps d’attente moyen sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le plus élevée sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le moins élevée sur la dernière heure\n",
    "- Construire un tableau de bord dans Tableau Software sur la base de ces indicateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les calculs demandés nécessiteraient une seconde opération aggrégation sur le stream _df0_. Or, en l'état actuel de Spark (2.4), [il n'est pas possible d'enchainer plusieurs opération d'aggrégation sur un même stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations). Il nous faut donc trouver une solution de contournement.\n",
    "\n",
    "L'idée retenue est d'effectuer les calculs sur chaque batch de _df0_. Cette approche focntionne ici car chaque batch contient l'intégralité des données sur lesquelles les calculs doivent être réalisés pour produire les résultats attendus - i.e. les données d'attente moyenne par station. Ces résulats sont enregistrés sous _Hive_ dans des tables spécifiques. Ils sont ainsi rendus accéssibles depuis Tableau pour l'élaboration du tableau de bord. \n",
    "\n",
    "Les calculs (Q1.2 à Q.1.5) sont regroupés dans un callback du type _foreachBatch_ dont les appels sont déclenchés par une _StreamingQuery_.\n",
    "\n",
    "La classe _TransilienIndicators_ implémente les fonctionnalités demandées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransilienIndicators():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 streaming_dataframe, \n",
    "                 awt_table=\"awt_table\", \n",
    "                 drop_existing_tables=True, \n",
    "                 auto_start=True, \n",
    "                 verbose=False):\n",
    "        # setup logger\n",
    "        self.logger = logging.getLogger(\"TransilienIndicators\")\n",
    "        self.logger.setLevel(logging.DEBUG if verbose else logging.ERROR)\n",
    "        self.logger.debug(\"TransilienIndicators: initializing...\")\n",
    "        # streaming dataframe \n",
    "        self.streaming_dataframe = streaming_dataframe\n",
    "        # name of the  temp. (i.e. in memory) table \n",
    "        self.awt_table = awt_table\n",
    "        # hive oriented spark session (configured to save the results at the right place in Hive)\n",
    "        self.logger.debug(f\"TransilienIndicators: creating SparkSession\")\n",
    "        self.session = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"MS-SIO-HADOOP-PROJECT-INDICATORS\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"hdfs://sandbox-hdp.hortonworks.com:8020/api-transilien\") \\\n",
    "            .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate() \n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        # drop existing tables (optional)\n",
    "        if drop_existing_tables:\n",
    "            self.__drop_tables()\n",
    "        # the 'save as table' streaming query - acts as a trigger for computeAndSaveAsTables\n",
    "        self.sat_query = None\n",
    "        # the 'console' streaming query - print batches in the console  \n",
    "        self.csl_query = None\n",
    "        # processing time (i.e. streaming queries trigger period)\n",
    "        self.processing_time = \"15 seconds\" #\"1 minutes\"\n",
    "        # start the streaming query?\n",
    "        if auto_start:\n",
    "            self.start()\n",
    "        self.logger.debug(f\"initialization done!\")\n",
    "            \n",
    "    def start(self):\n",
    "        # stop the streaming query if already running\n",
    "        self.stop()\n",
    "        # create then start the 'save' streaming query on the specified streaming dataframe\n",
    "        # also make 'self.computeAndSaveAsTables' member function the associated 'foreachBatch' callback\n",
    "        self.logger.debug(f\"TransilienIndicators: starting 'SaveAsTables' streaming query\")\n",
    "        self.sat_query = self.streaming_dataframe \\\n",
    "            .writeStream \\\n",
    "            .trigger(processingTime=self.processing_time) \\\n",
    "            .foreachBatch(self.computeAndSaveAsTables) \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .start() \n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        # create then start the 'console' streaming query on the specified streaming dataframe\n",
    "        self.logger.debug(f\"TransilienIndicators: starting 'Console' streaming query\")\n",
    "        self.csl_query = self.streaming_dataframe \\\n",
    "            .select(\"station\", \"window\", \"nt\", \"awt\") \\\n",
    "            .orderBy(\"awt\") \\\n",
    "            .writeStream \\\n",
    "            .trigger(processingTime=self.processing_time) \\\n",
    "            .outputMode(\"complete\") \\\n",
    "            .format(\"console\") \\\n",
    "            .option(\"truncate\", False) \\\n",
    "            .start() \n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        self.logger.debug(f\"TransilienIndicators: streaming queries are running\")\n",
    "        \n",
    "    def stop(self):\n",
    "        # stop the streaming query (best effort impl.)\n",
    "        if self.sat_query is  not None:\n",
    "            try:\n",
    "                self.logger.debug(f\"TransilienIndicators: stopping 'SaveAsTables' streaming query\")\n",
    "                self.sat_query.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.sat_query = None\n",
    "                self.logger.debug(f\"`-> done!\")\n",
    "        if self.csl_query is  not None:\n",
    "            try:\n",
    "                self.logger.debug(f\"TransilienIndicators: stopping 'Console' streaming query\")\n",
    "                self.csl_query.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.csl_query = None\n",
    "                self.logger.debug(f\"`-> done!\")\n",
    "            \n",
    "    def cleanup(self):\n",
    "        # cleanup the underlying session \n",
    "        # TODO: not sure this is the right way to do the job\n",
    "        self.logger.debug(f\"TransilienIndicators: shutting down SparkSession\")\n",
    "        self.session.stop()\n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        \n",
    "    def turnVerboseOn(self):\n",
    "        # turn verbose on\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "    def turnVerboseOff(self):\n",
    "         # turn verbose off\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "     \n",
    "    def __drop_tables(self):\n",
    "        # drop existing tables (best effort impl.)\n",
    "        for table in [\"ordered_awt\", \"global_awt\", \"min_awt\", \"max_awt\"]:\n",
    "            try:\n",
    "                self.logger.debug(f\"TransilienIndicators: dropping table {table}\")\n",
    "                self.session.sql(f\"drop table transilien.{table}\")\n",
    "                self.logger.debug(f\"`-> done!\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "    def computeAndSaveAsTables(self, batch, batch_number):\n",
    "        # the big trick... heart of the functionnality implemented by the Indicators class\n",
    "        try:\n",
    "            batch.persist()\n",
    "            # be sure we have some data to handle (incoming dataframe not empty)\n",
    "            # this will avoid creating empty tables on Hive side \n",
    "            if batch.rdd.isEmpty():\n",
    "                self.logger.debug(f\"TransilienIndicators: got empty batch #{batch_number} - aborting...\")\n",
    "                return\n",
    "            # compute ordered average waiting time in minutes (on the last hour period)\n",
    "            df1 = batch.orderBy(asc(\"awt\")).select(col(\"station\"),col(\"awt\"))\n",
    "            # compute global average waiting time in minutes (on the last hour period) \n",
    "            df2 = batch.agg(count(\"station\").alias(\"number_of_stations\"), avg(\"awt\").alias(\"global_awt\"))\n",
    "            # compute min average waiting time in minutes (on the last hour period)\n",
    "            df3 = batch.orderBy(asc(\"awt\")).limit(1).select(col(\"station\"),col(\"awt\").alias(\"min_awt\"))\n",
    "            # compute max average waiting time in minutes (on the last hour period)\n",
    "            df4 = batch.orderBy(desc(\"awt\")).limit(1).select(col(\"station\"),col(\"awt\").alias(\"max_awt\"))\n",
    "            # verbose\n",
    "            if self.logger.getEffectiveLevel() == logging.DEBUG:\n",
    "                df1.show(3, False)\n",
    "                df2.show(3, False)\n",
    "                df3.show(3, False)\n",
    "                df4.show(3, False)\n",
    "            # save results as tables (this is slow!!!)\n",
    "            if not df1.rdd.isEmpty(): # avoid creating empty tables on Hive side\n",
    "                self.logger.debug(f\"saving transilien.ordered_awt for batch #{batch_number}\")\n",
    "                df1.write.mode(\"overwrite\").saveAsTable(\"transilien.ordered_awt\")\n",
    "            if not df2.rdd.isEmpty(): # avoid creating empty tables on Hive side \n",
    "                self.logger.debug(f\"saving transilien.global_awt for batch #{batch_number}\")\n",
    "                df2.write.mode(\"overwrite\").saveAsTable(\"transilien.global_awt\")\n",
    "            if not df3.rdd.isEmpty(): # avoid creating empty tables on Hive side\n",
    "                self.logger.debug(f\"saving transilien.min_awt for batch #{batch_number}\")\n",
    "                df3.write.mode(\"overwrite\").saveAsTable(\"transilien.min_awt\")\n",
    "            if not df4.rdd.isEmpty(): # avoid creating empty tables on Hive side\n",
    "                self.logger.debug(f\"saving transilien.max_awt for batch #{batch_number}\")\n",
    "                df4.write.mode(\"overwrite\").saveAsTable(\"transilien.max_awt\")\n",
    "            self.logger.debug(f\"computeAndSaveAsTables successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"TransilienIndicators: failed to update hive table with batch #{batch_number}\")\n",
    "            self.logger.error(e)\n",
    "        finally:\n",
    "            batch.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = TransilienIndicators(last_hour_stream, drop_existing_tables=True, verbose=True, auto_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.turnVerboseOff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.turnVerboseOn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divers \n",
    "Les cellules suivantes permettent de détruire proprement les sessions Spark\n",
    "\n",
    "Note : dans un contexte de production, le code qui précède serait injecté dans un script Python lancé via _spark-submit_. Dans un tel cas, serait nécessaire placer un appel à _awaitTermination_ sur chaque streaming query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
