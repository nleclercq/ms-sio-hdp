{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN - PARTIE I & II\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.1 : calcul et publication du temps d'attente moyen par station\n",
    "\n",
    "Cette section donne le détail de construction du flux de données relatif à la dernière heure - tranche horaire à laquelle les métriques demandées s'appliquent. Le code est segmenté afin d'en faliciter le commentaire. Il sera repris plus loin afin d'être encapsulé dans une classe.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-KAFKA-STREAM\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"transilien-02\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 512) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire despécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-01 \n",
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    "```\n",
    "#### Séquence de calcul du temps d'attente moyen par station : étape-02\n",
    "Spécification de la watermark du stream spark. Nous n'acceptons pas les messages ayant plus d'une minute de retard. Il s'agit d'un choix arbitraire qui n'a que peu d'intérêt dans notre projet. Il est toutefois nécessaire de spécifier cette valeur car l'implémentation sous-jacente doit borner l'accumulation des données du stream. [La documentation de Spark explique clairement le concept de _Stateful Stream Processing in Structured Streaming_](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withWatermark(\"timestamp\", \"1 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-03\n",
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-04\n",
    "Définition de la fênêtre (temporelle) dans laquelle les calculs sont aggrégés. Il s'agit ici de calculer le temps d'attente moyen par station **sur la dernière heure**. Notre fenêtre a donc une largeur temporelle de 60 minutes (_window length_). On choisit de suivre cette moyenne par pas de 2 minutes (_sliding interval_). Dans la mesure où le calcul est demandé par station, la fonction _groupBy_ s'applique au champ _station_ du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"station\", sf.window(\"timestamp\", \"60 minutes\", \"2 minutes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-05\n",
    "Pour chaque station, on définit le temps moyen d'attente sur une période de P minutes comme le rapport de P vs le nombre de trains au départ de cette station sur la période P. Ici P = 60 minutes. \n",
    "\n",
    "On crée une _aggrégation_ qui contiendra, pour chaque station et pour chaque fenêtre d'une heure :\n",
    "- une colonne _nt_ qui indiquant le nombre de trains sur la période\n",
    "- une colonne _awt_ donnant le temps d'attente moyen recherché. \n",
    "\n",
    "La colonne _nt_ est injectée à titre indicatif (visualisation dans la console, validation du calcul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.agg(sf.count(\"train\").alias(\"nt\"), sf.format_number(60./ sf.count(\"train\"), 2).cast(\"double\").alias(\"awt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-06\n",
    "Selection de la fenêtre temporelle associée à la dernière heure. \n",
    "\n",
    "Il s'agit de selectionner, parmis les N fenêtres temporelles produites par Spark, celle qui correspond à la dernière heure écoulée. On utilise ici la fonction **current_timestamp** de Spark afin de rendre la sélection dynamique (i.e. glissante). Le calcul est effectué est dans l'unité de **unix_timestamp** (la seconde) - beaucoup plus facile à manipulée dans ce contexte. \n",
    "\n",
    "Dans l'idée de pourvoir visualiser (si besoin) les valeurs mises en jeu dans le calcul, on choisit de créér les colonnes associées:\n",
    "- oha = one hour ago = now - 62 minutes = now - (window length + sliding interval) \n",
    "- now = now - 1 minutes = now - (sliding interval / 2.) => **valeur ajustée pour n'obtenir qu'une seule fenêtre**\n",
    "- wstart = window.start = borne inférieure de la fenêtre temporelle \n",
    "- wend = window.wend = borne supérieure de la fenêtre temporelle\n",
    "\n",
    "**La clause _where_ permet de selectionner la fenêtre associée à la dernière heure**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"oha\", sf.unix_timestamp(sf.current_timestamp()) - int((60 + 2) * 60)) \\\n",
    "    .withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()) - int(60 * 2) / 2.) \\\n",
    "    .withColumn(\"wstart\", sf.unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", sf.unix_timestamp(\"window.end\")) \\\n",
    "    .where((sf.col(\"oha\") <= sf.col(\"wstart\")) & (sf.col(\"wend\") <= sf.col(\"now\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, _df_ constitue 'l'état' de référence de notre stream Spark. C'est à partir de cet état que l'on produit les résultats, métriques, indicateurs, ... demandés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation des étapes 01 à 06\n",
    "\n",
    "On lance un 'writeStream' vers la console afin de visualiser les données produites par le stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .select(\"station\", \"window\", \"nt\", \"awt\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "+--------+------------------------------------------+---+----+\n",
    "|station |window                                    |nt |awt |\n",
    "+--------+------------------------------------------+---+----+\n",
    "|87382473|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87386425|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87382259|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|19 |3.16|\n",
    "|87382457|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|18 |3.33|\n",
    "|87382440|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|20 |3.0 |\n",
    "|87382333|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87382887|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|12 |5.0 |\n",
    "|87334482|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|30 |2.0 |\n",
    "|87381137|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|48 |1.25|\n",
    "|87386318|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|17 |3.53|\n",
    "|87382374|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|12 |5.0 |\n",
    "|87382499|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|27 |2.22|\n",
    "|87382655|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|5  |12.0|\n",
    "|87382382|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|25 |2.4 |\n",
    "|87381905|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|31 |1.94|\n",
    "|87384008|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|16 |3.75|\n",
    "|87386003|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|19 |3.16|\n",
    "|87386300|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|24 |2.5 |\n",
    "|87381129|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|38 |1.58|\n",
    "|87386409|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|24 |2.5 |\n",
    "+--------+------------------------------------------+---+----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt de la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.2 à 1.6\n",
    "- Calcul du temps moyen d’attente sur la ligne station par station sur la dernière heure\n",
    "- Calcul du temps moyen d’attente globale sur la ligne sur la dernière heure\n",
    "- Trier les stations par temps d’attente moyen sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le plus élevée sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le moins élevée sur la dernière heure\n",
    "- Construire un tableau de bord dans Tableau Software sur la base de ces indicateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les calculs demandés nécessiteraient une seconde opération aggrégation sur le stream (last_hour_stream). Or, en l'état actuel de Spark (2.4), [il n'est pas possible d'enchainer plusieurs opération d'aggrégation sur un même stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations). Il nous faut donc trouver une solution de contournement.\n",
    "\n",
    "L'idée retenue est d'effectuer les calculs sur chaque batch de last_hour_stream. Cette approche fonctionne ici car chaque batch contient l'intégralité des données sur lesquelles les calculs doivent être réalisés pour produire les résultats attendus - i.e. les données d'attente moyenne par station. Ces résulats sont enregistrés sous _Hive_ dans des tables spécifiques. Ils sont ainsi rendus accéssibles depuis Tableau pour l'élaboration du tableau de bord. \n",
    "\n",
    "Les calculs (Q1.2 à Q.1.5) sont regroupés dans un callback du type _foreachBatch_ dont les appels sont déclenchés par une _StreamingQuery_.\n",
    "\n",
    "La classe _TransilienStreamProcessor_ implémente les fonctionnalités demandées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from py4j.java_gateway import java_import\n",
    "from api_transilien_tools import NotebookCellContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransilienStreamProcessor(NotebookCellContent):\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __init__(self, config):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        NotebookCellContent.__init__(self, \"TransilienStreamProcessor\")\n",
    "    \n",
    "        # store configuration \n",
    "        self.config = config\n",
    "        \n",
    "        # setup logger\n",
    "        self.set_logging_level(logging.DEBUG if self.config['verbose'] else logging.ERROR)\n",
    "        self.debug(\"TSP:initializing...\")\n",
    "        \n",
    "        # kafka oriented spark session (configured to process incoming Kafka messages)\n",
    "        self.debug(f\"TSP:creating kafka oriented spark session\")\n",
    "        self.kafka_session = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"MS-SIO-HADOOP-PROJECT-STREAM\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", self.config['spark_sql_shuffle_partitions']) \\\n",
    "            .config('spark.sql.hive.thriftServer.singleSession', True) \\\n",
    "            .config('hive.server2.thrift.port', self.config['hive_thrift_server_port']) \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "        self.debug(f\"`-> done!\")\n",
    "        \n",
    "        # kafka 'last hour awt' stream initialization\n",
    "        self.debug(f\"TSP:initializing 'last hour awt' stream\")\n",
    "        self.last_hour_awt_stream = self.setup_last_hour_awt_stream()\n",
    "        self.debug(f\"`-> done!\")\n",
    "        \n",
    "        # kafka 'trains progression' stream initialization\n",
    "        self.debug(f\"TSP:initializing 'trains progression' stream\")\n",
    "        self.trains_progression_stream = self.setup_trains_progression_stream()\n",
    "        self.debug(f\"`-> done!\")\n",
    "        \n",
    "        # start our own thrift server (in singleSession mode) \n",
    "        # this will allow to expose the temp. views and make them reachable from Tableau Software   \n",
    "        self.__start_thrift_server()\n",
    "            \n",
    "        # create a temp. view for the transilien stations data (label, geo.loc., ...)\n",
    "        self.__create_stations_view()\n",
    "        \n",
    "        # the last hour awt hive sink (streaming query)\n",
    "        # acts as a trigger for computeAwtMetricsAndSaveAsTempViews (forEachBatch callback)\n",
    "        self.lhawt_hive_sink = None\n",
    "        \n",
    "        # the console sibk (streaming query) \n",
    "        # print batches into the console  \n",
    "        self.lhawt_console_sink = None\n",
    "        \n",
    "        # the trains progression hive sink (streaming query)\n",
    "        # acts as a trigger for computeTrainsProgressionAndSaveAsTempView (forEachBatch callback)\n",
    "        self.trprg_hive_sink = None\n",
    "        \n",
    "        # processing time (i.e. streaming queries trigger period)\n",
    "        self.processing_time = \"10 seconds\"\n",
    "        \n",
    "        # start the streaming queries?\n",
    "        if self.config['auto_start']:\n",
    "            self.start()\n",
    "            \n",
    "        self.debug(f\"initialization done!\")\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __start_thrift_server(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        try:\n",
    "            self.debug(f\"TSP:starting thrift server on port {self.config['hive_thrift_server_port']}\") \n",
    "            #sc.setLogLevel('INFO')\n",
    "            java_import(sc._gateway.jvm,\"\")\n",
    "            sc._gateway.jvm.org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.startWithContext(spark._jwrapped)\n",
    "            #sc.setLogLevel('ERROR')       \n",
    "            self.debug(f\"TSP:thrift server successfully started\") \n",
    "        except Exception as e:\n",
    "            self.error(e)\n",
    "            \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __create_stations_view(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # read data local data file       \n",
    "        df = self.kafka_session \\\n",
    "            .read \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"sep\", \",\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .load(\"file:/root/ms-sio-hdp/api-transilien/transilien_line_l_stations_by_code.csv\") \\\n",
    "            .createOrReplaceTempView(\"stations_data\")\n",
    "                   \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def setup_last_hour_awt_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        wm = float(self.config['kafka_lhawt_stream_watermark'])\n",
    "        wl = float(self.config['kafka_lhawt_stream_window_length'])\n",
    "        si = float(self.config['kafka_lhawt_stream_sliding_interval'])\n",
    "        oha_offset = int((wl + si) * 60.)\n",
    "        now_offset = int(60. * si / 2.)\n",
    "        # setup 'last hour stream' (see above for details)\n",
    "        return self.kafka_session \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", self.config['kafka_broker']) \\\n",
    "            .option(\"subscribe\", self.config['kafka_topic']) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load() \\\n",
    "            .select(sf.from_json(sf.col(\"value\").cast(\"string\"), self.config['json_schema'], self.config['json_options']).alias(\"departure\")) \\\n",
    "            .select(\"departure.*\") \\\n",
    "            .withWatermark(\"timestamp\", f\"{int(wm)} minutes\") \\\n",
    "            .dropDuplicates([\"train\", \"timestamp\"]) \\\n",
    "            .groupBy(\"station\", sf.window(\"timestamp\", f\"{int(wl)} minutes\", f\"{int(si)} minutes\")) \\\n",
    "            .agg(sf.count(\"train\").alias(\"nt\"), sf.format_number(wl / sf.count(\"train\"), 1).cast(\"double\").alias(\"awt\")) \\\n",
    "            .withColumn(\"oha\", sf.unix_timestamp(sf.current_timestamp()) - oha_offset) \\\n",
    "            .withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()) - now_offset) \\\n",
    "            .withColumn(\"wstart\", sf.unix_timestamp(\"window.start\")) \\\n",
    "            .withColumn(\"wend\", sf.unix_timestamp(\"window.end\")) \\\n",
    "            .where((sf.col(\"oha\") <= sf.col(\"wstart\")) & (sf.col(\"wend\") <= sf.col(\"now\"))) \\\n",
    "            .drop(\"oha\", \"now\", \"wstart\", \"wend\")\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def setup_trains_progression_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        time_window = config['kafka_trprg_time_window']\n",
    "        contiguous_stations = self.config['kafka_trprg_stream_stations']\n",
    "        return self.kafka_session \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "            .option(\"subscribe\", \"transilien-02\") \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load() \\\n",
    "            .select(sf.from_json(sf.col(\"value\").cast(\"string\"), self.config['json_schema'], self.config['json_options']).alias(\"departure\")) \\\n",
    "            .select(\"departure.*\") \\\n",
    "            .filter(sf.col(\"station\").isin(list(contiguous_stations.values()))).filter(\"mode='R'\") \\\n",
    "            .withColumn(\"departure\", sf.unix_timestamp(\"timestamp\")).drop(\"timestamp\") \\\n",
    "            .where(sf.col(\"departure\").between(sf.unix_timestamp(sf.current_timestamp()) - int(time_window/2.), \\\n",
    "                                               sf.unix_timestamp(sf.current_timestamp()) + int(time_window/2.))) \\\n",
    "            .groupBy(\"train\", \"station\", \"departure\", \"mode\", \"mission\", \"terminus\").agg(sf.count(\"train\").alias(\"tmp\")).drop(\"tmp\")\n",
    "  \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        self.start_last_hour_awt_stream()\n",
    "        self.start_trains_progress_stream()\n",
    "     \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        self.stop_last_hour_awt_stream()\n",
    "        self.stop_trains_progress_stream()\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start_last_hour_awt_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries if already running\n",
    "        self.stop_last_hour_awt_stream()\n",
    "                       \n",
    "        # last hour awt: create then start the hive sink (streaming query)\n",
    "        # also make 'self.computeAwtMetricsAndSaveAsTempViews' member function the associated 'foreachBatch' callback\n",
    "        self.debug(f\"TSP:starting hive sink for the 'last hour average waiting time' stream (streaming query)\")\n",
    "        self.lhawt_hive_sink =  self.last_hour_awt_stream \\\n",
    "                            .writeStream \\\n",
    "                            .trigger(processingTime=self.processing_time) \\\n",
    "                            .foreachBatch(self.computeAwtMetricsAndSaveAsTempViews) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .start()\n",
    "        self.debug(f\"`-> done!\")\n",
    "                       \n",
    "        # last hour awt: create then start the console sink (streaming query)\n",
    "        self.debug(f\"TSP:starting console sink for the 'last hour average waiting time' stream (streaming query)\")\n",
    "        self.lhawt_console_sink = self.last_hour_awt_stream \\\n",
    "                            .orderBy(\"awt\") \\\n",
    "                            .writeStream \\\n",
    "                            .trigger(processingTime=self.processing_time) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .format(\"console\") \\\n",
    "                            .option(\"truncate\", False) \\\n",
    "                            .start() \n",
    "        self.debug(f\"`-> done!\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start_trains_progress_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries if already running\n",
    "        self.stop_trains_progress_stream()\n",
    "                       \n",
    "        # trains progression: create then start the hive sink (streaming query)\n",
    "        # also make 'self.computeTrainsProgressionAndSaveAsTempView' member function the associated 'foreachBatch' callback\n",
    "        self.debug(f\"TSP:starting console sink for the 'trains progression' stream (streaming query)\")\n",
    "        self.trprg_hive_sink = self.trains_progression_stream \\\n",
    "                            .writeStream \\\n",
    "                            .foreachBatch(self.computeTrainsProgressionAndSaveAsTempView) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .start()\n",
    "        self.debug(f\"`-> done!\")\n",
    "        self.debug(f\"TSP:streaming queries are running\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop_last_hour_awt_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries (best effort impl.)\n",
    "        if self.lhawt_hive_sink is  not None:\n",
    "            try:\n",
    "                self.debug(f\"TSP:stopping hive sink for the 'last hour average waiting time' stream (streaming query)\")\n",
    "                self.lhawt_hive_sink.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.lhawt_hive_sink = None\n",
    "                self.debug(f\"`-> done!\")\n",
    "        if self.lhawt_console_sink is  not None:\n",
    "            try:\n",
    "                self.debug(f\"TSP:stopping console sink for the 'last hour average waiting time' stream (streaming query)\")\n",
    "                self.lhawt_console_sink.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.lhawt_console_sink = None\n",
    "                self.debug(f\"`-> done!\")\n",
    "   \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop_trains_progress_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries (best effort impl.)\n",
    "        if self.trprg_hive_sink is  not None:\n",
    "            try:\n",
    "                self.debug(f\"TSP:stopping hive sink for the 'trains progression stream' (streaming query)\")\n",
    "                self.trprg_hive_sink.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.trprg_hive_sink = None\n",
    "                self.debug(f\"`-> done!\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def cleanup(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # cleanup the underlying session \n",
    "        # TODO: not sure this is the right way to do the job\n",
    "        self.debug(f\"TSP:shutting down Kafka-SparkSession\")\n",
    "        self.kafka_session.stop()\n",
    "        self.debug(f\"`-> done!\")\n",
    "        self.debug(f\"TSP:shutting down Hive-SparkSession\")\n",
    "        self.hive_session.sql(\"clear cache\")\n",
    "        self.hive_session.stop()\n",
    "        self.debug(f\"`-> done!\")\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def turnVerboseOn(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # turn verbose on\n",
    "        self.set_logging_level(logging.DEBUG)\n",
    "       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def turnVerboseOff(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # turn verbose off\n",
    "        self.set_logging_level(logging.ERROR)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def computeAwtMetricsAndSaveAsTempViews(self, batch, batch_number):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # PART-I: COMPUTE AVERAGE WAITING TIME METRICS\n",
    "        # --------------------------------------------\n",
    "        # this 'forEachBatch' callback is attached to the our 'lhawt_hive_sink' (streaming query)\n",
    "        try:\n",
    "            # clear cell content so that we don't cumulate the log               \n",
    "            #self.clear_output()\n",
    "                              \n",
    "            # be sure we have some data to handle (incoming dataframe not empty)\n",
    "            # this will avoid creating empty tables on Hive side \n",
    "            if batch.rdd.isEmpty():\n",
    "                self.warning(f\"TSP:ignoring empty batch #{batch_number}\")\n",
    "                return\n",
    "\n",
    "            self.debug(f\"TSP:entering computeAwtMetricsAndSaveAsTempViews for batch #{batch_number}...\")\n",
    "                              \n",
    "            # PART-I: Q1.1 & Q1.3: compute ordered average waiting time in minutes (on the last hour period)\n",
    "            self.debug(f\"computing ordered average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.orderBy(sf.asc(\"awt\")).select(batch.station, batch.awt)    \n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"ordered_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "                                                                  \n",
    "            # PART-I: Q1.2: compute global average waiting time in minutes (on the last hour period) \n",
    "            self.debug(f\"computing global average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.agg(sf.count(\"station\").alias(\"number_of_stations\"), sf.avg(\"awt\").alias(\"global_awt\"))\n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"global_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "            \n",
    "            # PART-I: Q1.4: compute min average waiting time in minutes (on the last hour period)\n",
    "            self.debug(f\"computing min. average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.orderBy(sf.asc(\"awt\")).limit(1).select(batch.station, batch.awt.alias(\"min_awt\"))\n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"min_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "           \n",
    "            # PART-I: Q1.5: compute max average waiting time in minutes (on the last hour period)\n",
    "            self.debug(f\"computing min. average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.orderBy(sf.desc(\"awt\")).limit(1).select(batch.station, batch.awt.alias(\"max_awt\"))\n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"max_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "                              \n",
    "            self.debug(f\"TSP:computeAwtMetricsAndSaveAsTempViews successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            self.error(f\"TSP:failed to update Hive tables for batch #{batch_number}\")\n",
    "            self.error(e)\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------    \n",
    "    def computeTrainsProgressionAndSaveAsTempView(self, batch, batch_number):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # PART-II: COMPUTE TRAINS PROGRESSION\n",
    "        # ------------------------------------\n",
    "        # this 'forEachBatch' callback is attached to the our 'trprg_hive_sink' (streaming query)\n",
    "        try:\n",
    "                       \n",
    "            # clear cell content so that we don't cumulate the log               \n",
    "            self.clear_output()\n",
    "                              \n",
    "            # be sure we have some data to handle (incoming dataframe not empty)\n",
    "            # this will avoid creating empty tables on Hive side \n",
    "            if batch.rdd.isEmpty():\n",
    "                self.warning(f\"TSP:ignoring empty batch #{batch_number}\")\n",
    "                return\n",
    "\n",
    "            self.debug(f\"TSP:entering computeTrainsProgressionAndSaveAsTempView for batch #{batch_number}...\")\n",
    "  \n",
    "            t = time.time()\n",
    "            contiguous_stations = self.config['kafka_trprg_stream_stations']\n",
    "            \n",
    "            # the main trick: covert rows to columns using spark SQL Pivot\n",
    "            # https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=pivot\n",
    "            tmp = batch.groupby(\"train\").pivot(\"station\").max(\"departure\").fillna(0)\n",
    "            # rename columns: station-a code => sa & station-b code => sb      \n",
    "            tmp = tmp.select(\"train\", sf.col(str(contiguous_stations['sa'])).alias(\"sa\"), sf.col(str(contiguous_stations['sb'])).alias(\"sb\"))\n",
    "            # remove undefined columns  (i.e. the ones containing the 'fillna' value - see above)\n",
    "            tmp = tmp.filter((tmp.sa > 0) & (tmp.sb > 0))\n",
    "            # compute delta time between contiguous stations\n",
    "            tmp = tmp.withColumn(\"dt\", tmp.sb - tmp.sa)\n",
    "            # compute travel direction: 1 for a -> b : 1 or -1 for b -> a (column created for debugging purpose)   \n",
    "            #tmp = tmp.withColumn(\"direction\", sf.when(sf.col(\"dt\") > sf.lit(0.), sf.lit(1)).otherwise(sf.lit(-1)))\n",
    "            # insert a column containing the 'now' timestamp \n",
    "            tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "            # is the train travel between station-a & station-b (or station-b to station-a) belongs to the past? (column created for debugging purpose)\n",
    "            #tmp = tmp.withColumn(\"in_past\", (tmp.now > tmp.sa) & (tmp.now > tmp.sb))\n",
    "            # is the train travel between station-a & station-b (or station-b to station-a) belongs to the future? (column created for debugging purpose)\n",
    "            #tmp = tmp.withColumn(\"in_future\", (tmp.now < tmp.sa) & (tmp.now < tmp.sb))\n",
    "            # is the train travel between station-a & station-b (or station-b to station-a) in progress?\n",
    "            tmp = tmp.withColumn(\"in_progress\", (~((tmp.now > tmp.sa) & (tmp.now > tmp.sb))) & (~((tmp.now < tmp.sa) & (tmp.now < tmp.sb))))\n",
    "            #tmp = tmp.filter(sf.col(\"in-progress\") | sf.col(\"in-future\"))\n",
    "            # select the right departure time: depends on the travel direction (A->B or B->A)\n",
    "            tmp = tmp.withColumn(\"departure_uxts\", sf.when(tmp.dt > sf.lit(0), tmp.sa).otherwise(tmp.sb))\n",
    "            # convert departure time to humanly readable format      \n",
    "            tmp = tmp.withColumn(\"departure_date\", sf.from_unixtime(sf.col(\"departure_uxts\")))\n",
    "            # order by departure time\n",
    "            #tmp = tmp.orderBy(sf.col(\"departure_uxts\"))\n",
    "            # log (for debugging purpose)\n",
    "            #tmp.show()\n",
    "            # compute trains progression: (now - depature_time) / (travel time between stations a & b)\n",
    "            #tmp = tmp.withColumn(\"progress\", sf.format_number((100. * (tmp.now - tmp.departure_uxts)) / sf.abs(tmp.dt), 1).cast(\"double\"))  \n",
    "            tmp = tmp.withColumn(\"progress\", (100. * (tmp.now - tmp.departure_uxts)) / sf.abs(tmp.dt))  \n",
    "            # compute trains progression: maintain value in  the [O, 100]% range \n",
    "            tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress < sf.lit(0.), sf.lit(0.)).otherwise(tmp.progress))             \n",
    "            # compute trains progression: maintain value in  the [O, 100]% range \n",
    "            tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress > sf.lit(100.), sf.lit(100.)).otherwise(tmp.progress))\n",
    "            # compute progress bar value than will be displayed in Tableau Software (this is a trick to display travel direction)            \n",
    "            tmp = tmp.withColumn(\"progress_bar_value\", sf.when(tmp.in_progress, sf.when(tmp.dt > sf.lit(0.), tmp.progress).otherwise(100. - tmp.progress)).otherwise(tmp.progress))\n",
    "            # round progress values to 1 digit\n",
    "            tmp = tmp.withColumn(\"progress\", sf.format_number(tmp.progress, 1).cast(\"double\"))\n",
    "            tmp = tmp.withColumn(\"progress_bar_value\", sf.format_number(tmp.progress_bar_value, 1).cast(\"double\"))\n",
    "            # re-inject columns from intial batch (some info could be useful to display in Tableau)\n",
    "            tmp = tmp.join(batch, \"train\", how=\"left\")\n",
    "            # remove duplicates \n",
    "            tmp = tmp.dropDuplicates([\"train\"])\n",
    "            # log (for debugging purpose)\n",
    "            # tmp.show()\n",
    "            # order by departure time\n",
    "            tmp = tmp.orderBy(tmp.departure_uxts)\n",
    "            # select required columns\n",
    "            #tmp = tmp.select(\"train\", sf.col(\"departure_date\").alias(\"departure\"), \"station\", \"mission\", \"progress\", \"direction\", \"progress_bar_value\", \"in_past\", \"in_future\", \"in_progress\")\n",
    "            tmp = tmp.select(\"train\", sf.col(\"departure_date\").alias(\"departure\"), \"station\", \"mission\", \"progress\", \"progress_bar_value\", \"in_progress\")\n",
    "            # log (for debugging purpose)\n",
    "            tmp.show()\n",
    "                                 \n",
    "            # create a temp. view that - visible from Tableau             \n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"trains_progression\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "                              \n",
    "            self.debug(f\"TSP:computeTrainsProgressionAndSaveAsTempView successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            self.error(f\"TSP:failed to update Hive tables for batch #{batch_number}\")\n",
    "            self.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour mémoire : dans un contexte de production, le code qui précède serait injecté dans un script Python lancé via _spark-submit_. Dans un tel cas, serait nécessaire placer un appel à _awaitTermination_ sur chaque streaming query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "config = {}\n",
    "\n",
    "# json schema & options for kafka messages deserialization \n",
    "config['json_schema'] = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")\n",
    "config['json_options'] = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}\n",
    "\n",
    "# spark sesssions options\n",
    "config['spark_sql_shuffle_partitions'] = 4\n",
    "\n",
    "# kafka source configuration: broker & topic\n",
    "config['kafka_broker'] = \"sandbox-hdp.hortonworks.com:6667\"\n",
    "config['kafka_topic'] = \"transilien-02\"\n",
    "\n",
    "# kafka stream configuration: structured stream windowing for the last hour average waiting time stream\n",
    "config['kafka_lhawt_stream_watermark'] = 1 \n",
    "config['kafka_lhawt_stream_window_length'] = 60\n",
    "config['kafka_lhawt_stream_sliding_interval'] = 2\n",
    "\n",
    "# kafka stream configuration: contiguous stations for the trains progression stream\n",
    "config['kafka_trprg_stream_stations'] = {\n",
    "    'sa':87381129, # Station A: CLICHY LEVALLOIS\n",
    "    'sb':87382002  # Station B: BECON LES BRUYERES\n",
    "    \n",
    "    #'sb':87381137  # Station B: ASNIERES SUR SEINE\n",
    "    \n",
    "}\n",
    "\n",
    "# kafka stream configuration: time window for the trains progression stream\n",
    "config['kafka_trprg_time_window'] = 1800\n",
    "\n",
    "# hive (local) thrift server configuration\n",
    "config['hive_thrift_server_port'] = 10015\n",
    "\n",
    "# misc. options\n",
    "config['auto_start'] = True \n",
    "config['verbose'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:05:47 DEBUG:TSP:entering computeTrainsProgressionAndSaveAsTempView for batch #0...\n",
      "17:05:49 DEBUG:TSP:entering computeAwtMetricsAndSaveAsTempViews for batch #0...\n",
      "17:05:49 DEBUG:computing ordered average waiting time...\n",
      "17:05:52 DEBUG:`-> took 2.34 s\n",
      "17:05:52 DEBUG:computing global average waiting time...\n",
      "17:05:53 DEBUG:`-> took 0.97 s\n",
      "17:05:53 DEBUG:computing min. average waiting time...\n",
      "17:05:54 DEBUG:`-> took 0.91 s\n",
      "17:05:54 DEBUG:computing min. average waiting time...\n",
      "17:05:55 DEBUG:`-> took 0.99 s\n",
      "17:05:55 DEBUG:TSP:computeAwtMetricsAndSaveAsTempViews successfully executed for batch #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------+-------+--------+------------------+-----------+\n",
      "| train|          departure| station|mission|progress|progress_bar_value|in_progress|\n",
      "+------+-------------------+--------+-------+--------+------------------+-----------+\n",
      "|133613|2019-03-03 16:55:00|87382002|   VOLA|   100.0|             100.0|      false|\n",
      "|133672|2019-03-03 17:01:00|87382002|   POVA|    97.3|               2.7|       true|\n",
      "|133617|2019-03-03 17:10:00|87382002|   VOLA|     0.0|               0.0|      false|\n",
      "|133676|2019-03-03 17:15:00|87382002|   POVA|     0.0|               0.0|      false|\n",
      "+------+-------------------+--------+-------+--------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:05:59 DEBUG:TSP:entering computeAwtMetricsAndSaveAsTempViews for batch #1...\n",
      "17:05:59 DEBUG:computing ordered average waiting time...\n",
      "17:06:00 DEBUG:`-> took 13.79 s\n",
      "17:06:00 DEBUG:TSP:computeTrainsProgressionAndSaveAsTempView successfully executed for batch #0\n",
      "17:06:01 DEBUG:`-> took 1.67 s\n",
      "17:06:01 DEBUG:computing global average waiting time...\n",
      "17:06:01 DEBUG:`-> took 0.52 s\n",
      "17:06:01 DEBUG:computing min. average waiting time...\n",
      "17:06:02 DEBUG:`-> took 0.47 s\n",
      "17:06:02 DEBUG:computing min. average waiting time...\n",
      "17:06:02 DEBUG:`-> took 0.48 s\n",
      "17:06:02 DEBUG:TSP:computeAwtMetricsAndSaveAsTempViews successfully executed for batch #1\n",
      "17:08:06 DEBUG:TSP:stopping hive sink for the 'last hour average waiting time' stream (streaming query)\n",
      "17:08:06 DEBUG:`-> done!\n",
      "17:08:06 DEBUG:TSP:stopping console sink for the 'last hour average waiting time' stream (streaming query)\n",
      "17:08:06 DEBUG:`-> done!\n",
      "17:08:06 DEBUG:TSP:stopping hive sink for the 'trains progression stream' (streaming query)\n",
      "17:08:06 DEBUG:`-> done!\n"
     ]
    }
   ],
   "source": [
    "ti = TransilienStreamProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.turnVerboseOff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.turnVerboseOn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleanup...\n",
    "\n",
    "NB: the python kernel must be restart after a call to TransilienStreamProcessor.cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
