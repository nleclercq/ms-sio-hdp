{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN - PARTIE I\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.1 : calcul et publication du temps d'attente moyen par station\n",
    "\n",
    "Cette section donne le détail de construction du flux de données relatif à la dernière heure - tranche horaire à laquelle les métriques demandées s'appliquent. Le code est segmenté afin d'en faliciter le commentaire. Il sera repris plus loin afin d'être encapsulé dans une classe.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-KAFKA-STREAM\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"transilien-02\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire despécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-01 \n",
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    "```\n",
    "#### Séquence de calcul du temps d'attente moyen par station : étape-02\n",
    "Spécification de la watermark du stream spark. Nous n'acceptons pas les messages ayant plus d'une minute de retard. Il s'agit d'un choix arbitraire qui n'a que peu d'intérêt dans notre projet. Il est toutefois nécessaire de spécifier cette valeur car l'implémentation sous-jacente doit borner l'accumulation des données du stream. [La documentation de Spark explique clairement le concept de _Stateful Stream Processing in Structured Streaming_](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withWatermark(\"timestamp\", \"1 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-03\n",
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-04\n",
    "Définition de la fênêtre (temporelle) dans laquelle les calculs sont aggrégés. Il s'agit ici de calculer le temps d'attente moyen par station **sur la dernière heure**. Notre fenêtre a donc une largeur temporelle de 60 minutes (_window length_). On choisit de suivre cette moyenne par pas de 2 minutes (_sliding interval_). Dans la mesure où le calcul est demandé par station, la fonction _groupBy_ s'applique au champ _station_ du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"station\", sf.window(\"timestamp\", \"60 minutes\", \"2 minutes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-05\n",
    "Pour chaque station, on définit le temps moyen d'attente sur une période de P minutes comme le rapport de P vs le nombre de trains au départ de cette station sur la période P. Ici P = 60 minutes. \n",
    "\n",
    "On crée une _aggrégation_ qui contiendra, pour chaque station et pour chaque fenêtre d'une heure :\n",
    "- une colonne _nt_ qui indiquant le nombre de trains sur la période\n",
    "- une colonne _awt_ donnant le temps d'attente moyen recherché. \n",
    "\n",
    "La colonne _nt_ est injectée à titre indicatif (visualisation dans la console, validation du calcul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.agg(sf.count(\"train\").alias(\"nt\"), sf.format_number(60./ sf.count(\"train\"), 2).cast(\"double\").alias(\"awt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-06\n",
    "Selection de la fenêtre temporelle associée à la dernière heure. \n",
    "\n",
    "Il s'agit de selectionner, parmis les N fenêtres temporelles produites par Spark, celle qui correspond à la dernière heure écoulée. On utilise ici la fonction **current_timestamp** de Spark afin de rendre la sélection dynamique (i.e. glissante). Le calcul est effectué est dans l'unité de **unix_timestamp** (la seconde) - beaucoup plus facile à manipulée dans ce contexte. \n",
    "\n",
    "Dans l'idée de pourvoir visualiser (si besoin) les valeurs mises en jeu dans le calcul, on choisit de créér les colonnes associées:\n",
    "- oha = one hour ago = now - 62 minutes = now - (window length + sliding interval) \n",
    "- now = now - 1 minutes = now - (sliding interval / 2.) => **valeur ajustée pour n'obtenir qu'une seule fenêtre**\n",
    "- wstart = window.start = borne inférieure de la fenêtre temporelle \n",
    "- wend = window.wend = borne supérieure de la fenêtre temporelle\n",
    "\n",
    "**La clause _where_ permet de selectionner la fenêtre associée à la dernière heure**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"oha\", sf.unix_timestamp(sf.current_timestamp()) - int((60 + 2) * 60)) \\\n",
    "    .withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()) - int(60 * 2) / 2.) \\\n",
    "    .withColumn(\"wstart\", sf.unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", sf.unix_timestamp(\"window.end\")) \\\n",
    "    .where((sf.col(\"oha\") <= sf.col(\"wstart\")) & (sf.col(\"wend\") <= sf.col(\"now\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, _df_ constitue 'l'état' de référence de notre stream Spark. C'est à partir de cet état que l'on produit les résultats, métriques, indicateurs, ... demandés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation des étapes 01 à 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .select(\"station\", \"window\", \"nt\", \"awt\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "+--------+------------------------------------------+---+----+\n",
    "|station |window                                    |nt |awt |\n",
    "+--------+------------------------------------------+---+----+\n",
    "|87382473|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87386425|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87382259|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|19 |3.16|\n",
    "|87382457|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|18 |3.33|\n",
    "|87382440|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|20 |3.0 |\n",
    "|87382333|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87382887|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|12 |5.0 |\n",
    "|87334482|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|30 |2.0 |\n",
    "|87381137|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|48 |1.25|\n",
    "|87386318|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|17 |3.53|\n",
    "|87382374|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|12 |5.0 |\n",
    "|87382499|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|27 |2.22|\n",
    "|87382655|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|5  |12.0|\n",
    "|87382382|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|25 |2.4 |\n",
    "|87381905|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|31 |1.94|\n",
    "|87384008|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|16 |3.75|\n",
    "|87386003|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|19 |3.16|\n",
    "|87386300|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|24 |2.5 |\n",
    "|87381129|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|38 |1.58|\n",
    "|87386409|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|24 |2.5 |\n",
    "+--------+------------------------------------------+---+----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt de la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.2 à 1.6\n",
    "- Calcul du temps moyen d’attente sur la ligne station par station sur la dernière heure\n",
    "- Calcul du temps moyen d’attente globale sur la ligne sur la dernière heure\n",
    "- Trier les stations par temps d’attente moyen sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le plus élevée sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le moins élevée sur la dernière heure\n",
    "- Construire un tableau de bord dans Tableau Software sur la base de ces indicateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les calculs demandés nécessiteraient une seconde opération aggrégation sur le stream (last_hour_stream). Or, en l'état actuel de Spark (2.4), [il n'est pas possible d'enchainer plusieurs opération d'aggrégation sur un même stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations). Il nous faut donc trouver une solution de contournement.\n",
    "\n",
    "L'idée retenue est d'effectuer les calculs sur chaque batch de last_hour_stream. Cette approche fonctionne ici car chaque batch contient l'intégralité des données sur lesquelles les calculs doivent être réalisés pour produire les résultats attendus - i.e. les données d'attente moyenne par station. Ces résulats sont enregistrés sous _Hive_ dans des tables spécifiques. Ils sont ainsi rendus accéssibles depuis Tableau pour l'élaboration du tableau de bord. \n",
    "\n",
    "Les calculs (Q1.2 à Q.1.5) sont regroupés dans un callback du type _foreachBatch_ dont les appels sont déclenchés par une _StreamingQuery_.\n",
    "\n",
    "La classe _TransilienStreamProcessor_ implémente les fonctionnalités demandées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransilienStreamProcessor():\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __init__(self, config):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # store configuration \n",
    "        self.config = config\n",
    "        \n",
    "        # setup logger\n",
    "        self.logger = logging.getLogger(\"TSP\")\n",
    "        self.logger.setLevel(logging.DEBUG if self.config['verbose'] else logging.ERROR)\n",
    "        self.logger.debug(\"TSP: initializing...\")\n",
    "        \n",
    "        # kafka oriented spark session (configured to process incoming Kafka messages)\n",
    "        self.logger.debug(f\"TSP: creating Kafka-SparkSession\")\n",
    "        self.kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-STREAM\").getOrCreate()\n",
    "        self.kafka_session.conf.set(\"spark.sql.shuffle.partitions\", self.config['spark_sql_shuffle_partitions'])\n",
    "        self.logger.debug(f\"TSP: initializing Kafka stream\")\n",
    "        self.kafka_stream = self.setup_kafka_stream()\n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        \n",
    "        # hive oriented spark session (configured to save the results at the right place in Hive)\n",
    "        self.logger.debug(f\"TSP: creating Hive-SparkSession\")\n",
    "        self.hive_session = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"MS-SIO-HADOOP-PROJECT-PROCESS\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", self.config['hive_warehouse']) \\\n",
    "            .config(\"hive.metastore.uris\", self.config['hive_metastore']) \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate() \n",
    "        self.hive_session.conf.set(\"spark.sql.shuffle.partitions\", self.config['spark_sql_shuffle_partitions'])\n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        \n",
    "        # tables in which the produced data will be published\n",
    "        self.tables = [\n",
    "            # Q1.1 & Q1.3: (ordered) average waiting time in minutes per station (on the last hour period)\n",
    "            \"ordered_awt\", \n",
    "            # Q1.2: global average waiting time in minutes (on the last hour period) \n",
    "            \"global_awt\", \n",
    "            # Q1.4: min average waiting time in minutes (on the last hour period) \n",
    "            \"min_awt\", \n",
    "            # Q1.5: max average waiting time in minutes (on the last hour period) \n",
    "            \"max_awt\"\n",
    "        ]\n",
    "        \n",
    "        # check/create the database \n",
    "        self.__select_database()\n",
    "        \n",
    "        # drop existing tables (optional)\n",
    "        if self.config['drop_existing_tables']:\n",
    "            self.__drop_tables()\n",
    "            \n",
    "        # the 'save as table' streaming query - acts as a trigger for computeAndSaveAsTables\n",
    "        self.sat_query = None\n",
    "        \n",
    "        # the 'console' streaming query - print batches in the console  \n",
    "        self.csl_query = None\n",
    "        \n",
    "        # processing time (i.e. streaming queries trigger period)\n",
    "        self.processing_time = \"15 seconds\" #\"1 minutes\"\n",
    "        \n",
    "        # start the streaming query?\n",
    "        if self.config['auto_start']:\n",
    "            self.start()\n",
    "            \n",
    "        self.logger.debug(f\"initialization done!\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __select_database(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        self.logger.debug(f\"TSP: check '{config['hive_database']}' database exists (will be created otherwise)\")\n",
    "        self.hive_session.sql(f'create database if not exists {config[\"hive_database\"]} location \"{config[\"hive_warehouse\"]}\"')                  \n",
    "        self.logger.debug(f\"`-> done!\")           \n",
    "        self.logger.debug(f\"TSP: using '{config['hive_database']}' database\")\n",
    "        self.hive_session.sql(f'use  {config[\"hive_database\"]}')\n",
    "        self.logger.debug(f\"`-> done!\")           \n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __drop_tables(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # drop existing tables (best effort impl.)\n",
    "        for table in self.tables:\n",
    "            try:\n",
    "                self.logger.debug(f\"TSP: dropping table {table}\")\n",
    "                self.session.sql(f\"drop table transilien.{table}\")\n",
    "                self.logger.debug(f\"`-> done!\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def setup_kafka_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        wm = float(self.config['kafka_stream_watermark'])\n",
    "        wl = float(self.config['kafka_stream_window_length'])\n",
    "        si = float(self.config['kafka_stream_sliding_interval'])\n",
    "        oha_offset = int((wl + si) * 60.)\n",
    "        now_offset = int(60. * si / 2.)\n",
    "        # setup 'last hour stream'\n",
    "        return self.kafka_session \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", self.config['kafka_broker']) \\\n",
    "            .option(\"subscribe\", self.config['kafka_topic']) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load() \\\n",
    "            .select(sf.from_json(sf.col(\"value\").cast(\"string\"), self.config['json_schema'], self.config['json_options']).alias(\"departure\")) \\\n",
    "            .select(\"departure.*\") \\\n",
    "            .withWatermark(\"timestamp\", f\"{int(wm)} minutes\") \\\n",
    "            .dropDuplicates([\"train\", \"timestamp\"]) \\\n",
    "            .groupBy(\"station\", sf.window(\"timestamp\", f\"{int(wl)} minutes\", f\"{int(si)} minutes\")) \\\n",
    "            .agg(sf.count(\"train\").alias(\"nt\"), sf.format_number(wl / sf.count(\"train\"), 2).cast(\"double\").alias(\"awt\")) \\\n",
    "            .withColumn(\"oha\", sf.unix_timestamp(sf.current_timestamp()) - oha_offset) \\\n",
    "            .withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()) - now_offset) \\\n",
    "            .withColumn(\"wstart\", sf.unix_timestamp(\"window.start\")) \\\n",
    "            .withColumn(\"wend\", sf.unix_timestamp(\"window.end\")) \\\n",
    "            .where((sf.col(\"oha\") <= sf.col(\"wstart\")) & (sf.col(\"wend\") <= sf.col(\"now\"))) \\\n",
    "            .drop(\"oha\", \"now\", \"wstart\", \"wend\")\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming query if already running\n",
    "        self.stop()\n",
    "        # create then start the 'save' streaming query on the specified streaming dataframe\n",
    "        # also make 'self.computeAndSaveAsTables' member function the associated 'foreachBatch' callback\n",
    "        self.logger.debug(f\"TSP: starting 'SaveAsTables' streaming query\")\n",
    "        self.sat_query =  self.kafka_stream \\\n",
    "                            .writeStream \\\n",
    "                            .trigger(processingTime=self.processing_time) \\\n",
    "                            .foreachBatch(self.computeAndSaveAsTables) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .start() \n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        # create then start the 'console' streaming query on the specified streaming dataframe\n",
    "        self.logger.debug(f\"TSP: starting 'Console' streaming query\")\n",
    "        self.csl_query =  self.kafka_stream \\\n",
    "                            .orderBy(\"awt\") \\\n",
    "                            .writeStream \\\n",
    "                            .trigger(processingTime=self.processing_time) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .format(\"console\") \\\n",
    "                            .option(\"truncate\", False) \\\n",
    "                            .start() \n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        self.logger.debug(f\"TSP: streaming queries are running\")\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming query (best effort impl.)\n",
    "        if self.sat_query is  not None:\n",
    "            try:\n",
    "                self.logger.debug(f\"TSP: stopping 'SaveAsTables' streaming query\")\n",
    "                self.sat_query.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.sat_query = None\n",
    "                self.logger.debug(f\"`-> done!\")\n",
    "        if self.csl_query is  not None:\n",
    "            try:\n",
    "                self.logger.debug(f\"TSP: stopping 'Console' streaming query\")\n",
    "                self.csl_query.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.csl_query = None\n",
    "                self.logger.debug(f\"`-> done!\")\n",
    "       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def cleanup(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # cleanup the underlying session \n",
    "        # TODO: not sure this is the right way to do the job\n",
    "        self.logger.debug(f\"TSP: shutting down Kafka-SparkSession\")\n",
    "        self.kafka_session.stop()\n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "        self.logger.debug(f\"TSP: shutting down Hive-SparkSession\")\n",
    "        self.hive_session.sql(\"clear cache\")\n",
    "        self.hive_session.stop()\n",
    "        self.logger.debug(f\"`-> done!\")\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def turnVerboseOn(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # turn verbose on\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def turnVerboseOff(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "         # turn verbose off\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def computeAndSaveAsTables(self, batch, batch_number):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # the big trick... heart of the functionnality implemented by this class\n",
    "        try:\n",
    "            # be sure we have some data to handle (incoming dataframe not empty)\n",
    "            # this will avoid creating empty tables on Hive side \n",
    "            if batch.rdd.isEmpty():\n",
    "                self.logger.warning(f\"TSP: ignoring empty batch #{batch_number}\")\n",
    "                return\n",
    "            # compute requested data - don't change computation order (list order matters)\n",
    "            results = []\n",
    "            # Q1.1 & Q1.3: compute ordered average waiting time in minutes (on the last hour period)\n",
    "            results.append(batch.orderBy(sf.asc(\"awt\")).select(sf.col(\"station\"), sf.col(\"awt\")))\n",
    "            # Q1.2: compute global average waiting time in minutes (on the last hour period) \n",
    "            results.append(batch.agg(sf.count(\"station\").alias(\"number_of_stations\"), sf.avg(\"awt\").alias(\"global_awt\")))\n",
    "            # Q1.4: compute min average waiting time in minutes (on the last hour period)\n",
    "            results.append(batch.orderBy(sf.asc(\"awt\")).limit(1).select(sf.col(\"station\"), sf.col(\"awt\").alias(\"min_awt\")))\n",
    "            # Q1.5: compute max average waiting time in minutes (on the last hour period)\n",
    "            results.append(batch.orderBy(sf.desc(\"awt\")).limit(1).select(sf.col(\"station\"), sf.col(\"awt\").alias(\"max_awt\")))\n",
    "            # Q1.6: save results as tables so that Tableau Software can access the data\n",
    "            for dataframe, table in zip(results, self.tables):\n",
    "                if dataframe.rdd.isEmpty(): # avoid creating empty tables on Hive side\n",
    "                    continue\n",
    "                self.logger.debug(f\"saving transilien.{table} for batch #{batch_number}\")\n",
    "                if self.logger.getEffectiveLevel() == logging.DEBUG:\n",
    "                    dataframe.show(3, False)\n",
    "                t = time.time()\n",
    "                dataframe.write.mode(\"overwrite\").saveAsTable(f\"transilien.{table}\")\n",
    "                self.logger.debug(f\"`-> took {round(time.time() - t, 2)} s\" )\n",
    "            self.logger.debug(f\"computeAndSaveAsTables successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"TSP: failed to update Hive tables for batch #{batch_number}\")\n",
    "            self.logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour mémoire : dans un contexte de production, le code qui précède serait injecté dans un script Python lancé via _spark-submit_. Dans un tel cas, serait nécessaire placer un appel à _awaitTermination_ sur chaque streaming query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "config = {}\n",
    "\n",
    "# json schema & options for kafka messages deserialization \n",
    "config['json_schema'] = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True)\n",
    "    ]\n",
    ")\n",
    "config['json_options'] = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}\n",
    "\n",
    "# spark sesssions options\n",
    "config['spark_sql_shuffle_partitions'] = 4\n",
    "\n",
    "# kafka source configuration: broker & topic\n",
    "config['kafka_broker'] = \"sandbox-hdp.hortonworks.com:6667\"\n",
    "config['kafka_topic'] = \"transilien-02\"\n",
    "\n",
    "# kafka stream configuration: structured stream windiowing\n",
    "config['kafka_stream_watermark'] = 1 \n",
    "config['kafka_stream_window_length'] = 60\n",
    "config['kafka_stream_sliding_interval'] = 2\n",
    "\n",
    "# hive session configuration\n",
    "config['hive_database'] = \"transilien\"\n",
    "config['hive_warehouse'] = \"hdfs://sandbox-hdp.hortonworks.com:8020/api-transilien\"\n",
    "config['hive_metastore'] = \"thrift://sandbox-hdp.hortonworks.com:9083\"\n",
    "\n",
    "# misc. options\n",
    "config['drop_existing_tables'] = False \n",
    "config['auto_start'] = False \n",
    "config['verbose'] = True\n",
    "config['context'] = \"jupyspark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = TransilienStreamProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.turnVerboseOff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.turnVerboseOn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup...\n",
    "NB: the python kernel must be restart after a call to TransilienStreamProcessor.cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
