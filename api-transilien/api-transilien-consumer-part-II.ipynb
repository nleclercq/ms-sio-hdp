{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN - PARTIE II\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window as spark_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-STREAM-PART-II\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"transilien-02\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 512) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire de spécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- mode: string (nullable = true)\n",
    " |-- mission: string (nullable = true)\n",
    " |-- terminus: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les stations contigües A et B (cf. énoncé partie II) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contiguous_stations = {\n",
    "    'sa':87381129, # Station A: CLICHY LEVALLOIS\n",
    "    'sb':87381137  # Station B: ASNIERES SUR SEINE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre sur les 'contiguous_stations' et sur le 'mode' de l'horaire de départ de chaque train. On ne retient que les trains au départ des stations qui apparaissent dans la liste _contiguous_stations_ pour lesquels le mode de l'horaire annoncé vaut \"R\" (horaire réel)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.filter(sf.col(\"station\").isin(list(contiguous_stations.values()))).filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversion de l'heure de départ au format unix timestamp (plus simple à manipuler) - on supprime la colonne \"timestamp\", devenue inutile."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\")).drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection des trains dont l'heure de départ est se situe dans l'interval : maintenant +/- (time_window/2) exprimé dqns l'unité unix timestamp (i.e. la seconde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(sf.col(\"departure\").between(sf.unix_timestamp(sf.current_timestamp()) - int(time_window/2.), \n",
    "                                          sf.unix_timestamp(sf.current_timestamp()) + int(time_window/2.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo aggregation pour obtenir notre ensemble de trains en un _batch_ unique - l'idée est de pouvoir effectuer une requête en mode _complete_ sur notre stream. Il s'agit d'une astuce qui vise à satisfaire une contrainte imposée par Spark : le mode 'complete' ne s'appliquer qu'à des données aggrégées - i.e. issue d'une fonction d'aggrégration de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"train\", \"departure\", \"timestamp\", \"station\", \"mission\", \"terminus\").agg(sf.count(\"train\").alias(\"tmp\")).drop(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy(\"train\", \"departure\", \"timestamp\", \"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatchCallback(batch, batch_number):\n",
    "    if batch.rdd.isEmpty():\n",
    "        print(f\"ignoring empty batch #{batch_number}\")\n",
    "        return\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    # create next_departure & next_station lead columns: departure & station columns up shifted by 1 row\n",
    "    tmp = batch.withColumn('next_departure', sf.lead('departure').over(spark_window.orderBy(\"train\")))\n",
    "    tmp = tmp.withColumn('next_station', sf.lead('station').over(spark_window.orderBy(\"train\")))\n",
    "    \n",
    "    # create humanly readable columns for departure date/time \n",
    "    tmp = tmp.withColumn(\"departure_date\", sf.from_unixtime(tmp.departure))\n",
    "    tmp = tmp.withColumn(\"next_departure_date\", sf.from_unixtime(tmp.next_departure))\n",
    "    \n",
    "    # swap departure date/time (due to train direction) - this is just for readability & display \n",
    "    tmp = tmp.withColumn(\"temp_departure_date\", tmp.departure_date)\n",
    "    tmp = tmp.withColumn(\"departure_date\", sf.when(tmp.departure < tmp.next_departure, tmp.departure_date).otherwise(tmp.next_departure_date))\n",
    "    tmp = tmp.withColumn(\"next_departure_date\", sf.when(tmp.departure < tmp.next_departure, tmp.next_departure_date).otherwise(tmp.temp_departure_date))\n",
    "    tmp = tmp.drop(\"temp_departure_date\")\n",
    "    \n",
    "    tmp.show()\n",
    "    \n",
    "    # compute travel time between 'departure' and 'next_departure' - i.e. from one station to the next\n",
    "    tmp = tmp.withColumn(\"dt\", tmp.departure -  tmp.next_departure)\n",
    "    \n",
    "    # create column to store the current time (i.e. now)\n",
    "    tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "    \n",
    "    # the travel (from one station to the next) can belong to the past, the future or can be in progress \n",
    "    tmp = tmp.withColumn(\"in_past\", (tmp.now > tmp.departure) & (tmp.now > tmp.next_departure))\n",
    "    tmp = tmp.withColumn(\"in_future\", (tmp.now < tmp.departure) & (tmp.now < tmp.next_departure))\n",
    "    tmp = tmp.withColumn(\"in_progress\", (tmp.in_past != sf.lit(True)) & (tmp.in_future != sf.lit(True)))\n",
    "    \n",
    "    tmp.show()\n",
    "    \n",
    "    # keep only 'in progress' travels - i.e. the ones not in past nor in the future\n",
    "    # we also remove standby (i.e fake travel from one station to the same - train waiting for next departure)\n",
    "    tmp = tmp.filter((~tmp.in_past & ~tmp.in_future) & (tmp.station != tmp.next_station))\n",
    "    \n",
    "    # order by departure time\n",
    "    tmp = tmp.orderBy(\"departure\") \n",
    "    \n",
    "    tmp.show()\n",
    "    \n",
    "    # keep only one travel per train - i.e. the one to the next station \n",
    "    tmp = tmp.dropDuplicates([\"train\"])\n",
    "    \n",
    "    # compute travel progression in %\n",
    "    tmp = tmp.withColumn(\"progress\", (100. * sf.abs((tmp.now - tmp.departure))) / sf.abs(tmp.dt))  \n",
    "    # compute trains progression: maintain value in  the [O, 100]% range \n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress < sf.lit(0.), sf.lit(0.)).otherwise(tmp.progress))             \n",
    "    # compute trains progression: maintain value in  the [O, 100]% range \n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress > sf.lit(100.), sf.lit(100.)).otherwise(tmp.progress))\n",
    "   \n",
    "    # compute progress bar value than will be displayed in Tableau Software (this is a trick to display travel direction)            \n",
    "    tmp = tmp.withColumn(\"progress_bar_value\", sf.when(tmp.in_progress, sf.when(tmp.dt > sf.lit(0.), tmp.progress).otherwise(100. - tmp.progress)))\n",
    "   \n",
    "    # round progress values to 1 digit\n",
    "    tmp = tmp.withColumn(\"progress\", sf.format_number(tmp.progress, 1).cast(\"double\"))\n",
    "    tmp = tmp.withColumn(\"progress_bar_value\", sf.format_number(tmp.progress_bar_value, 1).cast(\"double\"))\n",
    "    \n",
    "    # select the required columns\n",
    "    tmp = tmp.select(\"train\", \"departure_date\", \"next_departure_date\", \"station\", \"next_station\", \"in_past\", \"in_future\", \"progress\", \"progress_bar_value\") \n",
    "    \n",
    "    # re-order by departure time\n",
    "    tmp = tmp.orderBy(\"departure\") \n",
    "    \n",
    "    tmp.show()\n",
    "    \n",
    "    #kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"train_progression\")\n",
    "    print(f\"`-> took {round(time.time() - t, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(forEachBatchCallback) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
