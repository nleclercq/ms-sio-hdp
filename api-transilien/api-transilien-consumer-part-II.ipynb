{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN - PARTIE II\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window as spark_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-STREAM-PART-II\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column\n",
    "https://databricks.com/blog/2018/11/01/sql-pivot-converting-rows-to-columns.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_session.createDataFrame([\n",
    "    (\"s1\", \"t1\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s1\", \"t2\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s1\", \"t3\", int(time.time() - random.randint(300,1800)), \"E\"),\n",
    "    (\"s2\", \"t0\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s2\", \"t2\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s2\", \"t1\", int(time.time() - random.randint(300,1800)), \"R\"), \n",
    "    (\"s2\", \"t3\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s3\", \"t1\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s3\", \"t2\", int(time.time() - random.randint(300,1800)), \"R\")], \n",
    "    (\"station\", \"train\", \"departure-time\", \"mode\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------+----+\n",
      "|station|train|departure-time|mode|\n",
      "+-------+-----+--------------+----+\n",
      "|     s1|   t1|    1551639394|   R|\n",
      "|     s1|   t2|    1551640325|   R|\n",
      "|     s1|   t3|    1551640488|   E|\n",
      "|     s2|   t0|    1551639332|   R|\n",
      "|     s2|   t2|    1551639094|   R|\n",
      "|     s2|   t1|    1551640090|   R|\n",
      "|     s2|   t3|    1551639064|   R|\n",
      "|     s3|   t1|    1551639993|   R|\n",
      "|     s3|   t2|    1551640088|   R|\n",
      "+-------+-----+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_df = df.groupby('train').pivot('station').max('departure-time').fillna(0).orderBy('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+----------+\n",
      "|train|        s1|        s2|        s3|\n",
      "+-----+----------+----------+----------+\n",
      "|   t0|         0|1551639332|         0|\n",
      "|   t1|1551639394|1551640090|1551639993|\n",
      "|   t2|1551640325|1551639094|1551640088|\n",
      "|   t3|1551640488|1551639064|         0|\n",
      "+-----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reshaped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"transilien-02\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 512) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire de spécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- mode: string (nullable = true)\n",
    " |-- mission: string (nullable = true)\n",
    " |-- terminus: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les stations contigües A et B (cf. énoncé partie II) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contiguous_stations = {\n",
    "    'sa':87381129, # Station A: CLICHY LEVALLOIS\n",
    "    'sb':87381137  # Station B: ASNIERES SUR SEINE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre sur les 'contiguous_stations' et sur le 'mode' de l'horaire de départ de chaque train. On ne retient que les trains au départ des stations qui apparaissent dans la liste _contiguous_stations_ pour lesquels le mode de l'horaire annoncé vaut \"R\" (horaire réel)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.filter(sf.col(\"station\").isin(list(contiguous_stations.values()))).filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversion de l'heure de départ au format unix timestamp (plus simple à manipuler) - on supprime la colonne \"timestamp\", devenue inutile."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\")).drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection des trains dont l'heure de départ est se situe dans l'interval : maintenant +/- (time_window/2) exprimé dqns l'unité unix timestamp (i.e. la seconde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(sf.col(\"departure\").between(sf.unix_timestamp(sf.current_timestamp()) - int(time_window/2.), \n",
    "                                          sf.unix_timestamp(sf.current_timestamp()) + int(time_window/2.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo aggregation pour obtenir notre ensemble de trains en un _batch_ unique - l'idée est de pouvoir effectuer une requête en mode _complete_ sur notre stream. Il s'agit d'une astuce qui vise à satisfaire une contrainte imposée par Spark : le mode 'complete' ne s'appliquer qu'à des données aggrégées - i.e. issue d'une fonction d'aggrégration de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"train\", \"departure\", \"timestamp\", \"station\", \"mission\", \"terminus\").agg(sf.count(\"train\").alias(\"tmp\")).drop(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy(\"train\", \"departure\", \"timestamp\", \"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatchCallback(batch, batch_number):\n",
    "    if batch.rdd.isEmpty():\n",
    "        print(f\"ignoring empty batch #{batch_number}\")\n",
    "        return\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    tmp = batch.withColumn('next_departure', sf.lead('departure').over(spark_window.orderBy(\"train\")))\n",
    "    tmp = tmp.withColumn('next_station', sf.lead('station').over(spark_window.orderBy(\"train\")))\n",
    "    tmp = tmp.withColumn(\"departure_date\", sf.from_unixtime(sf.col(\"departure\")))\n",
    "    tmp = tmp.withColumn(\"next_departure_date\", sf.from_unixtime(sf.col(\"next_departure\")))\n",
    "    \n",
    "    #tmp.show()\n",
    "    \n",
    "    tmp = tmp.withColumn(\"dt\", tmp.departure -  tmp.next_departure)\n",
    "    tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "    tmp = tmp.withColumn(\"in_past\", (tmp.now > tmp.departure) & (tmp.now > tmp.next_departure))\n",
    "    tmp = tmp.withColumn(\"in_future\", (tmp.now < tmp.departure) & (tmp.now < tmp.next_departure))\n",
    "    tmp = tmp.withColumn(\"in_progress\", (tmp.in_past != sf.lit(True)) & (tmp.in_future != sf.lit(True)))\n",
    "    tmp = tmp.filter(~tmp.in_past & ~tmp.in_future)\n",
    "    \n",
    "    tmp = tmp.withColumn(\"progress\", (100. * sf.abs((tmp.now - tmp.departure))) / sf.abs(tmp.dt))  \n",
    "    # compute trains progression: maintain value in  the [O, 100]% range \n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress < sf.lit(0.), sf.lit(0.)).otherwise(tmp.progress))             \n",
    "    # compute trains progression: maintain value in  the [O, 100]% range \n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress > sf.lit(100.), sf.lit(100.)).otherwise(tmp.progress))\n",
    "    # compute progress bar value than will be displayed in Tableau Software (this is a trick to display travel direction)            \n",
    "    tmp = tmp.withColumn(\"progress_bar_value\", sf.when(tmp.in_progress, sf.when(tmp.dt > sf.lit(0.), tmp.progress).otherwise(100. - tmp.progress)))\n",
    "    # round progress values to 1 digit\n",
    "    tmp = tmp.withColumn(\"progress\", sf.format_number(tmp.progress, 1).cast(\"double\"))\n",
    "    tmp = tmp.withColumn(\"progress_bar_value\", sf.format_number(tmp.progress_bar_value, 1).cast(\"double\"))\n",
    "    #tmp = tmp.select(\"train\", \"departure_date\", \"next_departure_date\", \"station\", \"next_station\", \"dt\", \"in_past\", \"in_future\", \"in_progress\", \"progress\", \"progress_bar_value\")\n",
    "    tmp = tmp.select(\"train\", \"departure_date\", \"next_departure_date\", \"station\", \"next_station\", \"in_past\", \"in_future\", \"progress\", \"progress_bar_value\")\n",
    "    tmp = tmp.orderBy(sf.desc(\"progress\")) \n",
    "    \n",
    "    tmp.show()\n",
    "    \n",
    "    #kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"train_progression\")\n",
    "    print(f\"`-> took {round(time.time() - t, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "| train|     departure_date|next_departure_date| station|next_station|in_past|in_future|progress|progress_bar_value|\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "|133806|2019-03-03 20:53:00|2019-03-03 20:42:00|87381111|    87382861|  false|    false|    90.6|              90.6|\n",
      "|137708|2019-03-03 20:29:00|2019-03-03 20:45:00|87381137|    87381137|  false|    false|    87.7|              12.3|\n",
      "|134747|2019-03-03 20:57:00|2019-03-03 20:36:00|87382218|    87381459|  false|    false|    66.5|              66.5|\n",
      "|133697|2019-03-03 20:55:00|2019-03-03 20:35:00|87382879|    87384008|  false|    false|    59.8|              59.8|\n",
      "|133834|2019-03-03 20:57:00|2019-03-03 20:33:00|87382366|    87382358|  false|    false|    58.2|              58.2|\n",
      "|UPAL97|2019-03-03 20:58:00|2019-03-03 20:30:00|87381459|    87386409|  false|    false|    53.5|              53.5|\n",
      "|133834|2019-03-03 20:42:00|2019-03-03 20:44:00|87382861|    87382879|  false|    false|    51.7|              48.3|\n",
      "|133697|2019-03-03 20:42:00|2019-03-03 20:44:00|87382366|    87382358|  false|    false|    51.7|              48.3|\n",
      "|133705|2019-03-03 20:57:00|2019-03-03 20:28:00|87381137|    87382341|  false|    false|    48.2|              48.2|\n",
      "|132629|2019-03-03 20:50:00|2019-03-03 20:34:00|87366922|    87382804|  false|    false|    43.5|              43.5|\n",
      "|134741|2019-03-03 20:42:00|2019-03-03 20:45:00|87382432|    87382440|  false|    false|    34.4|              65.6|\n",
      "|133701|2019-03-03 20:42:00|2019-03-03 20:45:00|87381137|    87382002|  false|    false|    34.4|              65.6|\n",
      "|137710|2019-03-03 20:45:00|2019-03-03 20:28:00|87381137|    87381657|  false|    false|    11.6|              11.6|\n",
      "|133806|2019-03-03 20:43:00|2019-03-03 20:44:00|87382200|    87382200|  false|    false|     3.3|              96.7|\n",
      "|UPAL93|2019-03-03 20:43:00|2019-03-03 20:44:00|87381905|    87381905|  false|    false|     3.3|              96.7|\n",
      "|132629|2019-03-03 20:43:00|2019-03-03 20:48:00|87382481|    87382812|  false|    false|     0.7|              99.3|\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "\n",
      "`-> took 0.46 s\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "| train|     departure_date|next_departure_date| station|next_station|in_past|in_future|progress|progress_bar_value|\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "|133697|2019-03-03 20:42:00|2019-03-03 20:44:00|87382366|    87382358|  false|    false|    96.7|               3.3|\n",
      "|133834|2019-03-03 20:42:00|2019-03-03 20:44:00|87382861|    87382879|  false|    false|    96.7|               3.3|\n",
      "|137708|2019-03-03 20:29:00|2019-03-03 20:45:00|87381137|    87381137|  false|    false|    93.3|               6.7|\n",
      "|UPAL93|2019-03-03 20:43:00|2019-03-03 20:44:00|87381905|    87381905|  false|    false|    93.3|               6.7|\n",
      "|133806|2019-03-03 20:43:00|2019-03-03 20:44:00|87382200|    87382200|  false|    false|    93.3|               6.7|\n",
      "|133806|2019-03-03 20:53:00|2019-03-03 20:42:00|87381111|    87382861|  false|    false|    82.4|              82.4|\n",
      "|134741|2019-03-03 20:42:00|2019-03-03 20:45:00|87382432|    87382440|  false|    false|    64.4|              35.6|\n",
      "|133701|2019-03-03 20:42:00|2019-03-03 20:45:00|87381137|    87382002|  false|    false|    64.4|              35.6|\n",
      "|134747|2019-03-03 20:57:00|2019-03-03 20:36:00|87382218|    87381459|  false|    false|    62.2|              62.2|\n",
      "|133697|2019-03-03 20:55:00|2019-03-03 20:35:00|87382879|    87384008|  false|    false|    55.3|              55.3|\n",
      "|133834|2019-03-03 20:57:00|2019-03-03 20:33:00|87382366|    87382358|  false|    false|    54.4|              54.4|\n",
      "|UPAL97|2019-03-03 20:58:00|2019-03-03 20:30:00|87381459|    87386409|  false|    false|    50.2|              50.2|\n",
      "|133705|2019-03-03 20:57:00|2019-03-03 20:28:00|87381137|    87382341|  false|    false|    45.1|              45.1|\n",
      "|132629|2019-03-03 20:50:00|2019-03-03 20:34:00|87366922|    87382804|  false|    false|    37.9|              37.9|\n",
      "|132629|2019-03-03 20:43:00|2019-03-03 20:48:00|87382481|    87382812|  false|    false|    18.7|              81.3|\n",
      "|137710|2019-03-03 20:45:00|2019-03-03 20:28:00|87381137|    87381657|  false|    false|     6.3|               6.3|\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "\n",
      "`-> took 0.37 s\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "| train|     departure_date|next_departure_date| station|next_station|in_past|in_future|progress|progress_bar_value|\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "|133697|2019-03-03 20:42:00|2019-03-03 20:44:00|87382366|    87382358|  false|    false|    98.3|               1.7|\n",
      "|133834|2019-03-03 20:42:00|2019-03-03 20:44:00|87382861|    87382879|  false|    false|    98.3|               1.7|\n",
      "|133806|2019-03-03 20:43:00|2019-03-03 20:44:00|87382200|    87382200|  false|    false|    96.7|               3.3|\n",
      "|UPAL93|2019-03-03 20:43:00|2019-03-03 20:44:00|87381905|    87381905|  false|    false|    96.7|               3.3|\n",
      "|137708|2019-03-03 20:29:00|2019-03-03 20:45:00|87381137|    87381137|  false|    false|    93.5|               6.5|\n",
      "|133806|2019-03-03 20:53:00|2019-03-03 20:42:00|87381111|    87382861|  false|    false|    82.1|              82.1|\n",
      "|134741|2019-03-03 20:42:00|2019-03-03 20:45:00|87382432|    87382440|  false|    false|    65.6|              34.4|\n",
      "|133701|2019-03-03 20:42:00|2019-03-03 20:45:00|87381137|    87382002|  false|    false|    65.6|              34.4|\n",
      "|134747|2019-03-03 20:57:00|2019-03-03 20:36:00|87382218|    87381459|  false|    false|    62.1|              62.1|\n",
      "|133697|2019-03-03 20:55:00|2019-03-03 20:35:00|87382879|    87384008|  false|    false|    55.2|              55.2|\n",
      "|133834|2019-03-03 20:57:00|2019-03-03 20:33:00|87382366|    87382358|  false|    false|    54.3|              54.3|\n",
      "|UPAL97|2019-03-03 20:58:00|2019-03-03 20:30:00|87381459|    87386409|  false|    false|    50.1|              50.1|\n",
      "|133705|2019-03-03 20:57:00|2019-03-03 20:28:00|87381137|    87382341|  false|    false|    44.9|              44.9|\n",
      "|132629|2019-03-03 20:50:00|2019-03-03 20:34:00|87366922|    87382804|  false|    false|    37.7|              37.7|\n",
      "|132629|2019-03-03 20:43:00|2019-03-03 20:48:00|87382481|    87382812|  false|    false|    19.3|              80.7|\n",
      "|137710|2019-03-03 20:45:00|2019-03-03 20:28:00|87381137|    87381657|  false|    false|     6.1|               6.1|\n",
      "+------+-------------------+-------------------+--------+------------+-------+---------+--------+------------------+\n",
      "\n",
      "`-> took 0.44 s\n"
     ]
    }
   ],
   "source": [
    "query_2 = df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(forEachBatchCallback) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt de la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.streams.active"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def forEachBatchCallback(batch, batch_number):\n",
    "    if batch.rdd.isEmpty():\n",
    "        print(f\"ignoring empty batch #{batch_number}\")\n",
    "        return\n",
    "    t = time.time()\n",
    "    tmp = batch.groupby(\"train\").pivot(\"station\").max(\"departure\").fillna(0).orderBy(\"train\")\n",
    "    tmp = tmp.select(\"train\", sf.col(str(contiguous_stations['sa'])).alias(\"sa\"), sf.col(str(contiguous_stations['sb'])).alias(\"sb\"))\n",
    "    tmp = tmp.filter((sf.col(\"sa\") > 0) & (sf.col(\"sb\") > 0))\n",
    "    #tmp.show()\n",
    "    tmp = tmp.withColumn(\"sa-dpt\", sf.from_unixtime(sf.col(\"sa\")))\n",
    "    tmp = tmp.withColumn(\"sb-dpt\", sf.from_unixtime(sf.col(\"sb\")))\n",
    "    tmp = tmp.withColumn(\"dt\", sf.col(\"sb\") - sf.col(\"sa\"))\n",
    "    tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "    tmp = tmp.withColumn(\"in-past\", (tmp.now > tmp.sa) & (tmp.now > tmp.sb))\n",
    "    tmp = tmp.withColumn(\"in-future\", (tmp.now < tmp.sa) & (tmp.now < tmp.sb))\n",
    "    tmp = tmp.withColumn(\"in-progress\", (sf.col(\"in-past\") != sf.lit(True)) & (sf.col(\"in-future\") != sf.lit(True)))\n",
    "    #tmp = tmp.filter(sf.col(\"in-progress\") | sf.col(\"in-future\"))\n",
    "    tmp = tmp.withColumn(\"departure-uxts\", sf.when(sf.col(\"dt\") > sf.lit(0), sf.col(\"sa\")).otherwise(sf.col(\"sb\")))\n",
    "    tmp = tmp.withColumn(\"departure-date\", sf.when(sf.col(\"dt\") > sf.lit(0), sf.from_unixtime(sf.col(\"sa\"))).otherwise(sf.from_unixtime(sf.col(\"sb\"))))\n",
    "    #tmp = tmp.withColumn(\"now - departure\", sf.col(\"now\") - sf.col(\"departure\"))\n",
    "    tmp = tmp.orderBy(sf.col(\"departure-uxts\"))\n",
    "    #tmp.show()\n",
    "    tmp = tmp.withColumn(\"progress\", sf.format_number((100. * (sf.col(\"now\") - sf.col(\"departure-uxts\"))) / sf.abs(sf.col(\"dt\")), 1).cast(\"double\"))\n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(sf.col(\"progress\") < sf.lit(0.), sf.lit(0.)).otherwise(sf.col(\"progress\")))\n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(sf.col(\"progress\") > sf.lit(100.), sf.lit(100.)).otherwise(sf.col(\"progress\")))\n",
    "    tmp = tmp.withColumn(\"direction\", sf.when(sf.col(\"dt\") > sf.lit(0.), sf.lit(1)).otherwise(sf.lit(-1)))\n",
    "    tmp = tmp.withColumn(\"progress_bar_value\", sf.when(sf.col(\"in-progress\"), sf.when(sf.col(\"dt\") > sf.lit(0.), sf.col(\"progress\")).otherwise(100. - sf.col(\"progress\"))).otherwise(sf.col(\"progress\")))\n",
    "    tmp = tmp.withColumn(\"progress_bar_value\", sf.format_number(sf.col(\"progress_bar_value\"), 1).cast(\"double\"))\n",
    "    tmp = tmp.join(batch, \"train\", how=\"left\")\n",
    "    tmp = tmp.dropDuplicates([\"train\"])\n",
    "    tmp = tmp.orderBy(sf.col(\"departure-uxts\"))\n",
    "    tmp = tmp.select(\"train\", sf.col(\"departure-date\").alias(\"departure\"), \"station\", \"mission\", \"progress\", \"direction\", \"progress_bar_value\", \"in-past\", \"in-future\", \"in-progress\")\n",
    "    tmp.show()\n",
    "    kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"train_progression\")\n",
    "    print(f\"`-> took {round(time.time() - t, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
