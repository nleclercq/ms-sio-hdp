{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN - PARTIE II\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window as spark_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-STREAM-PART-II\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "                .option(\"subscribe\", \"transilien-02\") \\\n",
    "                .option(\"startingOffsets\", \"earliest\") \\\n",
    "                .option(\"kafkaConsumer.pollTimeoutMs\", 512) \\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données associées auxw stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_data = kafka_session \\\n",
    "                .read \\\n",
    "                .format(\"csv\") \\\n",
    "                .option(\"sep\", \",\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(\"file:/root/ms-sio-hdp/api-transilien/transilien_line_l_stations_by_code.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+-------------+\n",
      "| station|               label|     latitude|    longitude|\n",
      "+--------+--------------------+-------------+-------------+\n",
      "|87334482| NEUVILLE UNIVERSITE|49.0141135729|2.07888615344|\n",
      "|87366922|SAINT-GERMAIN EN ...|48.8950238658|2.07194236158|\n",
      "|87381111|       PONT CARDINET|48.8875699026|2.31401853193|\n",
      "|87381129|    CLICHY LEVALLOIS|48.8969214742|2.29837992753|\n",
      "|87381137|  ASNIERES SUR SEINE|48.9057768994|2.28332241877|\n",
      "|87381459| CONFLANS FIN D'OISE|48.9891848652|2.07455911528|\n",
      "|87381657|       ACHERES VILLE|48.9700946335|2.07739969014|\n",
      "|87381905|    CERGY PREFECTURE|49.0365179881|2.07971720177|\n",
      "|87382002|  BECON LES BRUYERES|48.9055805862|2.26857173893|\n",
      "|87382200|          COURBEVOIE|48.8982514361|2.24797129852|\n",
      "|87382218|          LA DEFENSE|48.8934895769|2.23827508094|\n",
      "|87382259|GARCHES MARNES LA...|48.8383162045| 2.1868735901|\n",
      "|87382267|          VAUCRESSON| 48.836784962|2.15255479028|\n",
      "|87382333|CHAVILLE RIVE DROITE| 48.812136312|2.18783398094|\n",
      "|87382341|SEVRES VILLE D'AVRAY|48.8274217325|2.20055846234|\n",
      "|87382358|         SAINT-CLOUD|48.8443540357|2.21690859829|\n",
      "|87382366|         LE VAL D'OR|48.8569094152|2.21677050352|\n",
      "|87382374|SURESNES MONT VAL...|48.8717069629|2.22110944905|\n",
      "|87382382|             PUTEAUX|48.8834917758| 2.2335382863|\n",
      "|87382432|LA CELLE SAINT-CLOUD|48.8425404524| 2.1379646985|\n",
      "+--------+--------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire de spécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- mode: string (nullable = true)\n",
    " |-- mission: string (nullable = true)\n",
    " |-- terminus: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les stations contigües A et B (cf. énoncé partie II) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "contiguous_stations = {\n",
    "    'sa':87381129, # Station A: CLICHY LEVALLOIS\n",
    "    'sb':87381137  # Station B: ASNIERES SUR SEINE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre sur les 'contiguous_stations' et sur le 'mode' de l'horaire de départ de chaque train. On ne retient que les trains au départ des stations qui apparaissent dans la liste _contiguous_stations_ pour lesquels le mode de l'horaire annoncé vaut \"R\" (horaire réel)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.filter(sf.col(\"station\").isin(list(contiguous_stations.values()))).filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversion de l'heure de départ au format unix timestamp (plus simple à manipuler) - on supprime la colonne \"timestamp\", devenue inutile."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\")).drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection des trains dont l'heure de départ est se situe dans l'interval : maintenant +/- (time_window/2) exprimé dqns l'unité unix timestamp (i.e. la seconde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: integer (nullable = true)\n",
      " |-- train: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- mission: string (nullable = true)\n",
      " |-- terminus: integer (nullable = true)\n",
      " |-- departure: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(sf.col(\"departure\").between(sf.unix_timestamp(sf.current_timestamp()) - int(time_window/2.), \n",
    "                                          sf.unix_timestamp(sf.current_timestamp()) + int(time_window/2.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo aggregation pour obtenir notre ensemble de trains en un _batch_ unique - l'idée est de pouvoir effectuer une requête en mode _complete_ sur notre stream. Il s'agit d'une astuce qui vise à satisfaire une contrainte imposée par Spark : le mode 'complete' ne s'appliquer qu'à des données aggrégées - i.e. issue d'une fonction d'aggrégration de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"train\", \"departure\", \"timestamp\", \"station\", \"mission\", \"terminus\").agg(sf.count(\"train\").alias(\"tmp\")).drop(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train: string (nullable = true)\n",
      " |-- departure: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- station: integer (nullable = true)\n",
      " |-- mission: string (nullable = true)\n",
      " |-- terminus: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.orderBy(\"train\", \"departure\", \"timestamp\", \"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train: string (nullable = true)\n",
      " |-- departure: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- station: integer (nullable = true)\n",
      " |-- mission: string (nullable = true)\n",
      " |-- terminus: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste des missions vers Paris pour distinction de la direction des trains : Paris -> Banlieue vs Banlieue -> Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "missions_to_paris = [\"PALS\", \"PASA\", \"PEBU\", \"PEGE\", \"POPI\", \"POPU\", \"POSA\", \"POVA\", \"POPE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback de traitement des batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatchCallback(batch, batch_number):\n",
    "\n",
    "    if batch.rdd.isEmpty():\n",
    "        print(f\"ignoring empty batch #{batch_number}\")\n",
    "        return\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    # clear cell content so that we don't cumulate the log               \n",
    "    # self.clear_output()\n",
    "\n",
    "    # be sure we have some data to handle (incoming dataframe not empty)\n",
    "    # this will avoid creating empty tables on Hive side \n",
    "    if batch.rdd.isEmpty():\n",
    "        print(f\"forEachBatchCallback: ignoring empty batch #{batch_number}\")\n",
    "        return\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    # create next_departure & next_station lead columns: departure & station columns up shifted by 1 row\n",
    "    tmp = batch.withColumn('next_departure', sf.lead('departure').over(spark_window.partitionBy(\"train\").orderBy(\"departure\")))\n",
    "    tmp = tmp.withColumn('next_station', sf.lead('station').over(spark_window.partitionBy(\"train\").orderBy(\"departure\")))\n",
    "\n",
    "    # create humanly readable columns for departure date/time \n",
    "    tmp = tmp.withColumn(\"departure_date\", sf.from_unixtime(tmp.departure, \"HH:mm:ss\"))\n",
    "    tmp = tmp.withColumn(\"next_departure_date\", sf.from_unixtime(tmp.next_departure, \"HH:mm:ss\"))\n",
    "\n",
    "    # compute travel time between 'departure' and 'next_departure' - i.e. from one station to the next\n",
    "    tmp = tmp.withColumn(\"time_to_st\", tmp.next_departure -  tmp.departure)\n",
    "\n",
    "    # travel direction: 1:paris -> banlieue, -1:banlieue->paris\n",
    "    tmp = tmp.withColumn(\"direction\", sf.when(tmp.mission.isin(missions_to_paris), sf.lit(1)).otherwise(sf.lit(-1)))\n",
    "\n",
    "    # swap departure date/time (due to train direction) - this is just for readability & display \n",
    "    tmp = tmp.withColumn(\"temp_departure_date\", tmp.departure_date)\n",
    "    tmp = tmp.withColumn(\"departure_date\", sf.when(tmp.departure < tmp.next_departure, tmp.departure_date).otherwise(tmp.next_departure_date))\n",
    "    tmp = tmp.withColumn(\"next_departure_date\", sf.when(tmp.departure < tmp.next_departure, tmp.next_departure_date).otherwise(tmp.temp_departure_date))\n",
    "    tmp = tmp.drop(\"temp_departure_date\")\n",
    "\n",
    "    # tmp.show()\n",
    "\n",
    "    # create column to store the current time (i.e. now)\n",
    "    tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "\n",
    "    # the travel (from one station to the next) can belong to the past, the future or can be in progress \n",
    "    tmp = tmp.withColumn(\"in_past\", (tmp.now > tmp.departure) & (tmp.now > tmp.next_departure))\n",
    "    tmp = tmp.withColumn(\"in_future\", (tmp.now < tmp.departure) & (tmp.now < tmp.next_departure))\n",
    "    tmp = tmp.withColumn(\"in_progress\", (tmp.in_past != sf.lit(True)) & (tmp.in_future != sf.lit(True)))\n",
    "\n",
    "    # tmp.show()\n",
    "\n",
    "    # keep only 'in progress' travels - i.e. the ones not in past nor in the future\n",
    "    # we also remove: \n",
    "    #    - trains in standby (i.e fake travel from one station to the same - train waiting for next departure)\n",
    "    #    - rows for which next_departure is null (introduced by the lead function)\n",
    "    tmp = tmp.filter((~tmp.in_past & ~tmp.in_future) & (tmp.station != tmp.next_station) & (tmp.next_departure.isNotNull()))\n",
    "\n",
    "    # tmp.show()\n",
    "\n",
    "    # compute travel progression in %\n",
    "    tmp = tmp.withColumn(\"progress\", (100. * sf.abs((tmp.now - tmp.departure))) / sf.abs(tmp.time_to_st))  \n",
    "    # compute trains progression: maintain value in  the [O, 100]% range \n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress < sf.lit(0.), sf.lit(0.)).otherwise(tmp.progress))             \n",
    "    # compute trains progression: maintain value in  the [O, 100]% range \n",
    "    tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress > sf.lit(100.), sf.lit(100.)).otherwise(tmp.progress))\n",
    "\n",
    "    # tmp.show()\n",
    "\n",
    "    # select the required columns\n",
    "    tmp = tmp.select(tmp.train, \n",
    "                     tmp.departure_date.alias(\"departure\"),\n",
    "                     tmp.next_departure_date.alias(\"arrival\"),\n",
    "                     tmp.mission, \n",
    "                     tmp.station.alias(\"from_st\"), \n",
    "                     tmp.next_station.alias(\"to_st\"), \n",
    "                     tmp.time_to_st,\n",
    "                     tmp.progress,\n",
    "                     tmp.direction) \n",
    "\n",
    "    # from (departure location)\n",
    "    tmp = tmp.join(self.stations_data, tmp.from_st == self.stations_data.station, how=\"left\")\n",
    "    tmp = tmp.withColumn(\"from_st_lt\", tmp.latitude).drop(\"latitude\")\n",
    "    tmp = tmp.withColumn(\"from_st_lg\", tmp.longitude).drop(\"longitude\")\n",
    "    tmp = tmp.withColumn(\"from_st_lb\", tmp.label).drop(\"label\")\n",
    "    tmp = tmp.drop(\"station\")\n",
    "\n",
    "    # to (destination location) \n",
    "    tmp = tmp.join(self.stations_data, tmp.to_st == self.stations_data.station, how=\"left\")\n",
    "    tmp = tmp.withColumn(\"to_st_lt\", tmp.latitude).drop(\"latitude\")\n",
    "    tmp = tmp.withColumn(\"to_st_lg\", tmp.longitude).drop(\"longitude\")\n",
    "    tmp = tmp.withColumn(\"to_st_lb\", tmp.label).drop(\"label\")\n",
    "    tmp = tmp.drop(\"station\")\n",
    "\n",
    "    # compute current train latitude & longitude\n",
    "    tmp = tmp.withColumn(\"train_lt\", tmp.from_st_lt + ((tmp.progress / 100.) * (tmp.to_st_lt - tmp.from_st_lt)))\n",
    "    tmp = tmp.withColumn(\"train_lg\", tmp.from_st_lg + ((tmp.progress / 100.) * (tmp.to_st_lg - tmp.from_st_lg)))\n",
    "\n",
    "    # cast progress to int\n",
    "    tmp = tmp.withColumn(\"progress\", sf.format_number(tmp.progress, 1).cast(\"double\"))\n",
    "\n",
    "    # remove tmp data from table\n",
    "    tmp = tmp.select(\"train\",       # train identifier \n",
    "                     \"departure\",   # departure time\n",
    "                     \"arrival\",     # arrival time\n",
    "                     \"mission\",     # mission code\n",
    "                     \"from_st\",     # departure station code\n",
    "                     \"to_st\",       # arrival station code\n",
    "                     \"from_st_lb\",  # departure station label\n",
    "                     \"to_st_lb\",    # arrival station label\n",
    "                     \"time_to_st\",  # time_to_st = arrival - departure in seconds\n",
    "                     \"progress\",    # travel progress\n",
    "                     \"direction\",   # 1: paris -> banlieue, -1: banlieue->paris\n",
    "                     \"train_lt\",    # current train location: latitude \n",
    "                     \"train_lg\")    # current train location: longitude \n",
    "              \n",
    "    tmp.show()\n",
    "                       \n",
    "    print(f\"`-> took {round(time.time() - t, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(forEachBatchCallback) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
