{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET HADOOP/SPARK - MS-SIO-2019 \n",
    "## CONSOMMATEUR FLUX KAFKA\n",
    "---\n",
    "### P.Hamy, N.Leclercq, L.Poncet\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ce notebook Jupyter implémente la partie _consommateur_ du flux Kafka dans lequel sont injectées les données issues de l'API Transilien.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des packages Python requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window as spark_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports de nos outils locaux:\n",
    "- NotebookCellContent permet d'attacher la \"cellule courante\" du notebook Jupyter à l'objet qui hérite de cette classe. Le principal intérêt est de router le logging asynchrone vers cette cellule et ce quelle que soit la cellule active (i.e. quelle que soit la cellule dans laquelle l'utilisateur travaille). Le lien entre l'objet python et la cellule du notebook s'effectue à l'instanciation de l'objet - c'est pourquoi la cellule cible est celle dans laquele l'objet est crée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.logging import NotebookCellContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de base du logging Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level de la couche Java (*) afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent.\n",
    "\n",
    "(*) Spark est implémenté en Java. PySpark est une interface Python qui s'appuie sur [py4j](https://www.py4j.org/) afin d'autoriser l'accès aux objets Java instanciés dans la JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### PARTIE I - QUESTION 1.1 : calcul et publication du temps d'attente moyen par station\n",
    "---\n",
    "\n",
    "Cette section donne le détail de construction du flux de données relatif à la dernière heure écoulée - tranche horaire à laquelle les métriques demandées s'appliquent. Le code est segmenté afin d'en faliciter le commentaire. Il sera repris plus loin et encapsulé dans une classe dédiée. \n",
    "\n",
    "**Note: la partie II du projet étant implémentée sur le même schéma, on fait le choix de ne détailler aussi finement que cette première partie du projet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession \\\n",
    "                .builder \\\n",
    "                .appName(\"MS-SIO-HADOOP-PROJECT-KAFKA-STREAM\") \\\n",
    "                .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On crée un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. Il s'agit de spécifier la source des messages via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. \n",
    "\n",
    "On chosit de potionner l'option __startingOffsets__ à la valeur __earliest__ de façon à obtenir toutes les données présentes (i.e. stockées) dans le topic à chaque redémarrage de notre application. Nous avosn toutefois pris soin de limiter le durée de rétention des messages à 8 heures - un réglage propre à chaque topic effectuer comme suit:\n",
    "```\n",
    "cd /usr/hdp/current/kafka-broker\n",
    "./bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic transilien-02 --config retention.ms=28800000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"transilien-02\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 512) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Au niveau du _producteur_, les messages injectés dans le flux Kafka sont sérialisés et encodés au format JSON (encodage binaire). Le connecteur Kafka les delivre sous la forme d'un dataframe dont le schéma est le suivant :\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "\n",
    "Le champ _value_ du dataframe contient les données utiles du message - données qu'il est nécessaire de décoder. Le schéma de désérialisation **json_schema** indique à la fonction Spark **from_json** comment extraire l'information contenue dans le message :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les champs du type _TimestampType_ soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-01 \n",
    "Le traitement du flux Kafka commence donc par une phase désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de cette opération le dataframe _df_ a le schéma suivant :\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- mode: string (nullable = true)\n",
    " |-- mission: string (nullable = true)\n",
    " |-- terminus: integer (nullable = true)\n",
    "```\n",
    "#### Séquence de calcul du temps d'attente moyen par station : étape-02\n",
    "Spécification de la watermark du stream Spark. Nous n'acceptons pas les messages ayant plus d'une minute de retard par rapport à la fin de la fenêtre temorelle à laquelle ils appartiennent. Il s'agit d'un choix arbitraire qui n'a que peu d'intérêt dans notre projet. Il est toutefois nécessaire de spécifier cette valeur car l'implémentation sous-jacente doit borner l'accumulation des données en RAM (contrainte liée au mécanisme de fenêtre glissante). [La documentation de Spark explique clairement le concept de _Stateful Stream Processing in Structured Streaming_](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withWatermark(\"timestamp\", \"1 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-03\n",
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé (voire un 'certain' temps après son départ). On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-04\n",
    "Définition de la fênêtre (temporelle) dans laquelle les calculs sont aggrégés. Il s'agit ici de calculer le temps d'attente moyen par station **sur la dernière heure écoulée**. Notre fenêtre a donc une largeur temporelle de 60 minutes (_window length_). On choisit de suivre cette moyenne par pas de 2 minutes (_sliding interval_). Dans la mesure où le calcul est demandé par station, le _groupBy_ s'applique au champ _station_ du dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"station\", sf.window(\"timestamp\", \"60 minutes\", \"2 minutes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-05\n",
    "**Pour chaque station, on définit le temps moyen d'attente sur une période de P minutes comme le rapport de P vs le nombre de trains au départ de cette station sur la période P**. \n",
    "\n",
    "Ici P = 60 minutes. \n",
    "\n",
    "On crée ainsi une _aggrégation_ qui contiendra, pour chaque station et pour chaque fenêtre d'une heure :\n",
    "- une colonne _nt_ indiquant le nombre de trains sur la période\n",
    "- une colonne _awt_ donnant le temps d'attente moyen recherché  (_awt = Average Waiting Time_). \n",
    "\n",
    "La colonne _nt_ est injectée dans le dataframe à titre indicatif (visualisation dans la console, validation du calcul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.agg(sf.count(\"train\").alias(\"nt\"), sf.format_number(60./ sf.count(\"train\"), 2).cast(\"double\").alias(\"awt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-06\n",
    "Identification et selection de la fenêtre temporelle associée à la dernière heure écoulée. \n",
    "\n",
    "Il s'agit de selectionner, parmis les N fenêtres temporelles produites par Spark, celle qui correspond à la dernière heure écoulée. On utilise ici la fonction **current_timestamp** de Spark afin de rendre la sélection dynamique (i.e. glissante). Le calcul est effectué dans l'unité de **unix_timestamp** (i.e. la seconde) - beaucoup plus facile à manipuler dans ce contexte. \n",
    "\n",
    "Dans l'idée de pourvoir visualiser (si besoin) les valeurs mises en jeu dans le calcul, on choisit de créér les colonnes associées:\n",
    "- oha = one hour ago = now - 62 minutes = now - (window length + sliding interval) \n",
    "- now = now - 1 minutes = now - (sliding interval / 2.) => **valeur ajustée pour n'obtenir qu'une seule fenêtre**\n",
    "- wstart = window.start = borne inférieure de la fenêtre temporelle \n",
    "- wend = window.wend = borne supérieure de la fenêtre temporelle\n",
    "\n",
    "La clause _where_ permet de selectionner la fenêtre associée à la dernière heure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"oha\", sf.unix_timestamp(sf.current_timestamp()) - int((60 + 2) * 60)) \\\n",
    "    .withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()) - int(60 * 2) / 2.) \\\n",
    "    .withColumn(\"wstart\", sf.unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", sf.unix_timestamp(\"window.end\")) \\\n",
    "    .where((sf.col(\"oha\") <= sf.col(\"wstart\")) & (sf.col(\"wend\") <= sf.col(\"now\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: à ce stade, _df_ constitue 'l'état' de référence de notre stream Spark. C'est à partir de cet état que l'on produit les résultats, métriques, indicateurs, ... relatifs à la dernière haure de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation des étapes 01 à 06\n",
    "\n",
    "On lance un 'writeStream' vers la console afin de visualiser les données produites par le stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .select(\"station\", \"window\", \"nt\", \"awt\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "+--------+------------------------------------------+---+----+\n",
    "|station |window                                    |nt |awt |\n",
    "+--------+------------------------------------------+---+----+\n",
    "|87382473|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87386425|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87382259|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|19 |3.16|\n",
    "|87382457|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|18 |3.33|\n",
    "|87382440|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|20 |3.0 |\n",
    "|87382333|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|14 |4.29|\n",
    "|87382887|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|12 |5.0 |\n",
    "|87334482|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|30 |2.0 |\n",
    "|87381137|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|48 |1.25|\n",
    "|87386318|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|17 |3.53|\n",
    "|87382374|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|12 |5.0 |\n",
    "|87382499|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|27 |2.22|\n",
    "|87382655|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|5  |12.0|\n",
    "|87382382|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|25 |2.4 |\n",
    "|87381905|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|31 |1.94|\n",
    "|87384008|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|16 |3.75|\n",
    "|87386003|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|19 |3.16|\n",
    "|87386300|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|24 |2.5 |\n",
    "|87381129|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|38 |1.58|\n",
    "|87386409|[2019-02-28 08:06:00, 2019-02-28 09:06:00]|24 |2.5 |\n",
    "+--------+------------------------------------------+---+----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt de la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### PARTIE I - QUESTIONS 1.2 à 1.6\n",
    "---\n",
    "- Calcul du temps moyen d’attente sur la ligne station par station sur la dernière heure\n",
    "- Calcul du temps moyen d’attente globale sur la ligne sur la dernière heure\n",
    "- Trier les stations par temps d’attente moyen sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le plus élevée sur la dernière heure\n",
    "- Trouver la station avec le temps d’attente le moins élevée sur la dernière heure\n",
    "- Construire un tableau de bord dans Tableau Software sur la base de ces indicateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les calculs demandés nécessiteraient une seconde opération aggrégation sur le stream (incarné dans ce qui précède par le dataframe _df_). Or, en l'état actuel de Spark (2.4), [il n'est pas possible d'enchainer plusieurs opération d'aggrégation sur un même stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations). Il nous faut donc trouver une solution de contournement.\n",
    "\n",
    "L'idée retenue est d'effectuer les calculs sur chaque _batch_ dans le contexte d'un callback du type _foreachBatch_. Cette approche fonctionne ici car chaque _batch_ contient l'intégralité des données sur lesquelles les calculs doivent être réalisés pour produire les résultats attendus - i.e. les données d'attente moyenne par station. \n",
    "\n",
    "Retenons que l'idée d'effectuer les calculs dans le contexte d'un callback du type _foreachBatch_ nous est venue après bien des recherches sur le net ! \n",
    "\n",
    "Ces résulats seront enregistrés en local sous la forme de _vues temporaires_. Ils sont rendus accéssibles depuis Tableau par l'intermédaire de notre propre serveur **_thrift_** - i.e. lancé en local et configuré de manière spécifique (i.e. en mode _single session_). \n",
    "\n",
    "Les calculs (Q1.2 à Q.1.5) sont regroupés dans un callback du type _foreachBatch_ dont les appels sont déclenchés par une _StreamingQuery_.\n",
    "\n",
    "**La classe _TransilienStreamProcessor_ implémente les fonctionnalités de la partie I du projet à travers les fonctions setup_last_hour_awt_stream et computeAwtMetricsAndSaveAsTempViews**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### PARTIE II - Calcul de la progression des trains et localisation géographique.\n",
    "---\n",
    "\n",
    "Nous proposons une solution globale qui répond à l'ensemble des demandes et implémente la totalité des '_bonus_':\n",
    "\n",
    "Demandes de base:\n",
    "\n",
    "- calculer la durée du parcours d’un train entre les stations A et B\n",
    "- calculer en continu (à partir de l’heure courante) le % du trajet réalisé par le train \n",
    "- afficher la barre de progression du trajet dans Tableau Software (avec rafraichissement automatique)\n",
    "\n",
    "Bonus :\n",
    "- afficher la progression de l’ensemble des trains de la ligne\n",
    "- géolocaliser de l’ensemble des trains de la ligne\n",
    "- publier le code sous gitlab (ou github)\n",
    "\n",
    "Dans son implémetation l'approche est similaire à celle des questions de la partie I et utilise également un callback du type _foreachBatch_. \n",
    "\n",
    "**La classe _TransilienStreamProcessor_ implémente les fonctionnalités de la partie II du projet à travers les fonctions setup_trains_progression_stream et computeTrainsProgressionAndSaveAsTempView**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Implementation de la solution proposée pour les parties I & II du projet\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des packages Python requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window as spark_window\n",
    "from py4j.java_gateway import java_import\n",
    "from tools.task import Task\n",
    "from tools.logging import NotebookCellContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de base du logging Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level de la couche Java (*) afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent.\n",
    "\n",
    "(*) Spark est implémenté en Java. PySpark est une interface Python qui s'appuie sur [py4j](https://www.py4j.org/) afin d'autoriser l'accès aux objets Java instanciés dans la JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les trois fonctions suivantes sont liées aux _UDF_ Spark (_User Defined Fonctions_) que nous mettons en oeuvre dans le calcul de la position 'temps réel' des trains. \n",
    "\n",
    "Nous proposons en fait deux modes de calcul: \n",
    "- une première évaluation de la position de chaque train est calculée sur la base d'une interpolation linéaire de la latitude et de la longitude entre les gares de départ et d'arrrivée. L'inconvénient de cette approche est de produire des positions incohérentes (hors rails) dans le cas des 'trains directs'. \n",
    "\n",
    "- le second mode de calcul de la positioin des trains est basé sur un \"modèle géolocalisée\" de la ligne. Ce modèle, construit sur la base des données SNCF et stocké dans le fichier '_scnf-paths-line-l.json_', décrit la ligne de station en station. Quelles que soient les stations _sA_ et _sB_ de la ligne, le parcours _sA_ -> _sB_ est stocké sous la forme d'un vecteur de _Np_ points géolocalisés qui suivent le tracé des rails entre les gares _sA_ et _sB_. Ces données permettent d'interpoler la position du train de manière beaucoup plus précise. On notera que l'interpolation entre deux points du parcours est - en fonction de _Np_ - du type _cubic spline_ ou _linear_ et qu'elle repose sur les fonctionnalirés du package _scipy_. Enfin, on retiendra que ce calcul est delégué à des UDF Spark et qu'il peut être déactivé à la demande (afin d'en constater visuellement l'effet). \n",
    "\n",
    "**A propos des fonctions UDF de Spark :**\n",
    "Une contrainte technique liée à Spark ne permet pas d'associer les fonctions membres d'une instance de classe à une UDF Spark. Pour faire court, la référence _self_ - qui désigne une instance particulière d'une classe - [ne peut être _broadcastée_ ](https://stackoverflow.com/questions/31396323/spark-error-it-appears-that-you-are-attempting-to-reference-sparkcontext-from-a/31600775) - i.e. propogagée du _driver_ vers les _workers_. [Cet article](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-broadcast.html) explique en détail ce qu'est le mécanique de broadcast _driver_ -> _worker_ des variables dans Spark. Nous sommes donc contraints d'utiliser des fonctions indépendantes. D'où la présence des trois fonctions suivantes. Idéalement, on aurait souhaité les encapsuler dans notre classe d'implémentation du projet: **TransilienStreamProcessor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# load trains paths data (geolocalized paths from station to station)\n",
    "with open(\"./scnf-paths-line-l.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    g_trains_paths = json.load(f)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "def accurate_latitude(from_station, to_station, progression):  \n",
    "    # compute accurate latitude\n",
    "    return accurate_train_position('lat', from_station, to_station, progression)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "def accurate_longitude(from_station, to_station, progression):  \n",
    "    # compute accurate longitude\n",
    "    return accurate_train_position('lon', from_station, to_station, progression)  \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "def accurate_train_position(geo_component, from_station, to_station, train_progression):\n",
    "    # compute accurate trains positions\n",
    "    # the interpolation is done along the paths stored into <g_trains_paths>\n",
    "    path_from_st_to_st = g_trains_paths[f\"{from_station}-{to_station}\"]\n",
    "    # special case for 'standby trains' (trains waiting for departure)\n",
    "    if from_station == to_station:\n",
    "        if geo_component == 'lat':\n",
    "            geo_pos = float(path_from_st_to_st[\"geoPoints\"][0][\"latitude\"])\n",
    "        else:\n",
    "            geo_pos = float(path_from_st_to_st[\"geoPoints\"][0][\"longitude\"])\n",
    "    else:    \n",
    "        # the scipy cubic spline interpolator doesn't like short interpolation domains \n",
    "        # use cubic spline when the number of points between from_station & to_station is above 4\n",
    "        interpolator = 'linear' if path_from_st_to_st[\"number_step\"] <= 4 else 'cubic'\n",
    "        # prepare data for interpolator: progression axis in % \n",
    "        x = np.linspace(0., 100., num=path_from_st_to_st[\"number_step\"], endpoint=True)\n",
    "        # interpolation of the requested pos. component (latitude or longitude)\n",
    "        if geo_component == 'lat':\n",
    "             # prepare data for interpolator: latitude axis\n",
    "            lat_y = [float(point[\"latitude\"]) for point in path_from_st_to_st[\"geoPoints\"]]\n",
    "            # get latitude for the cuurent 'train_progression'\n",
    "            geo_pos = float(interp1d(x, lat_y, kind=interpolator)(train_progression))\n",
    "        else:\n",
    "            # prepare data for interpolator: longitude axis \n",
    "            lon_y = [float(point[\"longitude\"]) for point in path_from_st_to_st[\"geoPoints\"]]\n",
    "            # get longitude for the cuurent 'train_progression'\n",
    "            geo_pos = float(interp1d(x, lon_y, kind=interpolator)(train_progression))\n",
    "    # return current latitude and longitude values\n",
    "    return geo_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransilienStreamProcessor**: notre classe de traitement du stream API Transilien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransilienStreamProcessor(NotebookCellContent):\n",
    "    \n",
    "    # unique TransilienStreamProcessor instance\n",
    "    singleton = None\n",
    "    \n",
    "    # spark user defined fonction: accurate latitude computation\n",
    "    alt_udf = sf.udf(accurate_latitude, st.FloatType())\n",
    "    \n",
    "    # spark user defined fonction: accurate longitude computation\n",
    "    alg_udf = sf.udf(accurate_longitude, st.FloatType())\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __init__(self, config):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # init mother class (NotebookCellContent details above)\n",
    "        NotebookCellContent.__init__(self, \"TransilienStreamProcessor\")\n",
    "            \n",
    "        # store the configuration \n",
    "        self.config = config\n",
    "        \n",
    "        # setup logging: timestamp of the last cell clearing (avoid accumulating log in the notebook cell)\n",
    "        self.last_clear_outputs_ts = time.time()\n",
    "        # setup logging: show/display train progression table (computation result)\n",
    "        self.show_trprg_table = self.config.get('show_trprg_table', False)\n",
    "        # setup logging: temp logging level (we want to see the init messages whatever is the logging level)\n",
    "        self.set_logging_level(logging.DEBUG)\n",
    "        \n",
    "        # release any existing instance\n",
    "        if TransilienStreamProcessor.singleton is not None:\n",
    "            self.warning(\"TSP:releasing existing instance...\")\n",
    "            try:\n",
    "                TransilienStreamProcessor.singleton.stop()\n",
    "                del(TransilienStreamProcessor.singleton)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            self.warning(\"TSP:`-> done!\")\n",
    "            \n",
    "        # self is the unique TransilienStreamProcessor instance\n",
    "        TransilienStreamProcessor.singleton = self\n",
    "  \n",
    "        # enable 'accurate trains position' computation\n",
    "        self.accurate_trains_position_enabled = True\n",
    "        \n",
    "        self.debug(\"TSP:initializing...\")\n",
    "        \n",
    "        # kafka oriented spark session (i.e. configured to process incoming Kafka messages)\n",
    "        # we notably specify the thrift server mode and port\n",
    "        self.debug(f\"TSP:creating kafka oriented spark session\")\n",
    "        self.kafka_session = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"MS-SIO-HADOOP-PROJECT-STREAM\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", self.config['spark_sql_shuffle_partitions']) \\\n",
    "            .config('spark.sql.hive.thriftServer.singleSession', True) \\\n",
    "            .config('hive.server2.thrift.port', self.config['hive_thrift_server_port']) \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "        self.debug(\"`-> done!\")\n",
    "                \n",
    "        # average waiting time on the last hour of data (awt): kafka stream setup\n",
    "        self.debug(f\"TSP:initializing 'last hour awt' stream\")\n",
    "        self.last_hour_awt_stream = self.__setup_last_hour_awt_stream()\n",
    "        self.debug(\"`-> done!\")\n",
    "        \n",
    "        # real time trains progression: kafka stream setup\n",
    "        self.debug(f\"TSP:initializing 'trains progression' stream\")\n",
    "        self.trains_progression_stream = self.__setup_trains_progression_stream()\n",
    "        self.debug(\"`-> done!\")\n",
    "        \n",
    "        # start our own thrift server\n",
    "        # this will allow to expose the temp. views and make them reachable from Tableau Software   \n",
    "        self.__start_thrift_server()\n",
    "            \n",
    "        # create a temp. view for the transilien stations data (label, geo.loc., ...)\n",
    "        # this will allow to expose this data and make them reachable from Tableau Software\n",
    "        self.stations_data = self.__create_stations_view()\n",
    "        \n",
    "        # average waiting time on the last hour of data (awt): computation streaming query\n",
    "        # acts as a trigger for computeAwtMetricsAndSaveAsTempViews (forEachBatch callback)\n",
    "        self.lhawt_sink = None\n",
    "        \n",
    "        # average waiting time on the last hour of data (awt): console logging streaming query\n",
    "        # this will allow to print batches into the console  \n",
    "        self.lhawt_console_sink = None\n",
    "        \n",
    "        # real time trains progression (trprg): computation streaming query\n",
    "        # acts as a trigger for computeTrainsProgressionAndSaveAsTempView (forEachBatch callback)\n",
    "        self.trprg_sink = None\n",
    "\n",
    "        # start the streaming queries?\n",
    "        if self.config['auto_start']:\n",
    "            self.start()\n",
    "        \n",
    "        self.debug(f\"initialization done!\")\n",
    "        \n",
    "        # set actual logging level (the one specified by the configuration)\n",
    "        self.set_logging_level(logging.DEBUG if self.config['verbose'] else logging.ERROR)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __start_thrift_server(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # start our own thrift server\n",
    "        # port and mode were specified at 'kafka_session' instanciation\n",
    "        # ---------------------------------------------------------------------------\n",
    "        # note: there is now way to stop the server once started! there's also no way\n",
    "        # note: to check whether or not the thrift server is already running! we \n",
    "        # note: consequently have to restart our python kernel  each time we want to \n",
    "        # note: change something into the code - that's real annoying! the best \n",
    "        # note: workaround we have is to start the server is it's not already running!\n",
    "        # ---------------------------------------------------------------------------\n",
    "        def __is_thrift_server_running(port):\n",
    "            import socket\n",
    "            thrift_server_running = False\n",
    "            try:\n",
    "                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "                s.connect((\"localhost\", port))\n",
    "                s.close()\n",
    "                thrift_server_running = True\n",
    "            except ConnectionRefusedError:\n",
    "                pass\n",
    "            return thrift_server_running\n",
    "        if __is_thrift_server_running(self.config['hive_thrift_server_port']):\n",
    "            wm = f\"TSP:thrift server already running on port {self.config['hive_thrift_server_port']}\"\n",
    "            self.warning(wm) \n",
    "            return\n",
    "        try:\n",
    "            self.debug(f\"TSP:starting thrift server on port {self.config['hive_thrift_server_port']}\") \n",
    "            #sc.setLogLevel('INFO')\n",
    "            java_import(sc._gateway.jvm,\"\")\n",
    "            sc._gateway.jvm.org.apache.spark.sql \\\n",
    "                       .hive.thriftserver.HiveThriftServer2 \\\n",
    "                       .startWithContext(spark._jwrapped)\n",
    "            #sc.setLogLevel('ERROR')       \n",
    "            self.debug(f\"TSP:thrift server successfully started\") \n",
    "        except Exception as e:\n",
    "            self.error(e)\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __create_stations_view(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # read then save stations data as a tmep view so that we can retrieve it from\n",
    "        # Tableau Software (or any client having the ability to talk to our thrift server)         \n",
    "        df = self.kafka_session \\\n",
    "            .read \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"sep\", \",\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .load(\"file:/root/ms-sio-hdp/api-transilien/transilien_line_l_stations_by_code.csv\")\n",
    "        df.createOrReplaceTempView(\"stations_data\")\n",
    "        return df\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def enable_accurate_trains_position(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "         # enable 'accurate trains position' feature\n",
    "         self.accurate_trains_position_enabled = True\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def disable_accurate_trains_position(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "         # disable 'accurate trains position' feature\n",
    "         self.accurate_trains_position_enabled = False\n",
    "                \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __setup_last_hour_awt_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # setup stream for the 'average waiting time on the last hour of data' \n",
    "        # --------------------------------------------------------------------\n",
    "        # processing sequence comments:\n",
    "        # 1 - create the kafka stream\n",
    "        # 2 - extract data from kafka messages (i.e. deserialization) \n",
    "        # 3 - set watermark (using kafka_lhawt_stream_watermark config parameter)\n",
    "        # 4 - drop (train, departure-time) duplicates         \n",
    "        # 5 - setup sliding window (using config parameters)\n",
    "        # 6 - count trains in each window & compute average waiting time \n",
    "        # 7 - select 'last hour window'\n",
    "        # 8 - drop temp. columns\n",
    "        wm = float(self.config['kafka_lhawt_stream_watermark'])\n",
    "        wl = float(self.config['kafka_lhawt_stream_window_length'])\n",
    "        si = float(self.config['kafka_lhawt_stream_sliding_interval'])\n",
    "        oha_offset = int((wl + si) * 60.)\n",
    "        now_offset = int(60. * si / 2.)\n",
    "        return self.kafka_session \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", self.config['kafka_broker']) \\\n",
    "            .option(\"subscribe\", self.config['kafka_topic']) \\\n",
    "            .option(\"spark.streaming.kafka.consumer.poll.ms\", 100) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load() \\\n",
    "            .select(sf.from_json(sf.col(\"value\").cast(\"string\"), \n",
    "                                 self.config['json_schema'], \n",
    "                                 self.config['json_options']).alias(\"departure\")) \\\n",
    "            .select(\"departure.*\") \\\n",
    "            .withWatermark(\"timestamp\", f\"{int(wm)} minutes\") \\\n",
    "            .dropDuplicates([\"train\", \"timestamp\"]) \\\n",
    "            .groupBy(\"station\", sf.window(\"timestamp\", f\"{int(wl)} minutes\", f\"{int(si)} minutes\")) \\\n",
    "            .agg(sf.count(\"train\").alias(\"nt\"), \\\n",
    "                 sf.format_number(wl / sf.count(\"train\"), 1).cast(\"double\").alias(\"awt\")) \\\n",
    "            .withColumn(\"oha\", sf.unix_timestamp(sf.current_timestamp()) - oha_offset) \\\n",
    "            .withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()) - now_offset) \\\n",
    "            .withColumn(\"wstart\", sf.unix_timestamp(\"window.start\")) \\\n",
    "            .withColumn(\"wend\", sf.unix_timestamp(\"window.end\")) \\\n",
    "            .where((sf.col(\"oha\") <= sf.col(\"wstart\")) & (sf.col(\"wend\") <= sf.col(\"now\"))) \\\n",
    "            .drop(\"oha\", \"now\", \"wstart\", \"wend\")\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __setup_trains_progression_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # setup stream for trains progression \n",
    "        # -----------------------------------\n",
    "        # processing sequence comments:\n",
    "        # 1 - create the kafka stream\n",
    "        # 2 - extract data from kafka messages (i.e. deserialization) \n",
    "        # 3 - drop (train, departure-time) duplicates\n",
    "        # 4 - filter on departure-time mode ('R' only)\n",
    "        # 5 - convert departure time (i.e. timestamp) to unix timestamp then drop initial column\n",
    "        # 6 - filter on 'time window' (keep only trains which departure time is in now +/- half-time-window) \n",
    "        # 7 - execute a 'dummy' aggregation so that we can work in 'complete' mode (nice trick)\n",
    "        # 8 - order by (\"train\", \"departure\", \"station\")\n",
    "        time_window = config['kafka_trprg_time_window']            \n",
    "        return self.kafka_session \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "            .option(\"subscribe\", \"transilien-02\") \\\n",
    "            .option(\"spark.streaming.kafka.consumer.poll.ms\", 100) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load() \\\n",
    "            .select(sf.from_json(sf.col(\"value\").cast(\"string\"), \\\n",
    "                                 self.config['json_schema'], \\\n",
    "                                 self.config['json_options']).alias(\"departure\")) \\\n",
    "            .select(\"departure.*\") \\\n",
    "            .dropDuplicates([\"train\", \"timestamp\"]) \\\n",
    "            .filter(\"mode='R'\") \\\n",
    "            .withColumn(\"departure\", sf.unix_timestamp(\"timestamp\")).drop(\"timestamp\") \\\n",
    "            .where(sf.col(\"departure\") \\\n",
    "            .between(sf.unix_timestamp(sf.current_timestamp()) - int(time_window/2.), \\\n",
    "                                       sf.unix_timestamp(sf.current_timestamp()) + int(time_window/2.))) \\\n",
    "            .groupBy(\"train\", \"station\", \"departure\", \"mode\", \"mission\", \"terminus\") \\\n",
    "            .agg(sf.count(\"train\").alias(\"tmp\")).drop(\"tmp\") \\\n",
    "            .orderBy(\"train\", \"departure\", \"station\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # start streaming queries\n",
    "        self.start_last_hour_awt_stream()\n",
    "        self.start_trains_progress_stream()\n",
    "     \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop streaming queries\n",
    "        self.stop_last_hour_awt_stream()\n",
    "        self.stop_trains_progress_stream()\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start_last_hour_awt_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries if already running\n",
    "        self.stop_last_hour_awt_stream()            \n",
    "        # processing time of the queries\n",
    "        proc_time = f\"{self.config.get('kafka_lhawt_processing_time', 5.)} seconds\"     \n",
    "        # last hour awt: create then start the (computation) streaming query\n",
    "        # also make 'self.computeAwtMetricsAndSaveAsTempViews' the 'foreachBatch' callback\n",
    "        self.debug(f\"TSP:starting 'awt' sink (stream query)\")\n",
    "        self.lhawt_sink =  self.last_hour_awt_stream \\\n",
    "                            .writeStream \\\n",
    "                            .trigger(processingTime=proc_time) \\\n",
    "                            .foreachBatch(self.computeAwtMetricsAndSaveAsTempViews) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .start()\n",
    "        self.debug(f\"`-> done!\")    \n",
    "        if self.config.get('kafka_lhawt_console_sink_enabled', False):\n",
    "            # last hour awt: create then start the (console) streaming query\n",
    "            self.debug(f\"TSP:starting 'awt' console stream (stream query)\")\n",
    "            self.lhawt_console_sink = self.last_hour_awt_stream \\\n",
    "                                .orderBy(\"awt\") \\\n",
    "                                .writeStream \\\n",
    "                                .trigger(processingTime=proc_time) \\\n",
    "                                .outputMode(\"complete\") \\\n",
    "                                .format(\"console\") \\\n",
    "                                .option(\"truncate\", False) \\\n",
    "                                .start() \n",
    "        self.debug(f\"`-> done!\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def start_trains_progress_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries if already running\n",
    "        self.stop_trains_progress_stream()\n",
    "        # trains progression: create then start the hive sink (streaming query)\n",
    "        # also make 'self.computeTrainsProgressionAndSaveAsTempView' the 'foreachBatch' callback\n",
    "        self.debug(f\"TSP:starting trains progression sink (stream query)\")\n",
    "        self.trprg_sink = self.trains_progression_stream \\\n",
    "                            .writeStream \\\n",
    "                            .foreachBatch(self.computeTrainsProgressionAndSaveAsTempView) \\\n",
    "                            .outputMode(\"complete\") \\\n",
    "                            .start()\n",
    "        self.debug(f\"`-> done!\")\n",
    "        self.debug(f\"TSP:streaming queries are running\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop_last_hour_awt_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries (best effort impl.)\n",
    "        if self.lhawt_sink is  not None:\n",
    "            try:\n",
    "                self.debug(f\"TSP:stopping 'awt' sink (stream query)\")\n",
    "                self.lhawt_sink.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.lhawt_sink = None\n",
    "                self.debug(f\"`-> done!\")\n",
    "        if self.lhawt_console_sink is  not None:\n",
    "            try:\n",
    "                self.debug(f\"TSP:stopping 'awt' console sink (stream query)\")\n",
    "                self.lhawt_console_sink.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.lhawt_console_sink = None\n",
    "                self.debug(f\"`-> done!\")\n",
    "   \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def stop_trains_progress_stream(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # stop the streaming queries (best effort impl.)\n",
    "        if self.trprg_sink is  not None:\n",
    "            try:\n",
    "                self.debug(f\"TSP:stopping trains progression sink (stream query)\")\n",
    "                self.trprg_sink.stop()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            finally:\n",
    "                self.trprg_sink = None\n",
    "                self.debug(f\"`-> done!\")\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def cleanup(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # cleanup the underlying session \n",
    "        # TODO: not sure this is the right way to do the job\n",
    "        self.stop()\n",
    "        self.debug(f\"TSP:shutting down Kafka-SparkSession\")\n",
    "        self.kafka_session.stop()\n",
    "        self.debug(f\"`-> done!\")\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def turnVerboseOn(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # turn verbose on          \n",
    "        self.set_logging_level(logging.DEBUG)\n",
    "       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def turnVerboseOff(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # turn verbose off   \n",
    "        self.set_logging_level(logging.ERROR)\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def clearOutputs(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # clear outputs (i.e. clear our 'mother notebook-cell')\n",
    "        clear_outputs_period = self.config.get('clear_outputs_period', 15)\n",
    "        if (time.time() - self.last_clear_outputs_ts) > clear_outputs_period:\n",
    "            self.clear_output()\n",
    "            self.last_clear_outputs_ts = time.time()\n",
    "   \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def showTrainsProgressionTable(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # trains progression table will be diplayed        \n",
    "        self.show_trprg_table = True\n",
    "       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def hideTrainsProgressionTable(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # trains progression table will NOT be diplayed \n",
    "        self.show_trprg_table = False\n",
    "                       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def computeAwtMetricsAndSaveAsTempViews(self, batch, batch_number):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # PART-I: COMPUTE AVERAGE WAITING TIME METRICS\n",
    "        # --------------------------------------------\n",
    "        # this 'forEachBatch' callback is attached to the our 'lhawt_sink' (streaming query)\n",
    "        try:\n",
    "            # clear cell content so that we don't cumulate the log \n",
    "            self.clearOutputs()\n",
    "                              \n",
    "            # be sure we have some data to handle (incoming dataframe not empty)\n",
    "            # this will avoid creating empty tables on Hive side \n",
    "            if batch.rdd.isEmpty():\n",
    "                self.warning(f\"TSP:computeAwtMetrics: ignoring empty batch #{batch_number}\")\n",
    "                return\n",
    "\n",
    "            self.debug(f\"TSP:entering computeAwtMetrics for batch #{batch_number}...\")\n",
    "                              \n",
    "            # PART-I: Q1.1 & Q1.3: ordered average waiting time in minutes (over last hour)\n",
    "            self.debug(f\"computing ordered average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.orderBy(sf.asc(\"awt\")).select(batch.station, batch.awt)    \n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"ordered_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "                                                                  \n",
    "            # PART-I: Q1.2: global average waiting time in minutes (over last hour)\n",
    "            self.debug(f\"computing global average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.agg(sf.count(\"station\").alias(\"number_of_stations\"), \n",
    "                            sf.avg(\"awt\").alias(\"global_awt\"))\n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"global_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "            \n",
    "            # PART-I: Q1.4: min average waiting time in minutes (over last hour)\n",
    "            self.debug(f\"computing min. average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.orderBy(sf.asc(\"awt\")).limit(1).select(batch.station, \n",
    "                                                               batch.awt.alias(\"min_awt\"))\n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"min_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "           \n",
    "            # PART-I: Q1.5: max average waiting time in minutes (over last hour)\n",
    "            self.debug(f\"computing min. average waiting time...\")\n",
    "            t = time.time()\n",
    "            tmp = batch.orderBy(sf.desc(\"awt\")).limit(1).select(batch.station, \n",
    "                                                                batch.awt.alias(\"max_awt\"))\n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"max_awt\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "                              \n",
    "            self.debug(f\"TSP:computeAwtMetrics successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            self.error(f\"TSP:failed to compute awt metrics from batch #{batch_number}\")\n",
    "            self.error(e)                            \n",
    "                       \n",
    "    # -------------------------------------------------------------------------------    \n",
    "    def computeTrainsProgressionAndSaveAsTempView(self, batch, batch_number):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # PART-II: COMPUTE TRAINS PROGRESSION\n",
    "        # ------------------------------------\n",
    "        # this 'forEachBatch' callback is attached to the our 'trprg_sink' (streaming query)\n",
    "        try:    \n",
    "            # clear cell content so that we don't cumulate the log               \n",
    "            # self.clear_output()\n",
    "                              \n",
    "            # be sure we have some data to handle (incoming dataframe not empty)\n",
    "            # this will avoid creating empty tables on Hive side \n",
    "            if batch.rdd.isEmpty():\n",
    "                self.warning(f\"TSP:computeTrainsProgression ignoring empty batch #{batch_number}\")\n",
    "                return\n",
    "\n",
    "            t = time.time()\n",
    "\n",
    "            # create next_departure 'lead' columns: departure columns up shifted by 1 row\n",
    "            tmp = batch.withColumn('next_departure', sf.lead('departure') \\\n",
    "                       .over(spark_window.partitionBy(\"train\") \\\n",
    "                       .orderBy(\"departure\")))\n",
    "            # create next_station 'lead' columns: station columns up shifted by 1 row\n",
    "            tmp = tmp.withColumn('next_station', sf.lead('station') \\\n",
    "                      .over(spark_window.partitionBy(\"train\") \\\n",
    "                      .orderBy(\"departure\")))\n",
    "            # create humanly readable columns for departure date/time \n",
    "            tmp = tmp.withColumn(\"departure_date\", sf.from_unixtime(tmp.departure, \"HH:mm:ss\"))\n",
    "            tmp = tmp.withColumn(\"next_departure_date\", sf.from_unixtime(tmp.next_departure, \"HH:mm:ss\"))\n",
    "\n",
    "            # compute travel time between 'departure' and 'next_departure' - i.e. from one station to the next\n",
    "            tmp = tmp.withColumn(\"time_to_st\", tmp.next_departure -  tmp.departure)\n",
    "                       \n",
    "            # travel direction encoding: 1:paris->banlieue or -1:banlieue->paris\n",
    "            tmp = tmp.withColumn(\"direction\", \n",
    "                                 sf.when(tmp.mission.isin(self.config['missions_to_paris']), sf.lit(1)) \\\n",
    "                                   .otherwise(sf.lit(-1)))\n",
    "                       \n",
    "            # swap departure date/time (due to train direction) - this is just for readability & display \n",
    "            tmp = tmp.withColumn(\"temp_departure_date\", tmp.departure_date)\n",
    "            tmp = tmp.withColumn(\"departure_date\", \n",
    "                                 sf.when(tmp.departure < tmp.next_departure, tmp.departure_date) \\\n",
    "                                   .otherwise(tmp.next_departure_date))\n",
    "            tmp = tmp.withColumn(\"next_departure_date\", \n",
    "                                 sf.when(tmp.departure < tmp.next_departure, tmp.next_departure_date) \\\n",
    "                                   .otherwise(tmp.temp_departure_date))\n",
    "            tmp = tmp.drop(\"temp_departure_date\")\n",
    "\n",
    "            # tmp.show()\n",
    "\n",
    "            # create column to store the current time (i.e. now)\n",
    "            tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "\n",
    "            # the travel (from one station to the next) can belong to the past, the future or can be in progress \n",
    "            tmp = tmp.withColumn(\"in_past\", (tmp.now > tmp.departure) & (tmp.now > tmp.next_departure))\n",
    "            tmp = tmp.withColumn(\"in_future\", (tmp.now < tmp.departure) & (tmp.now < tmp.next_departure))\n",
    "            tmp = tmp.withColumn(\"in_progress\", (tmp.in_past != sf.lit(True)) & (tmp.in_future != sf.lit(True)))\n",
    "\n",
    "            # tmp.show()\n",
    "\n",
    "            # keep only 'in progress' travels - i.e. the ones not in past nor in the future\n",
    "            # we also remove: \n",
    "            #    - rows for which next_departure is null (introduced by the lead function)\n",
    "            # note that we keep:\n",
    "            #    - trains in standby (i.e fake travel from one station to the same - train waiting for next departure)\n",
    "            tmp = tmp.filter((~tmp.in_past & ~tmp.in_future) & (tmp.next_departure.isNotNull()))\n",
    "\n",
    "            # tmp.show()\n",
    "\n",
    "            # compute travel progression in %\n",
    "            tmp = tmp.withColumn(\"progress\", (100. * sf.abs((tmp.now - tmp.departure))) / sf.abs(tmp.time_to_st))  \n",
    "            # compute trains progression: maintain value in  the [O, 100]% range \n",
    "            tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress < sf.lit(0.), sf.lit(0.)).otherwise(tmp.progress))             \n",
    "            # compute trains progression: maintain value in  the [O, 100]% range \n",
    "            tmp = tmp.withColumn(\"progress\", sf.when(tmp.progress > sf.lit(100.), sf.lit(100.)).otherwise(tmp.progress))\n",
    "\n",
    "            # tmp.show()\n",
    "              \n",
    "            # select the required columns\n",
    "            tmp = tmp.select(tmp.train, \n",
    "                             tmp.departure_date.alias(\"departure\"),\n",
    "                             tmp.next_departure_date.alias(\"arrival\"),\n",
    "                             tmp.mission, \n",
    "                             tmp.station.alias(\"from_st\"), \n",
    "                             tmp.next_station.alias(\"to_st\"), \n",
    "                             tmp.time_to_st,\n",
    "                             tmp.progress,\n",
    "                             tmp.direction) \n",
    "    \n",
    "            # from (departure location)\n",
    "            tmp = tmp.join(self.stations_data, tmp.from_st == self.stations_data.station, how=\"left\")\n",
    "            tmp = tmp.withColumn(\"from_st_lt\", tmp.latitude).drop(\"latitude\")\n",
    "            tmp = tmp.withColumn(\"from_st_lg\", tmp.longitude).drop(\"longitude\")\n",
    "            tmp = tmp.withColumn(\"from_st_lb\", tmp.label).drop(\"label\")\n",
    "            tmp = tmp.drop(\"station\")\n",
    "\n",
    "            # to (destination location) \n",
    "            tmp = tmp.join(self.stations_data, tmp.to_st == self.stations_data.station, how=\"left\")\n",
    "            tmp = tmp.withColumn(\"to_st_lt\", tmp.latitude).drop(\"latitude\")\n",
    "            tmp = tmp.withColumn(\"to_st_lg\", tmp.longitude).drop(\"longitude\")\n",
    "            tmp = tmp.withColumn(\"to_st_lb\", tmp.label).drop(\"label\")\n",
    "            tmp = tmp.drop(\"station\")\n",
    "\n",
    "            # compute current train latitude & longitude\n",
    "            tmp = tmp.withColumn(\"train_lt\", \n",
    "                                 tmp.from_st_lt + ((tmp.progress / 100.) * (tmp.to_st_lt - tmp.from_st_lt)))\n",
    "            tmp = tmp.withColumn(\"train_lg\", \n",
    "                                 tmp.from_st_lg + ((tmp.progress / 100.) * (tmp.to_st_lg - tmp.from_st_lg)))\n",
    "              \n",
    "            # cast progress to int\n",
    "            tmp = tmp.withColumn(\"progress\", sf.format_number(tmp.progress, 1).cast(\"double\"))\n",
    "            \n",
    "            # compute accurate latitute and longitude using our user defined functions (udf)\n",
    "            if self.accurate_trains_position_enabled:\n",
    "                tmp = tmp.withColumn(\"train_alt\", \n",
    "                                     TransilienStreamProcessor.alt_udf(tmp.from_st, tmp.to_st, tmp.progress))\n",
    "                tmp = tmp.withColumn(\"train_alg\", \n",
    "                                     TransilienStreamProcessor.alg_udf(tmp.from_st, tmp.to_st, tmp.progress))\n",
    "            else:\n",
    "                tmp = tmp.withColumn(\"train_alt\", tmp.train_lt)\n",
    "                tmp = tmp.withColumn(\"train_alg\", tmp.train_lg)\n",
    "                       \n",
    "            # remove tmp data from table\n",
    "            tmp = tmp.select(\"train\",       # train identifier \n",
    "                             \"departure\",   # departure time\n",
    "                             \"arrival\",     # arrival time\n",
    "                             \"mission\",     # mission code\n",
    "                             \"from_st\",     # departure station code\n",
    "                             \"to_st\",       # arrival station code\n",
    "                             \"from_st_lb\",  # departure station label\n",
    "                             \"to_st_lb\",    # arrival station label\n",
    "                             \"time_to_st\",  # time_to_st = arrival - departure in seconds\n",
    "                             \"progress\",    # travel progress\n",
    "                             \"direction\",   # 1: paris -> banlieue, -1: banlieue->paris\n",
    "                             \"train_lt\",    # current train location: latitude \n",
    "                             \"train_lg\",    # current train location: longitude\n",
    "                             \"train_alt\",   # current train location: latitude \n",
    "                             \"train_alg\")   # current train location: longitude\n",
    "            \n",
    "            # log/debug/validate...\n",
    "            if self.show_trprg_table:\n",
    "                tmp.show()\n",
    "                       \n",
    "            # create a temp. view that - visible from Tableau             \n",
    "            self.kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"trains_progression\")\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "                              \n",
    "            self.debug(f\"TSP:computeTrainsProgression successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            self.error(f\"TSP:failed to compute trains progression from batch #{batch_number}\")\n",
    "            self.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de l'instance de la classe TransilienStreamProcessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "config = {}\n",
    "\n",
    "# json schema & options for kafka messages deserialization \n",
    "config['json_schema'] = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")\n",
    "config['json_options'] = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}\n",
    "\n",
    "# spark sesssions options\n",
    "config['spark_sql_shuffle_partitions'] = 4\n",
    "\n",
    "# kafka source configuration: broker & topic\n",
    "config['kafka_broker'] = \"sandbox-hdp.hortonworks.com:6667\"\n",
    "config['kafka_topic'] = \"transilien-02\"\n",
    "\n",
    "# kafka stream configuration: \n",
    "# structured stream windowing for the last hour average waiting time stream\n",
    "config['kafka_lhawt_stream_watermark'] = 1 \n",
    "config['kafka_lhawt_stream_window_length'] = 60\n",
    "config['kafka_lhawt_stream_sliding_interval'] = 2\n",
    "config['kafka_lhawt_processing_time'] = 5\n",
    "config['kafka_lhawt_console_sink_enabled'] = True \n",
    "\n",
    "# kafka stream configuration: \n",
    "# pseudo time window for the trains progression stream (logical batch window)\n",
    "config['kafka_trprg_time_window'] = 3600\n",
    "\n",
    "# local thrift server configuration\n",
    "config['hive_thrift_server_port'] = 10015\n",
    "\n",
    "# list of missions to Paris (for trains direction identification)\n",
    "config['missions_to_paris'] = [\n",
    "    \"PALS\", \"PASA\", \"PEBU\", \n",
    "    \"PEGE\", \"POPI\", \"POPU\", \n",
    "    \"POSA\", \"POVA\", \"POPE\"\n",
    "]\n",
    "\n",
    "# misc. options\n",
    "config['auto_start'] = True\n",
    "config['verbose'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciation du TransilienStreamProcessor...\n",
    "\n",
    "Pour mémoire, voici une liste des actions possibles sur l'instance TransilienStreamProcessor:\n",
    "- tsp.enable_accurate_trains_position() : enable accurate trains position comptuation\n",
    "- tsp.disable_accurate_trains_position() : disable accurate trains position comptuation\n",
    "- tsp.showTrainsProgressionTable() : display trains progression dataframe each time it's updated\n",
    "- tsp.hideTrainsProgressionTable() : don't show trains progression dataframe\n",
    "- tsp.turnVerboseOff() : switch logging level to logging.ERROR\n",
    "- tsp.turnVerboseOn()  : switch logging level to logging.DEBUG\n",
    "- tsp.stop()  : stop the streaming queries\n",
    "- tsp.start() : (re)start the streaming queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = TransilienStreamProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp.turnVerboseOn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp.turnVerboseOff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: laissons le _TransilienStreamProcessor_ tourner - si nécessaire nous pourrons l'arrêter avec la commande suivante:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du contenu de la table **trains_progression** (produite dans _TransilienStreamProcessor.computeTrainsProgressionAndSaveAsTempView_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp.kafka_session.sql(\"select * from trains_progression\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Bokeh (& Google Maps) comme alternative à Tableau Desktop pour la visualisation dynamique des données \n",
    "---\n",
    "\n",
    "L'animation suivante illustre ce qu'il est possible d'obtenir en comninant les fonctionnalités de notre classe **Task** à celle de la [la librairie Bokeh](https://bokeh.pydata.org/en/latest/). L'idée est d'implémenter une visualisation dynamique de la localisation des trains dans la cellule d'un notebook Jupyter.  \n",
    "\n",
    "La classe **TrainsTracker** implémente la fonctionnalité proposée. \n",
    "\n",
    "Note: L'implémentation repose - à travers Bokeh - sur l'API Google Maps. L'utilisation de cette API exige une clé qui peut être obtenue - moyennant l'enregistrement d'un moyen de paiement - sur [Google Cloud](https://developers.google.com/maps/documentation/javascript/get-api-key)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GitHub Logo](./trains-tracker.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.models import GMapOptions, ColumnDataSource, LabelSet, Label\n",
    "from bokeh.models import Circle, Diamond\n",
    "from bokeh.models import Range1d, HoverTool, PanTool, WheelZoomTool, CrosshairTool\n",
    "from bokeh.plotting import gmap\n",
    "from tools.task import Task\n",
    "from tools.logging import NotebookCellContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsTracker(Task, NotebookCellContent):\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __init__(self, config):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        Task.__init__(self, \"TrainsTracker\")\n",
    "        NotebookCellContent.__init__(self, \"TrainsTracker\")\n",
    "        output_notebook()\n",
    "        self.config = config\n",
    "        self.trains_to_paris = None\n",
    "        self.trains_to_suburbs = None\n",
    "        self.set_logging_level(self.config.get('loging_level', logging.ERROR))\n",
    "        # accurate trains position: load paths from station to station\n",
    "        self.accurate_trains_position_enabled = True\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def on_init(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        try:\n",
    "            self.plot_handle = self.__setup_gmap()\n",
    "            self.handle_periodic_message()\n",
    "            p = self.config.get('update_period_in_seconds', 1.)\n",
    "            self.enable_periodic_message(p)\n",
    "        except Exception as e:\n",
    "            self.error(e)\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def on_exit(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        try:\n",
    "            self.clear_output()\n",
    "        except Exception as e:\n",
    "            self.error(e)\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def handle_periodic_message(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        try:\n",
    "            self.__update_trains_position(self.trains_to_paris, 1)\n",
    "            self.__update_trains_position(self.trains_to_suburbs, -1)\n",
    "            push_notebook(handle=self.plot_handle)\n",
    "        except Exception as e:\n",
    "            self.error(e)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __setup_gmap(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # GMAP SETUP -------- \n",
    "        map_options = GMapOptions(lat=48.885, lng=2.19, \n",
    "                                  map_type=\"roadmap\", styles=map_style, zoom=12)\n",
    "        p = gmap(self.config['gmap_api_key'], map_options=map_options)\n",
    "        p.title.text = \"Transilien Ligne L\"\n",
    "        p.plot_height=600\n",
    "        p.plot_width=1200\n",
    "        p.toolbar.logo = None\n",
    "        p.outline_line_color = None\n",
    "        p.grid.grid_line_color = None\n",
    "        # TRAINS SETUP ------\n",
    "        self.trains_to_paris = self.__setup_trains_components(p, \"yellow\")\n",
    "        self.trains_to_suburbs = self.__setup_trains_components(p, \"blue\")\n",
    "        # STATIONS SETUP ----\n",
    "        # get stations data from our deticate spark table\n",
    "        self.__setup_stations_components(p)\n",
    "        # return notebook handle\n",
    "        return show(p, notebook_handle=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __setup_trains_components(self, plot, glyph_color):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        source = ColumnDataSource(data=dict(lat=[], lon=[], tid=[], mission=[], departure=[], \n",
    "                                            arrival=[], from_st_lb=[], to_st_lb=[]))\n",
    "        glyph = Diamond(x=\"lon\", y=\"lat\", size=17, fill_color=glyph_color, fill_alpha=0.4)\n",
    "        renderer = plot.add_glyph(source_or_glyph=source, glyph=glyph)\n",
    "        labels = LabelSet(x='lon', y='lat', text='tid', level='glyph',\n",
    "                          x_offset=0, y_offset=5, text_align='center', \n",
    "                          text_font_size=\"9pt\", source=source, render_mode='canvas')\n",
    "        plot.add_layout(labels)\n",
    "        tooltips = [\n",
    "            (\"Train\", \"@tid\"),\n",
    "            (\"Mission\", \"@mission\"),\n",
    "            (\"Départ\", \"@departure\"),\n",
    "            (\"Gare Départ\", \"@from_st_lb\"),\n",
    "            (\"Arrivée\", \"@arrival\"),\n",
    "            (\"Gare Arrivée\", \"@to_st_lb\")\n",
    "        ]\n",
    "        plot.add_tools(HoverTool(renderers=[renderer], tooltips=tooltips))\n",
    "        return source \n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __setup_stations_components(self, plot):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # get stations data from our deticate spark table\n",
    "        ks = self.config['transilien_stream_processor'].kafka_session\n",
    "        stations = ks.sql(\"select * from stations_data\").toPandas()\n",
    "        # configure teh stations glyph (rendering on map)\n",
    "        short_labels = []\n",
    "        for label in stations['label']:\n",
    "            if len(label) > 16:\n",
    "                short_labels.append(\"\".join([label[:12],\"...\"]))\n",
    "            else:\n",
    "                short_labels.append(label)\n",
    "        stations_source = ColumnDataSource(data=dict(lat=stations.latitude, \n",
    "                                                     lon=stations.longitude, \n",
    "                                                     sid=stations.station, \n",
    "                                                     flb=stations.label, \n",
    "                                                     slb=short_labels))\n",
    "        stations_glyph = Circle(x=\"lon\", y=\"lat\", size=6, fill_color=\"red\", fill_alpha=0.8)\n",
    "        stations_renderer = plot.add_glyph(source_or_glyph=stations_source, glyph=stations_glyph)\n",
    "        stations_labels = LabelSet(x='lon', y='lat', text='slb', level='glyph',\n",
    "                                   x_offset=0, y_offset=-15, text_align='center', \n",
    "                                   text_font_size=\"7pt\", source=stations_source, \n",
    "                                   render_mode='canvas')\n",
    "        plot.add_layout(stations_labels)\n",
    "        tooltips_schema = [\n",
    "            (\"Gare\", \"@sid\"),\n",
    "            (\"Label\", \"@flb\")\n",
    "        ]\n",
    "        plot.add_tools(HoverTool(renderers=[stations_renderer], tooltips=tooltips_schema))\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def enable_accurate_trains_position(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "         self.accurate_trains_position_enabled = True\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def disable_accurate_trains_position(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "         self.accurate_trains_position_enabled = False\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __update_trains_position(self, data_source, direction):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # get a ref. to the kafka session\n",
    "        ks = self.config['transilien_stream_processor'].kafka_session\n",
    "        # get data from 'trains_progression' temp. view\n",
    "        pdf = ks.sql(f\"select * from trains_progression where direction == {direction}\").toPandas()\n",
    "        # select 'accurate' or 'standard' (direct trajectory between depature and arrival)\n",
    "        lat_data = pdf.train_alt if self.accurate_trains_position_enabled else pdf.train_lt\n",
    "        lon_data = pdf.train_alg if self.accurate_trains_position_enabled else pdf.train_lg\n",
    "        # create a bokeh.ColumnDataSource for updated data\n",
    "        new_data = ColumnDataSource(\n",
    "                       data = dict(\n",
    "                            lat=lat_data, \n",
    "                            lon=lon_data,\n",
    "                            tid=pdf.train,\n",
    "                            mission=pdf.mission,\n",
    "                            departure=pdf.departure,\n",
    "                            arrival=pdf.arrival,\n",
    "                            from_st_lb=pdf.from_st_lb,\n",
    "                            to_st_lb=pdf.to_st_lb\n",
    "                       )\n",
    "                   )\n",
    "        # tell bokeh we changed the data source\n",
    "        data_source.data.update(new_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a free gmap style from https://snazzymaps.com\n",
    "map_style = \"\"\" \n",
    "[\n",
    "    {\n",
    "        \"featureType\": \"all\",\n",
    "        \"elementType\": \"labels.text\",\n",
    "        \"stylers\": [\n",
    "            {\n",
    "                \"visibility\": \"off\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"featureType\": \"road\",\n",
    "        \"elementType\": \"all\",\n",
    "        \"stylers\": [\n",
    "            {\n",
    "                \"visibility\": \"simplified\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"featureType\": \"road\",\n",
    "        \"elementType\": \"geometry.stroke\",\n",
    "        \"stylers\": [\n",
    "            {\n",
    "                \"visibility\": \"simplified\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"featureType\": \"road\",\n",
    "        \"elementType\": \"labels.text\",\n",
    "        \"stylers\": [\n",
    "            {\n",
    "                \"visibility\": \"off\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"featureType\": \"road\",\n",
    "        \"elementType\": \"labels.icon\",\n",
    "        \"stylers\": [\n",
    "            {\n",
    "                \"visibility\": \"off\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du fichier  'gmap_api_key.json' contenant la clé Google Maps. Le fichier doit respecter le schéma json suivant: (\"xxxxxxxxxxxxxxxx\" étant la clé [Google Map API Key](https://developers.google.com/maps/documentation/javascript/get-api-key)): \n",
    "```\n",
    "credentials = {'gmap_api_key': \"xxxxxxxxxxxxxxxx\"}\n",
    "with open('./gmap_api_key.json', 'w+', encoding='utf-8') as f:\n",
    "    json.dump(credentials, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement de la clé depuis le fichier 'gmap_api_key.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./gmap_api_key.json', 'r', encoding='utf-8') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration du TrainsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtt_config = {\n",
    "    'gmap_api_key': credentials['gmap_api_key'],\n",
    "    'gmap_style': map_style,\n",
    "    'update_period_in_seconds': 1.,\n",
    "    'transilien_stream_processor': tsp\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciation du TrainsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtt = TrainsTracker(mtt_config)\n",
    "mtt.start_asynchronously()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtt.disable_accurate_trains_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtt.enable_accurate_trains_position()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt (et destruction) de notre TrainsTracker.\n",
    "\n",
    "Note: Un appel à _exit_ demande au Thread sous-jacent de retourner - ce qui provoque sa 'disparition'. Il existe aucun moyen de redémarrer une Task sur laquelle _exit_ a été invoquée. L'objet doit être reconstruit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtt.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
