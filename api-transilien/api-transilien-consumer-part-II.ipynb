{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN - PARTIE II\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session Spark associé au flux Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-KAFKA-STREAM\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation du nombre de taches lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un [structured spark stream](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) associé à une source Kafka. \n",
    "\n",
    "Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = kafka_session \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox-hdp.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"transilien-02\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/33732346/spark-dataframe-transform-multiple-rows-to-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_session.createDataFrame([\n",
    "    (\"s1\", \"t1\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s1\", \"t2\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s1\", \"t3\", int(time.time() - random.randint(300,1800)), \"E\"),\n",
    "    (\"s2\", \"t0\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s2\", \"t2\", int(time.time() - random.randint(300,1800)), \"R\"),\n",
    "    (\"s2\", \"t1\", int(time.time() - random.randint(300,1800)), \"R\"), \n",
    "    (\"s2\", \"t3\", int(time.time() - random.randint(300,1800)), \"R\")], \n",
    "    (\"station\", \"train\", \"departure-time\", \"mode\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_df = df.groupby('train').pivot('station').max('departure-time').fillna(0).orderBy('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du dataframe (format générique des dataframe issus d'un stream Kafka).\n",
    "```\n",
    "kafka_stream.printSchema()\n",
    "root\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "Il est donc nécessaire de spécifier le schéma de désérialisation qui sera passé à la fonction **from_json**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"station\", st.IntegerType(), True),\n",
    "        st.StructField(\"train\", st.StringType(), True),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), True),\n",
    "        st.StructField(\"mode\", st.StringType(), True),\n",
    "        st.StructField(\"mission\", st.StringType(), True),\n",
    "        st.StructField(\"terminus\", st.IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A travers, la variable **json_options**, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_stream \\\n",
    "    .select(sf.from_json(sf.col(\"value\").cast(\"string\"), json_schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe a le schéma suivant:\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- mode: string (nullable = true)\n",
    " |-- mission: string (nullable = true)\n",
    " |-- terminus: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les stations contigües A et B (cf. énoncé partie II) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contiguous_stations = {\n",
    "    'sa':87381129, # Station A: CLICHY LEVALLOIS\n",
    "    'sb':87381137  # Station B: ASNIERES SUR SEINE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre sur les 'contiguous_stations' et sur le 'mode' de l'horaire de départ de chaque train. On ne retient que les trains au départ des stations qui apparaissent dans la liste _contiguous_stations_ pour lesquels le mode de l'horaire annoncé vaut \"R\" (horaire réel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(sf.col(\"station\").isin(list(contiguous_stations.values()))).filter(\"mode='R'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversion de l'heure de départ au format unix timestamp (plus simple à manipuler) - on supprime la colonne \"timestamp\", devenue inutile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"departure\", sf.unix_timestamp(\"timestamp\")).drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection des trains dont l'heure de départ est se situe dans l'interval : maintenant +/- (time_window/2) exprimé dqns l'unité unix timestamp (i.e. la seconde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(sf.col(\"departure\").between(sf.unix_timestamp(sf.current_timestamp()) - int(time_window/2.), \n",
    "                                          sf.unix_timestamp(sf.current_timestamp()) + int(time_window/2.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo aggregation pour obtenir notre ensemble de trains en un _batch_ unique - l'idée est de pouvoir effectuer une requête en mode _complete_ sur notre stream. Il s'agit d'une astuce qui vise à satisfaire une contrainte imposée par Spark : le mode 'complete' ne s'appliquer qu'à des données aggrégées - i.e. issue d'une fonction d'aggrégration de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"train\", \"station\", \"departure\", \"mode\", \"mission\", \"terminus\").agg(sf.count(\"train\").alias(\"tmp\")).drop(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forEachBatchCallback(batch, batch_number):\n",
    "    t = time.time()\n",
    "    tmp = batch.groupby(\"train\").pivot(\"station\").max(\"departure\").fillna(0).orderBy(\"train\")\n",
    "    tmp = tmp.select(\"train\", sf.col(str(contiguous_stations['sa'])).alias(\"sa\"), sf.col(str(contiguous_stations['sb'])).alias(\"sb\"))\n",
    "    tmp = tmp.filter((sf.col(\"sa\") > 0) & (sf.col(\"sb\") > 0))\n",
    "    tmp = tmp.withColumn(\"sa-dpt\", sf.from_unixtime(sf.col(\"sa\")))\n",
    "    tmp = tmp.withColumn(\"sb-dpt\", sf.from_unixtime(sf.col(\"sb\")))\n",
    "    tmp = tmp.withColumn(\"dt\", sf.col(\"sb\") - sf.col(\"sa\"))\n",
    "    tmp = tmp.withColumn(\"now\", sf.unix_timestamp(sf.current_timestamp()))\n",
    "    tmp = tmp.withColumn(\"in-past\", (tmp.now > tmp.sa) & (tmp.now > tmp.sb))\n",
    "    tmp = tmp.withColumn(\"in-future\", (tmp.now < tmp.sa) & (tmp.now < tmp.sb))\n",
    "    tmp = tmp.withColumn(\"in-progress\", (sf.col(\"in-past\") != sf.lit(True)) & (sf.col(\"in-future\") != sf.lit(True)))\n",
    "    tmp = tmp.orderBy(sf.col(\"sa\"))\n",
    "    tmp.show()\n",
    "    kafka_session.createDataFrame(tmp.rdd).createOrReplaceTempView(\"reshaped_batch\")\n",
    "    print(f\"`-> took {round(time.time() - t, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .foreachBatch(forEachBatchCallback) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt de la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1.stop()\n",
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_session.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
