{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJET HADOOP - MS-SIO-2019 - SNCF - API TRANSILIEN\n",
    "\n",
    "#### SPARK STRUCTURED STREAMING (KAFKA CONSUMER)\n",
    "\n",
    "P. Hamy,  N. Leclercq, L. Poncet - MS-SIO-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 1.1 : calcul et publication du temps d'attente moyen par station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changement du logging level afin d'éliminer le bruit généré dans la console par un [_warning_](https://stackoverflow.com/questions/39351690/got-interruptedexception-while-executing-word-count-mapreduce-job) récurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la session associée au [Strutured Spark Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark4kafka = SparkSession.builder.appName(\"MS-SIO-HADOOP-PROJECT-KAFKA-CONSUMER\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations du nombre de taches de lancées par spark (conseil de configutation glané sur internet pour les configurations matérielles les plus modestes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark4kafka.conf.set('spark.sql.shuffle.partitions', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du flux Kafka\n",
    "On utilise ici un 'structured spark stream' associé à une source Kafka. Il s'agit de spécifier la source via l'adresse du serveur Kafka et le nom du topic auquel on souhaite s'abonner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = \"sandbox-hdp.hortonworks.com:6667\"\n",
    "topic = \"transilien-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark4kafka \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schéma de désérialisation des messages  \n",
    "Les messages injectés dans le flux Kafka sont sérialisés et encodés en binaire dans le champ _value_ du DataFrame _df_.\n",
    "```\n",
    "df.printSchema()\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    " ```\n",
    "On spécifie le format de désérialisation qui sera passé à la fonction _from_json_. A travers, la variable _json_options_, on précise également le format du champ _timestamp_ afin que les valeurs temporelles soient correctement interprétées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"station\", IntegerType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"train\", StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_options = {\"timestampFormat\": \"yyyy-MM-dd'T'HH:mm:ss.sss'Z'\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-01 \n",
    "Désérialisation/reformatage des messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'issue de opération le dataframe _df0_ a le schéma suivant:\n",
    "```\n",
    "df0.printSchema()\n",
    "root\n",
    " |-- station: integer (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- train: string (nullable = true)\n",
    "```\n",
    "#### Séquence de calcul du temps d'attente moyen par station : étape-02\n",
    "Spécification de la watermark du stream spark. Nous n'acceptons pas les messages ayant plus d'une minute de retard. Il s'agit d'un choix arbitraire qui n'a que peu d'intérêt dans notre projet. Il toutefois nécessaire de spécifier cette valeur car l'implémentation sous-jacente doit borner l'accumulation des données du stream. [La documentation de Spark explique clairement le concept de _Stateful Stream Processing in Structured Streaming_](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.withWatermark(\"timestamp\", \"1 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-03\n",
    "Un train apparaitra dans les réponses aux requêtes de l'API SNCF tant que son heure de départ n'appartient pas au passé. On supprime donc les doublons associés aux couples (train, heure de départ). Inutile d'ajouter la station à la contrainte d'exclusion car l'idenfiant d'un train est unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.dropDuplicates([\"train\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-04\n",
    "Définition de la fênêtre (temporelle) dans laquelle les calculs sont aggrégés. Il s'agit ici de calculer le temps d'attente moyen par station sur la dernière heure. Notre fenêtre a donc une largeur temporelle de 60 minutes (_window length_). On choisit de suivre cette moyenne par pas de 2 minutes (_sliding interval_). Dans la mesure où le calcul est demandé par station, la fonction _groupBy_ s'applique au champ _station_ du DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.groupBy(\"station\", window(\"timestamp\", \"60 minutes\", \"2 minutes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-05\n",
    "Pour chaque station, on définit le temps moyen d'attente sur une période _P_ comme le rapport de _P_ vs le nombre de trains au départ de cette station sur la période _P_. Ici _P_ = 60 minutes. On crée une _aggrégation_ sur la fenêtre temporelle précédemment définie. On crée ainsi, pour chaque station et pour chaque fenêtre d'une heure, une colonne _nt_ qui contient le nombre de trains sur la période et une colonne _awt_ qui contiendra le temps d'attente moyen recherché. La colonne _nt_ est donnée à titre indicatif (visualisation dans la console, validation du calcul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.agg(count(\"train\").alias(\"nt\"), format_number((60. / count(\"train\")), 2).cast(\"double\").alias(\"awt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : étape-06\n",
    "Selection de la fenêtre temporelle associée à la dernière heure. Il s'agit de selectionner, parmis les N fenêtres associées au stream, celle qui correspond à la dernière heure écoulée. On utilise ici la fonction _current_timestamp_ de Spark afin de rendre la sélection dynamique (i.e. glissante). Le calcul est effectué est dans l'unité de _unix_timestamp_ (la seconde) - beaucoup plus facile à manipulée dans ce contexte. Dans l'idée de pourvoir visualiser les valeurs mises en jeu dans le calcul, on choisit de créér les colonnes associées:\n",
    "- oha = one hour ago = now - 62 minutes = now - (window length + sliding interval) \n",
    "- now = now - 1 minutes = now - (sliding interval / 2.) => valeur ajustée pour n'obtenir qu'une seule fenêtre\n",
    "- wstart = window.start = borne inférieure de la fenêtre temporelle \n",
    "- wend = window.wend = borne supérieure de la fenêtre temporelle\n",
    "\n",
    "La clause _where_ permet de selectionner la fenêtre associée à la dernière heure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0 \\\n",
    "    .withColumn(\"oha\", unix_timestamp(current_timestamp()) - 3720) \\\n",
    "    .withColumn(\"now\", unix_timestamp(current_timestamp()) - 60) \\\n",
    "    .withColumn(\"wstart\", unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", unix_timestamp(\"window.end\")) \\\n",
    "    .where((col(\"oha\") <= col(\"wstart\")) & (col(\"wend\") <= col(\"now\"))) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note importante : _df0_ constitue l'état de référence de notre stream Spark. C'est à partir de cet état que l'on produit les résultats, métriques, indicateurs, ... demandés**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séquence de calcul du temps d'attente moyen par station : concaténation des étapes 01 à 06\n",
    "On s'offre simplement la possibilité d'exécuter la séquence en un seul appel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema, json_options).alias(\"departure\")) \\\n",
    "    .select(\"departure.*\") \\\n",
    "    .select(\"station\", \"train\", \"timestamp\") \\\n",
    "    .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "    .dropDuplicates([\"train\", \"timestamp\"]) \\\n",
    "    .groupBy(\"station\", window(\"timestamp\", \"60 minutes\", \"2 minutes\")) \\\n",
    "    .agg(count(\"train\").alias(\"nt\"), format_number((60. / count(\"train\")), 2).cast(\"double\").alias(\"awt\")) \\\n",
    "    .withColumn(\"oha\", unix_timestamp(current_timestamp()) - 3720) \\\n",
    "    .withColumn(\"now\", unix_timestamp(current_timestamp()) - 60) \\\n",
    "    .withColumn(\"wstart\", unix_timestamp(\"window.start\")) \\\n",
    "    .withColumn(\"wend\", unix_timestamp(\"window.end\")) \\\n",
    "    .where((col(\"oha\") <= col(\"wstart\")) & (col(\"wend\") <= col(\"now\"))) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requête _writeStream_ pour affichage dans la console\n",
    "Le 'processing' des données est déclenché toutes les minutes (_trigger_). On chosit d'afficher les colonnes suivantes : \n",
    "- _station_ : identifiant de la station \n",
    "- _window_ : fenêtre temporelle \n",
    "- _nt_ : nombre de trains sur la période définie par _window_\n",
    "- _awt_ : temps d'attente moyen en secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = df0 \\\n",
    "    .select(\"station\", \"window\", \"nt\", \"awt\") \\\n",
    "    .orderBy(\"station\") \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"1 minutes\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q0.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ainsi un affichage de la forme : \n",
    "```\n",
    "+--------+------------------------------------------+---+----+\n",
    "|station |window                                    |nt |awt |\n",
    "+--------+------------------------------------------+---+----+\n",
    "|87334482|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|15 |4.0 |\n",
    "|87366922|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|3  |20.0|\n",
    "|87381111|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|26 |2.31|\n",
    "|87381129|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|25 |2.4 |\n",
    "|87381137|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|39 |1.54|\n",
    "|87381459|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|24 |2.5 |\n",
    "|87381657|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|20 |3.0 |\n",
    "|87381905|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|18 |3.33|\n",
    "|87382002|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|29 |2.07|\n",
    "|87382200|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|10 |6.0 |\n",
    "|87382218|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|26 |2.31|\n",
    "|87382259|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|10 |6.0 |\n",
    "|87382267|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|12 |5.0 |\n",
    "|87382333|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|9  |6.67|\n",
    "|87382341|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|12 |5.0 |\n",
    "|87382358|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|24 |2.5 |\n",
    "|87382366|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|13 |4.62|\n",
    "|87382374|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|12 |5.0 |\n",
    "|87382382|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|10 |6.0 |\n",
    "|87382432|[2019-02-26 10:16:00, 2019-02-26 11:16:00]|13 |4.62|\n",
    "+--------+------------------------------------------+---+----+```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecriture du stream sous la forme d'une table Hive \n",
    "Idéalement, on souhaiterait utiliser le 'sink Hive' natif que l'on s'attend à trouver dans l'outillage pySpark. Un tel sink n'existe malheureusement pas. [Hortonworks en propose un pour Spark >= 2.3](https://github.com/hortonworks-spark/spark-hive-streaming-sink) mais il s'agit d'une implémentation purement Scala pour laquelle il n'existe pas d'interface Python. On choisit donc de s'orienter vers une solution du type [sink générique](https://docs.databricks.com/spark/latest/structured-streaming/foreach.html) basée sur l'utilisation du callback _streamingDataFrame.writeStream.foreachBatch_.\n",
    "\n",
    "L'idée est ici de 'transférer' notre DataFrame de la _SparkSession_ de streamming - dont il est issu - vers une _SparkSession_ orientée _Hive_ connectée au _warehouse_ accessible depuis _Tableau_ via le serveur _Spark Thrift_. \n",
    "\n",
    "Nous avons intuité cette approche après avoir réalisé un POC d'écriture d'un dataframe Spark sour la forme d'un table visible et accessible depuis _HiveView_ sous _Ambari_.\n",
    "\n",
    "Le transfert d'une session à l'autre s'effectue dans le contexte d'un callback du type _foreachBatch_\n",
    "associé à une instance de _StreamingQuery_ (cf. _q1_ ci-dessous). \n",
    "\n",
    "Commencons par instancier notre _SparkSession_ connectée à _Hive_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark4hive = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"MS-SIO-HADOOP-PROJECT-SPARK-SQL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://sandbox-hdp.hortonworks.com:8020/api-transilien\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création(si besoin) puis sélection de la base 'transilien' sous Hive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_location = \"hdfs://sandbox-hdp.hortonworks.com:8020/api-transilien\"\n",
    "spark4hive.sql(f'create database if not exists transilien location \"{db_location}\"')\n",
    "spark4hive.sql(\"use transilien\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implémentation du callback 'foreachBatch'\n",
    "La fonction de callback sera appelée chaque fois qu'un micro-batch sera délivré par le stream Spark. La période d'appel sera celle du _trigger_ associé à la _StreamingQuery_ (cf _q1_ ci-dessous). Notre solution repose sur l'affectation _spark4hive = streamingdataframe_. Contrairement au comportement attendu dans Python, cet opérateur effectue une copie - ce qui a pour effet de transferer le DataFrame du contexte de _streaming_ vers le contexte _hive_. Il suffit ensuite d'utiliser la fonction _saveAsTable_ en mode _overwrite_ pour écrire le dataframe sous la forme d'une table _Hive_ accéssible sous _Tableau_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveBatchAsTable(streaming_dataframe, batch_number):\n",
    "    try:\n",
    "        spark4hive = streaming_dataframe\n",
    "        if spark4hive.rdd.isEmpty():\n",
    "            return\n",
    "        spark4hive.write.mode(\"overwrite\").saveAsTable(\"awt\")\n",
    "    except Exception as e:\n",
    "        print(f\"failed to update hive table with batch #{batch_number}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'une instance de _StreamingQuery_ associée au callback _saveBatchAsTable_\n",
    "En plus de déclencher l'appel à notre callback, cete StreamingQuery nous permet également de sélectionner les champs du DataFrame qui seront injectés dans la table. On choisit ici de ne retenir que l'identifiant de la station et le temps d'attente moyen associé. Les informations relatives à la station - label, coordonnées GPS, ... - seront obtenus par jointure avec une table dédiée (déjà) disponible sous Hive. \n",
    "\n",
    "Note : il aurait été possible d'effectuer cette jointure depuis le stream Spark mais nous choisissons, pour des raisons de simplification, de la réaliser sous Tableau.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df0 \\\n",
    "    .select(\"station\", \"awt\") \\\n",
    "    .orderBy(\"station\") \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"1 minutes\") \\\n",
    "    .foreachBatch(saveBatchAsTable) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.2 à 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une instance de _StreamingQuery_ pour le stockage du dataframe de référence (i.e. _df0_) sous forme d'une table en mémoire. Cette approche va nous permettre de calculer les indicateurs demandés à partir d'une table stockée en mémoire. On utilise ici le _sink memory_ natif de Spark. Le nom de la table créée est spécifié par l'option _queryName_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = df0 \\\n",
    "    .select(\"station\", \"awt\") \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"1 minutes\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"awt_table\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.2  : Calcul du temps moyen d’attente globale sur la ligne sur la dernière heure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------------+\n",
      "|number_of_stations|global_waiting_time_in_minutes|\n",
      "+------------------+------------------------------+\n",
      "|                 0|                          null|\n",
      "+------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark4kafka \\\n",
    "    .sql(\"select * from awt_table\") \\\n",
    "    .agg(count(\"station\").alias(\"number_of_stations\"), avg(\"awt\").alias(\"global_waiting_time_in_minutes\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.3  : Trier les stations par temps d’attente moyen sur la dernière heure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+\n",
      "|station_identifier|waiting_time_in_minutes|\n",
      "+------------------+-----------------------+\n",
      "+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark4kafka \\\n",
    "    .sql(\"select * from awt_table\") \\\n",
    "    .orderBy(desc('awt')) \\\n",
    "    .select(col(\"station\").alias(\"station_identifier\"), col(\"awt\").alias(\"waiting_time_in_minutes\")) \\\n",
    "    .show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.4  : Trouver la station avec le temps d’attente le plus élevée sur la dernière heure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|station|awt|\n",
      "+-------+---+\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark4kafka \\\n",
    "    .sql(\"select * from awt_table\") \\\n",
    "    .orderBy(desc(\"awt\")) \\\n",
    "    .limit(1) \\\n",
    "    .show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.5 : Trouver la station avec le temps d’attente le moins élevée sur la dernière heure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|station|awt|\n",
      "+-------+---+\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark4kafka \\\n",
    "    .sql(\"select * from awt_table\") \\\n",
    "    .orderBy(asc(\"awt\")) \\\n",
    "    .limit(1) \\\n",
    "    .show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1.6 : Construire un tableau de bord dans Tableau Software avec les indicateurs Q1.2 à Q.1.5\n",
    "Nous choisissons l'approche suivante: les calculs précédents (Q1.2 à Q.1.5) sont regroupés dans un callback du type _foreachBatch_ dont les appels sont déclenchés par une _StreamingQuery_. Les résulats sont enregistrés sous _Hive_ dans des tables spécifiques. Ils sont ainsi accéssibles depuis Tableau pour l'élaboration du tableau de bord.   \n",
    "\n",
    "Cette fonctionnalité est implémentée par la classe _Indicators_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indicators():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 streaming_dataframe, \n",
    "                 awt_table=\"awt_table\", \n",
    "                 drop_existing_tables=True, \n",
    "                 auto_start=True, \n",
    "                 verbose=False):\n",
    "        # streaming dataframe\n",
    "        self.streaming_dataframe = streaming_dataframe\n",
    "        # name of the (in memory) awt table \n",
    "        self.awt_table = awt_table\n",
    "        # hive oriented spark session\n",
    "        self.session = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"yarn\") \\\n",
    "            .appName(\"MS-SIO-HADOOP-PROJECT-INDICATORS\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"hdfs://sandbox-hdp.hortonworks.com:8020/api-transilien\") \\\n",
    "            .config(\"hive.metastore.uris\", \"thrift://sandbox-hdp.hortonworks.com:9083\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "        # drop existing tables\n",
    "        if drop_existing_tables:\n",
    "            self.__drop_tables()\n",
    "        # streaming query (computeAndSaveAsTables trigger)\n",
    "        self.streaming_query = None\n",
    "        # start the the streaming query?\n",
    "        if auto_start:\n",
    "            self.start()\n",
    "        # verbose flag (TODO: use python logging)\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def start(self):\n",
    "        self.stop()\n",
    "        self.streaming_query = self.streaming_dataframe \\\n",
    "            .writeStream \\\n",
    "            .trigger(processingTime=\"1 minutes\") \\\n",
    "            .foreachBatch(self.computeAndSaveAsTables) \\\n",
    "            .start()\n",
    "        \n",
    "    def stop(self):\n",
    "        try:\n",
    "            self.streaming_query.stop()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        finally:\n",
    "            self.streaming_query = None\n",
    "        \n",
    "    def cleanup(self):\n",
    "        self.session.stop()\n",
    "        \n",
    "    def turnVerboseOn(self):\n",
    "        self.verbose = True\n",
    "        \n",
    "    def turnVerboseOff(self):\n",
    "        self.verbose = False\n",
    "     \n",
    "    def __drop_tables(self):\n",
    "        for table in [\"ordered_awt\", \"global_awt\", \"min_awt\", \"max_awt\"]:\n",
    "            try:\n",
    "                self.session.sql(f\"drop table transilien.{table}\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "    def computeAndSaveAsTables(self, streaming_dataframe, batch_number):\n",
    "        try:\n",
    "            # request data from awt_table (fight against empty table symdrom)\n",
    "            df = self.session.sql(f\"select * from {self.awt_table}\")\n",
    "            # be sure there's something to compute...\n",
    "            if df.rdd.isEmpty():\n",
    "                if self.verbose:\n",
    "                    print(f\"got empty table for batch #{batch_number} - giving up...\")\n",
    "                return\n",
    "            # ordered_waiting_time_in_minutes\n",
    "            df1 = df.orderBy(asc(\"awt\"))\n",
    "            # global_waiting_time_in_minutes\n",
    "            df2 = df.agg(count(\"station\").alias(\"number_of_stations\"), avg(\"awt\").alias(\"global_awt\"))\n",
    "            # min_waiting_time_in_minutes\n",
    "            df3 = df.orderBy(asc(\"awt\")).limit(1).select(col(\"station\"),col(\"awt\").alias(\"min_awt\"))\n",
    "            # max_waiting_time_in_minutes\n",
    "            df4 = df.orderBy(desc(\"awt\")).limit(1).select(col(\"station\"),col(\"awt\").alias(\"max_awt\"))\n",
    "            # verbose\n",
    "            if self.verbose:\n",
    "                df1.show(3, False)\n",
    "                df2.show(3, False)\n",
    "                df3.show(3, False)\n",
    "                df4.show(3, False)\n",
    "            # save results as tables \n",
    "            if not df1.rdd.isEmpty():\n",
    "                if self.verbose:\n",
    "                    print(f\"saving transilien.ordered_awt for batch #{batch_number}\")\n",
    "                df1.write.mode(\"overwrite\").saveAsTable(\"transilien.ordered_awt\")\n",
    "            if not df2.rdd.isEmpty():\n",
    "                if self.verbose:\n",
    "                    print(f\"saving transilien.global_awt for batch #{batch_number}\")\n",
    "                df2.write.mode(\"overwrite\").saveAsTable(\"transilien.global_awt\")\n",
    "            if not df3.rdd.isEmpty():\n",
    "                if self.verbose:\n",
    "                    print(f\"saving transilien.min_awt for batch #{batch_number}\")\n",
    "                df3.write.mode(\"overwrite\").saveAsTable(\"transilien.min_awt\")\n",
    "            if not df4.rdd.isEmpty():\n",
    "                if self.verbose:\n",
    "                    print(f\"saving transilien.max_awt for batch #{batch_number}\")\n",
    "                df4.write.mode(\"overwrite\").saveAsTable(\"transilien.max_awt\")\n",
    "            if self.verbose:\n",
    "                print(f\"computeAndSaveAsTables successfully executed for batch #{batch_number}\")\n",
    "        except Exception as e:\n",
    "            print(f\"failed to update hive table with batch #{batch_number}\")\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|station |awt |\n",
      "+--------+----+\n",
      "|87381137|1.76|\n",
      "|87382002|2.0 |\n",
      "|87382358|2.14|\n",
      "+--------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+------------------+\n",
      "|number_of_stations|global_awt        |\n",
      "+------------------+------------------+\n",
      "|40                |7.7444999999999995|\n",
      "+------------------+------------------+\n",
      "\n",
      "+--------+-------+\n",
      "|station |min_awt|\n",
      "+--------+-------+\n",
      "|87381137|1.76   |\n",
      "+--------+-------+\n",
      "\n",
      "+--------+-------+\n",
      "|station |max_awt|\n",
      "+--------+-------+\n",
      "|87382655|60.0   |\n",
      "+--------+-------+\n",
      "\n",
      "saving transilien.ordered_awt for batch #0\n",
      "saving transilien.global_awt for batch #0\n",
      "saving transilien.min_awt for batch #0\n",
      "saving transilien.max_awt for batch #0\n",
      "computeAndSaveAsTables successfully executed for batch #0\n"
     ]
    }
   ],
   "source": [
    "indicators = Indicators(df0, drop_existing_tables=False, verbose=True, auto_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.turnVerboseOff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.turnVerboseOn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divers \n",
    "Les cellules suivantes permettent :\n",
    "- de stopper les instances de StreamingQuery \n",
    "- de détruire proprement les sessions Spark\n",
    "\n",
    "Note : dans un contexte de production, le code qui précède serait injecté dans un script Python lancé via _spark-submit_. Dans un tel cas, serait nécessaire placer les appels suivants en fin de script : _q0.awaitTermination()_ et _q1.awaitTermination()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark4kafka.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0.stop()\n",
    "q1.stop()\n",
    "q2.stop()\n",
    "indicators.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark4kafka.stop()\n",
    "spark4hive.stop()\n",
    "indicators.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
