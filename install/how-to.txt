# Connexion ssh en root sur le container HDP Sandbox
ssh root@localhost -p 2222

# Telechargement de miniconda
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Installation de paquets
yum install -y bzip2 git

# Installation de miniconda
 chmod +x Miniconda3-latest-Linux-x86_64.sh
 ./Miniconda3-latest-Linux-x86_64.sh # Répondre yes tout le temps (surtout pour le .bashrc)

# Nécessaire si absence de redémarrage pour la suite
export PATH=/root/miniconda3/bin:$PATH

# Installation des librairies via anaconda
conda install -c conda-forge -y jupyterlab kafka-python pyspark xmltodict

# Vérifier que jupyter fonctionne
jupyter-lab --no-browser --ip=0.0.0.0 --port 60000 --allow-root 
# Via le navigateur sur ta machine accéder à localhost:60000
# Copier le token qui apparait sur la console (token=[le truc à récupérer])
# Copier le token dans le navigateur (champ token) et renseigné un mdp. Puis cliquer sur "Login and set new password"
# A verifier
# - peut générer une erreur 500
# - dans ce cas copier directement le token dans le premier champ et cliquer sur les boutons associé pour se connecter

# Intégrer une clé RSA à ton compte GitHub (source : https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/)
ssh-keygen -t rsa -b 4096 -C "[ton_mail]" # générer par defaut une clé rsa dans ~/.ssh/
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/id_rsa
vi ~/.ssh/id_rsa # Copier la totatlité du texte (tout sur une ligne mail compris)
# Aller dans GitHub > Profil > Setting > SSH and GPG keys > New SSH Key

# Cloner le repository git dans le conteneur (on suppose que l'on fait ça depuis le directory /root)
cd /root
git clone https://github.com/projet-hadoop-transilien/prj-hdp-ms-sio.git
cd prj-hdp-ms-sio/
git checkout [ta_branche] # Par défaut tu es sur le master, avec cette commande tu repasses sur ta branche
git fetch origin # Les trois commandes suivantes synchronise ta branche et la branche master
git rebase origin/master
git push


# ajouter les droits d'exec au script jupyspark 
chmod +x /root/prj-hdp-ms-sio/jupyter/jupyspark
# créer le lien symbolique vers le /bin d'anaconda
ln -s /root/prj-hdp-ms-sio/jupyter/jupyspark /root/miniconda3/bin
# lancer jupyspark  (pour tester)
jupyspark 

# De nouveau dans jupyter
# - Executer prj-hdp-ms-sio/api-transilier/api-transilien-sncf-data.ipynb -> Des .json doivent se créer dans ton répertoire
# - Puis executer prj-hdp-ms-sio/api-transilier/api-transilien-producer.ipynb en suivant la procédure pour intégrer ton mot de passe
# - Ensuite executer prj-hdp-ms-sio/api-transilier/api-transilien-consumer-sparksession.ipynb et visualiser dans la console de jupyter les données (peut prendre du temps à se lancer)


# installation du serveur Spark Thrift via Ambari
https://community.hortonworks.com/articles/29110/querying-data-via-sparksql-with-odbc-tools.html

