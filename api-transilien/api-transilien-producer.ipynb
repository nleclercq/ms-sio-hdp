{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET HADOOP/SPARK - MS-SIO-2019 \n",
    "## API MANAGER & PRODUCTEUR FLUX KAFKA\n",
    "### P.Hamy, N.Leclercq, L.Poncet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ce notebook Jupyter implémente la partie _producteur_ du flux Kafka dans lequel sont injectées les données issues de l'API Transilien.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des packages Python requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import xmltodict\n",
    "from collections import OrderedDict\n",
    "from kafka import KafkaProducer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports de nos outils locaux:\n",
    "- Task est une classe implémentant un modèle d'object actif du type \"thread + message queue\".  On utilise ici sa capacité à executer périodiquement une action définie par l'utilisateur (i.e. activité asynchrone). Dans notre cas, il s'agira d'effectuer une requête sur l'API SNCF et d'injecter les données retournées dans un stream Kafka.\n",
    "- NotebookCellContent permet d'attacher la \"cellule courante\" du notebook Jupyter à l'objet qui hérite de cette classe. Le principal intérêt est de router le logging asynchrone vers cette cellule et ce quelle que soit la cellule active (i.e. quelle que soit la cellule dans laquelle l'utilisateur travaille). Le lien entre l'objet python et la cellule du notebook s'effectue à l'instanciation de l'objet - c'est pourquoi la cellule cible est celle dans laquele l'objet est crée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.task import Task\n",
    "from tools.logging import NotebookCellContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de base du [logging](https://docs.python.org/3/library/logging.html#logging.basicConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.ERROR, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les infos de login relatives à l'API Transilien sont chargées depuis un fichier local nommé 'api_transilien_login.json' contenant le dictionnaire suivant (attention il est important de mettre login et  password entre \"quotes\" afin qu'ils soient interprétés comme des chaines de caractères):\n",
    "```\n",
    "{\n",
    "    login: \"xxxxxx\",\n",
    "    password: \"zzzzzz\"\n",
    "}\n",
    "```\n",
    "Afin de contourner le problème de quota sur l'API SNCF - très faible par défaut - il est possible d'utiliser plusieurs comptes. Pour cela, il suffit de les spécificer comme suit dans le fichier:\n",
    "```\n",
    "{\n",
    "    logins: [\"xxx1xxx\", \"xxx2xxx\", \"xxx3xxx\"],\n",
    "    passwords: [\"zzz1zzz\", \"zzz2zzz\", \"zzz3zzz\"]\n",
    "}\n",
    "```\n",
    "\n",
    "La cellule suivante permet de créer ce fichier. Il suffit de decommenter le code et de l'executer (pensez à changer le loggin et password :-) sinon, recommencer, l'option 'w+' écrase le fichier éxistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "credentials = {'logins': [\"xxx1xxx\", \"xxx2xxx\", \"xxx3xxx\"], 'passwords': [\"zzz1zzz\", \"zzz2zzz\", \"zzz3zzz\"]}\n",
    "with open('./api_transilien_login.json', 'w+', encoding='utf-8') as f:\n",
    "    json.dump(credentials, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransilienApi** : classe d'interface de l'API SNCF \n",
    "Cette classe implémente prend en charge les requête sue l'API SNCF et la conversion des données (_xml_) au format souhaité.Le code est entièrement commenté. Rien de particulier ici en dehors des deux points suivants : \n",
    "- afin de lutter contre un problème de quota sur l'API SNCF, on utiilse un itérarteur circulaire sur les gares et sur les couples login/password. Voir TransilienApi.next_login, next_password et next_station.\n",
    "- la conversion des données est (optionnellement) déléguée à une instance de la classe Converter (voir plus loin dans ce notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransilienApi(NotebookCellContent):\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __init__(self, credentials_file, managed_stations=None, owner=None):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # this TransilienApi instance share the same notebook cell with its owner (log_output)\n",
    "        NotebookCellContent.__init__(self, \"KafkaProducerTask\", parent=owner)\n",
    "        # the list of stations for which we want to retrieve departure data\n",
    "        self.managed_stations = managed_stations\n",
    "        # dictionnary of stations in which the key is the station code\n",
    "        self.stations_by_code = None\n",
    "        # optional xml -> ? data converter\n",
    "        self.converter = None\n",
    "        # circular stations iterator: returns the next station to use in the API request\n",
    "        self.stations_iterator = None\n",
    "        # list of logins\n",
    "        self.logins = None\n",
    "        # circular login iterator: returns the next login to use in the API request\n",
    "        self.logins_iterator = None\n",
    "        # list of passwords\n",
    "        self.passwords = None\n",
    "        # circular password iterator: returns the next password to use in the API request\n",
    "        self.passwords_iterator = None\n",
    "        # load the lists of logins and passwords from the specified file\n",
    "        self.__load_credentials(credentials_file)\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __load_credentials(self, credentials_file):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # load the lists of logins and passwords from the specified file\n",
    "        with open(credentials_file, 'r', encoding='utf-8') as f:\n",
    "            self.credentials = json.load(f)\n",
    "        self.logins = self.credentials.get('logins', [self.credentials['login']])\n",
    "        self.passwords = self.credentials.get('passwords', [self.credentials['password']])\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def set_converter(self, converter):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # attach the specified converter to the TransilienApi instance\n",
    "        assert(isinstance(converter, Converter))\n",
    "        self.converter = converter\n",
    "       \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def set_managed_stations(self, managed_stations):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # attach the list of stations for which we want to retrieve departure data\n",
    "        if  managed_stations == '*':\n",
    "            self.managed_stations = list(self.stations_by_code.keys())\n",
    "        else:\n",
    "            assert(isinstance(managed_stations, list))\n",
    "            assert(len(managed_stations))\n",
    "            self.managed_stations = managed_stations\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def load_stations_data(self, fullpath):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # load stations data from the specified json file\n",
    "        with open(fullpath, \"r\", encoding='utf-8') as f:\n",
    "            tmp = json.load(f)\n",
    "        self.stations_by_code = OrderedDict(sorted(tmp.items(), key=lambda x: x[0]))\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __next_login(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "         # circular login iterator: returns the next login to use in the API request\n",
    "        try:\n",
    "            return next(self.logins_iterator)\n",
    "        except:\n",
    "            self.logins_iterator = iter(self.logins)\n",
    "            return next(self.logins_iterator)\n",
    "      \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __next_password(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # circular password iterator: returns the next login to use in the API request\n",
    "        try:\n",
    "            return next(self.passwords_iterator)\n",
    "        except:\n",
    "            self.passwords_iterator = iter(self.passwords)\n",
    "            return next(self.passwords_iterator)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __next_station(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # circular stations iterator: returns the next station to use in the API request\n",
    "        try:\n",
    "            return next(self.stations_iterator)\n",
    "        except:\n",
    "            self.stations_iterator = iter(self.managed_stations)\n",
    "            return next(self.stations_iterator)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def poll_next_station_data(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # execute some requests on the SNCF API  \n",
    "        # here we fight against the very poor quota we have on the API: the idea is to \n",
    "        # execute N consecutive requests using a different (login,password) and station \n",
    "        # identifier for each request - N is simply the number of (login,password) we have \n",
    "        assert(self.managed_stations is not None)\n",
    "        assert(self.stations_by_code is not None)\n",
    "        trains_data = {}\n",
    "        for i in range(len(self.logins)):\n",
    "            station = self.__next_station()\n",
    "            self.debug(f\"API: polling data from {self.stations_by_code[station]['label']}...\")\n",
    "            url = f\"https://api.transilien.com/gare/{station}/depart\"\n",
    "            response = requests.get(url, auth=(self.__next_login(), self.__next_password()))\n",
    "            self.debug(f\"API: request response {response}\")\n",
    "            # by station basic/generic data extraction from xml response (whatever the reponse is!)\n",
    "            trains_data[station] = self.__parse_xml_data(station, response.content)\n",
    "        # optional data convertion\n",
    "        if self.converter is not None:\n",
    "            trains_data = self.converter.convert(trains_data)\n",
    "        return trains_data\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __parse_xml_data(self, station, xml_response):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # basic/generic data extraction from xml response (whatever xml_response is!)\n",
    "        # schema of the dictionnary we return\n",
    "        station_trains_data = {\n",
    "            'station': {\n",
    "                'code':station, \n",
    "                'label':self.stations_by_code[station]['label'], \n",
    "                'latitude':self.stations_by_code[station]['latitude'], \n",
    "                'longitude':self.stations_by_code[station]['longitude']}, \n",
    "            'departures':[]\n",
    "        }\n",
    "        # here, we make use of xml2dict:\n",
    "        # basically, each xml <tag></tag> becomes a dict 'key'.\n",
    "        try:\n",
    "            xml2dict = xmltodict.parse(xml_response)\n",
    "            xml2dict_trains = xml2dict['passages']['train']\n",
    "            for entry in xml2dict_trains:\n",
    "                # filter trains: we reject trains having a terminus which is not one of \n",
    "                # our list of registered stations (i.e. self.stations_by_code)\n",
    "                terminus_code = entry.get('term', None)\n",
    "                if terminus_code is None or terminus_code not in self.stations_by_code:\n",
    "                    continue\n",
    "                # format train data so that is we be easier to handle by the converter\n",
    "                train = {}\n",
    "                train['date'] = entry['date']['#text'].split(' ')[0]\n",
    "                train['time'] = entry['date']['#text'].split(' ')[1]\n",
    "                train['number'] = entry['num']\n",
    "                train['mission'] = entry['miss']\n",
    "                train['mode'] = entry['date']['@mode']\n",
    "                train['terminus'] = {'code':terminus_code, 'label':self.stations_by_code[terminus_code]}\n",
    "                # append the train to the departures list of the specified station\n",
    "                station_trains_data['departures'].append(train)\n",
    "        except Exception as e:\n",
    "            # we don't want to deal with xml-response type\n",
    "            # simply return an empty list of departures in case the parsing failed\n",
    "            # self.error(e)\n",
    "            station_trains_data['departures'] = []\n",
    "        return station_trains_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit une classe **Converter** dont les classes filles ont pour rôle est de convertir les données préformatées par TransilienApi vers différents formats (e.g. _json_, _google-protobuf_, ...). \n",
    "\n",
    "Note: ce choix est historique. Nous avons testé plusieurs formats et avons finalement retenu le plus simple et le plus naturel (en termes de traitement côté Spark): _json_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter:\n",
    "    # convert: the function member the child classes have to override\n",
    "    def convert(trains_data):\n",
    "        raise Exception(\"Converter.convert: default impl. called!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JsonConverter** est un **Converter** dédié au format _json_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonConverter(Converter):\n",
    "    # convert: TransilienApi generic format to json...\n",
    "    def convert(self, trains_data):\n",
    "        assert(isinstance(trains_data, (dict, OrderedDict)))\n",
    "        departures = []\n",
    "        for station, station_data in trains_data.items():\n",
    "            for train_data in station_data['departures']:\n",
    "                # split departure in date & time \n",
    "                # note since we will use 'unix timestamps' (seconds sin 1.1.1970) \n",
    "                # we don't have to deal with day transition between two departures\n",
    "                # of the same train (e.g. departure @23H59 & next one @00H05). the\n",
    "                # year-month-day part is consequently almost useless in our case. \n",
    "                time = f\"{train_data['time']}\"\n",
    "                date = '-'.join(train_data['date'].split('/')[::-1])\n",
    "                timestamp = f\"{date}T{time}:00.000Z\"\n",
    "                departure = {\n",
    "                    # station identifier (number)\n",
    "                    'station':int(station), \n",
    "                    # train identifier (string)\n",
    "                    'train': train_data['number'], \n",
    "                    # departure time (string)\n",
    "                    'timestamp':timestamp,\n",
    "                    # departure mode (string) \n",
    "                    'mode':train_data['mode'],\n",
    "                    # mission code (string)\n",
    "                    'mission':train_data['mission'],\n",
    "                    # terminus (i.e. station) identifier (number)\n",
    "                    'terminus':int(train_data['terminus']['code'])\n",
    "                } \n",
    "                departures.append(departure)\n",
    "        return departures      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KafkaProducerTask**, une Task dédiée à l'activité asynchrone de notre producteur. \n",
    "\n",
    "Comme indiqué plus haut, Task (dont hérite KafkaProducerTask) implémente un modèle d'object actif du type \"thread + message queue\". On utilise ici sa capacité à executer périodiquement une action définie par l'utilisateur (i.e. activité asynchrone). Dans notre cas, il s'agit d'effectuer une requête sur l'API SNCF et d'injecter les données retournées dans un stream Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaProducerTask(Task, NotebookCellContent):\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __init__(self, config):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # init the Task part of our instance\n",
    "        Task.__init__(self, \"KafkaProducerTask\")\n",
    "        # init the NotebookCellContent part of our instance\n",
    "        NotebookCellContent.__init__(self, \"KafkaProducerTask\")\n",
    "        # store our configuration parameters\n",
    "        self.config = config\n",
    "        # set the logging level to the one specified (or default to logging.DEBUG)\n",
    "        self.set_logging_level(self.config.get('loging_level', logging.DEBUG))\n",
    "        self.debug(\"TSP:initializing...\")\n",
    "        # our kafka producer\n",
    "        self.producer = None\n",
    "        # setup API data polling\n",
    "        self.__setup_api()\n",
    "        self.debug(\"TSP:`-> done!\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def __setup_api(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # setup the SNCF API\n",
    "        # path to the credentials file\n",
    "        credentials_file = self.config.get('credentials', './api_transilien_login.json')\n",
    "        # instanciate our TransilienApi manager (share same cell for logging)\n",
    "        self.api = TransilienApi(credentials_file, owner=self)\n",
    "        # attach a JsonConverter to the TransilienApi manager\n",
    "        self.api.set_converter(JsonConverter())\n",
    "        # attach stations data to the TransilienApi manager (register potentially managed stations)\n",
    "        self.api.load_stations_data(\"./transilien_line_l_stations_by_code.json\")\n",
    "        # tell the TransilienApi manager which stations we manage ('*' = all registered ones)\n",
    "        self.api.set_managed_stations('*')\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def on_init(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # this function is called when the Task starts and is executed in the context \n",
    "        # of the associated thread - it provides us with a chance to perform some \n",
    "        # intialization actions - like instanciating the KafkaProducer:\n",
    "        self.debug(\"KafkaProducerTask: intializing KafkaProducer instance...\")\n",
    "        self.producer = KafkaProducer(\n",
    "                            client_id='transilien-producer-01',\n",
    "                            bootstrap_servers = self.config.get('bootstrap_servers', ['sandbox-hdp.hortonworks.com:6667']),\n",
    "                            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                            acks=0,\n",
    "                            api_version=(0, 10, 1)\n",
    "                        )\n",
    "        self.debug(\"`-> done!\")\n",
    "        # force call to handle_periodic_message (trick to force first data update)\n",
    "        self.handle_periodic_message()\n",
    "        # then setup ourself to poll data from the SNCF API at a given period (in seconds)\n",
    "        p = self.config.get('api_polling_period_in_seconds', 2.)\n",
    "        self.enable_periodic_message(p)\n",
    "        \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def on_exit(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # this function is called when the Task exits and is executed in the context \n",
    "        # of the associated thread - it provides us with a chance to perform some \n",
    "        # termination actions - like closing the KafkaProducer:\n",
    "        self.debug(\"KafkaProducerTask: closing the KafkaProducer instance...\")\n",
    "        self.producer.close()\n",
    "        self.debug(\"`-> done!\")\n",
    "            \n",
    "    # -------------------------------------------------------------------------------\n",
    "    def handle_periodic_message(self):\n",
    "    # -------------------------------------------------------------------------------\n",
    "        # in case 'enable_periodic_message' has been previously invoked on the Task \n",
    "        # instance, this function will called at the specified period - providing\n",
    "        # us with teh ability to execute a periodic and asynchronous job in the \n",
    "        # of the associated thread - a job like polling a API and pushing the data\n",
    "        # into a kafka stream...\n",
    "        try:\n",
    "            # clear cell content (avoid cumiulating to much log into the notebook cell)\n",
    "            self.clear_output()\n",
    "            # do the job...\n",
    "            self.debug(\"KafkaProducerTask: polling data from the SNCF API...\")\n",
    "            t = time.time()\n",
    "            # get some data from the API\n",
    "            departures = self.api.poll_next_station_data()\n",
    "            self.debug(f\"KafkaProducerTask: obtained {len(departures)} train entries in {round(time.time() - t, 2)} s\")\n",
    "            self.debug(f\"KafkaProducerTask: injecting data into the Kafka topic '{self.config['topic']}'...\")\n",
    "            t = time.time()\n",
    "            # inject the data into the kafka stream\n",
    "            for departure in departures:\n",
    "                try:\n",
    "                    # we do that train by train... \n",
    "                    self.producer.send(self.config['topic'], departure)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            self.debug(f\"`-> took {round(time.time() - t, 2)} s\")\n",
    "        except Exception as e:\n",
    "            self.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration de notre KafkaProducerTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_task_config = {\n",
    "    # logging level\n",
    "    'logging_level': logging.DEBUG,\n",
    "    # path to credentials file\n",
    "    'credentials': './api_transilien_login.json',\n",
    "    # kafka servers\n",
    "    'bootstrap_servers': ['sandbox-hdp.hortonworks.com:6667'], \n",
    "    # kafka topic\n",
    "    'topic': 'transilien-02',\n",
    "    # API polling period (in seconds)\n",
    "    'api_polling_period_in_seconds':2.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciation et démarrage de notre KafkaProducerTask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_task = KafkaProducerTask(producer_task_config)\n",
    "producer_task.start_asynchronously()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrêt (et destruction) de notre KafkaProducerTask.\n",
    "\n",
    "Note: Un appel à _exit_ demande au Thread sous-jacent de retourner - ce qui provoque sa 'disparition'. Il existe aucun moyen de redémarrer une Task sur laquelle _exit_ a été invoquée. L'objet doit être reconstruit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_task.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
